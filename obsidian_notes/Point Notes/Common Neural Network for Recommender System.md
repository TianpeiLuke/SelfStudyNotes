---
tags:
  - chatgpt
  - recommender_system/architecture
aliases: 
date of note: 2025-04-07
---

>[!question]
>what are commonly encountered neural network based model for recommender system

## ğŸ”¥ **1. Neural Collaborative Filtering (NCF)**

**Paper:** _He et al., 2017_

- Replaces dot product in matrix factorization with a neural network.
    
- Combines:
    
    - **GMF (Generalized Matrix Factorization)**
        
    - **MLP (Multi-Layer Perceptron)**
        
- Final model: **NeuMF**
    

ğŸ“Œ **Use case:** User-item interaction modeling  
ğŸ“š Libraries: TensorFlow Recommenders, RecBole

---

## ğŸ§  **2. Wide & Deep Learning (Google)**

**Paper:** _Cheng et al., 2016_

- Two-tower structure:
    
    - **Wide**: memorization (linear model)
        
    - **Deep**: generalization (neural network)
        
- Scales well and works great for large, feature-rich datasets.
    

ğŸ“Œ **Use case:** Apps like Google Play recommendations

---

## ğŸ§® **3. DeepFM (Deep Factorization Machines)**

**Paper:** _Guo et al., 2017_

- Combines the strengths of:
    
    - **Factorization Machines (FM)**: for sparse feature interactions
        
    - **DNNs**: for high-order interactions
        
- Works well with tabular data (click-through, conversion rates)
    

ğŸ“Œ **Use case:** Ad click prediction, e-commerce

---

## ğŸŒ€ **4. Autoencoders / Variational Autoencoders (VAE)**

- Used for collaborative filtering via reconstruction.
    
- **Mult-VAE** is a popular variant for user-item implicit feedback.
    
- Learns compressed latent space â†’ used to recommend items
    

ğŸ“Œ **Use case:** Implicit feedback (e.g., likes, views)

---

## â±ï¸ **5. Sequence-Based Models (RNN, GRU, LSTM)**

### â–¶ï¸ GRU4Rec

**Paper:** _Hidasi et al., 2015_

- Uses GRU for **session-based recommendations**.
    
- Learns the sequence of user actions.
    

ğŸ“Œ **Use case:** E-commerce sessions, playlist generation

---

## ğŸ§¬ **6. Transformer-Based Models**

### SASRec (Self-Attentive Sequential Recommender)

**Paper:** _Kang & McAuley, 2018_

- Applies Transformers to model user behavior sequences.
    
- Handles long-term dependencies better than RNNs.
    

### BERT4Rec

**Paper:** _Sun et al., 2019_

- Adapts BERT-style masked language modeling to predict items.
    

ğŸ“Œ **Use case:** Next-item prediction, content streaming (Netflix/Spotify)

---

## ğŸ§± **7. Two-Tower Models**

- User and item representations are learned separately using DNNs.
    
- Combined via similarity (dot product or cosine).
    
- Extremely scalable, used in:
    
    - **YouTube DNN**
        
    - **Facebook DLRM**
        

ğŸ“Œ **Use case:** Large-scale retrieval (millions of users/items)

---

## ğŸŒ **8. Graph Neural Networks for Recommendations**

### LightGCN, NGCF, PinSage

- Models the **user-item graph**.
    
- Learns better embeddings via message passing.
    

ğŸ“Œ **Use case:** Pinterest, social recommendations, implicit feedback

---

### ğŸ”§ Bonus: Tools & Frameworks to Explore These Models

- ğŸ”· **[RecBole](https://recbole.io/)** â€“ Unified framework for deep recommenders
    
- ğŸ”¶ **TensorFlow Recommenders** â€“ Production-friendly TF-based models
    
- ğŸ”µ **[Microsoft Recommenders GitHub](https://github.com/microsoft/recommenders)** â€“ Huge collection of models + tutorials
    

---








-----------
##  Recommended Notes

- [[Recommender System]]