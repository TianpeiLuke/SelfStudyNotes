---
tags:
  - concept
  - natural_language_processing/large_language_models
  - deep_learning/large_language_models
  - parallel_computing
keywords: 
topics: 
name: Parallel Pretraining of Large Language Models
date of note: 2025-03-16
---

## Concept Definition

>[!important]
>**Name**: 


### Data Parallelism 

- [[Data Parallelism]]
- [[Fully Sharded Data Parallel or FSDP for LLM Training]]
- [[Zero Redundancy Optimizer or ZeRO for Optimized Training of LLM]]

### Model Parallelism

- [[Model Parallelism]]
- [[Pipeline Parallelism]]
- [[Tensor Parallelism]]

### Mixture of Experts

- [[Expert Parallelism]]


## Explanation


## Examples

>[!example]
>RnR with BSM **(DDP implementation)**
>- [[Trainer for BERT Fine-tune v2]]
>- [[Multi-modal BERT Classification Model v2 for BSM]]
>- [[Tabular Embedding with Pydantic]]
>- [[BERT Base Embedding Model with Pydantic]]


>[!example] 
>RnR with BSM (**FSDP implementation**)
> - [[Trainer for FSDP Model Parallelism]]
> - [[Inference for FSDP Model Parallelism]]
> - [[Multi-modal BERT for FSDP Model Parallelism]]
> - [[Export to ONNX and Inference]]







-----------
##  Recommended Notes and References






- [[Large Language Model and Pretrained Language Models]]
- Tutorial
	- [Parallel Training Techniques](https://github.com/saforem2/parallel-training-slides#parallel-training-techniques) [Slides](https://saforem2.github.io/parallel-training-slides/#/title-slide)