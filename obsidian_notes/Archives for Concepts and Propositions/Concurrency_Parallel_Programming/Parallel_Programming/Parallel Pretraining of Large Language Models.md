---
tags:
  - concept
  - natural_language_processing/large_language_models
  - deep_learning/large_language_models
  - parallel_computing
keywords: 
topics: 
name: Parallel Pretraining of Large Language Models
date of note: 2025-03-16
---

## Concept Definition

>[!important]
>**Name**: 



## Explanation





-----------
##  Recommended Notes and References

- [[Data Parallelism]]
- [[Model Parallelism]]
- [[Pipeline Parallelism]]
- [[Tensor Parallelism]]
- [[Expert Parallelism]]
- [[Fully Sharded Data Parallel or FSDP for LLM Training]]
- [[Zero Redundancy Optimizer or ZeRO for Optimized Training of LLM]]


- [[Large Language Model and Pretrained Language Models]]
- Tutorial
	- [Parallel Training Techniques](https://github.com/saforem2/parallel-training-slides#parallel-training-techniques) [Slides](https://saforem2.github.io/parallel-training-slides/#/title-slide)