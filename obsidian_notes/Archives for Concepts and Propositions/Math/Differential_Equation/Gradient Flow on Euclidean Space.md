---
tags:
  - concept
  - math/differential_equation
  - math/differential_geometry
  - optimization/theory
  - optimization/algorithm
  - deep_learning/generative_models
keywords: 
topics: 
name: 
date of note: 2024-10-25
---

## Concept Definition

>[!important]
>**Name**: 


>[!important] Definition
>Let $f: \mathbb{R} \to \mathcal{X} \subset \mathbb{R}$ be a smooth map.
>
>The **gradient flow** on $f$ is a smooth curve that is described by the *ordinary differential equation*
>$$
>\frac{d}{dt} x(t) = - \nabla f(x(t))
>$$

- [[Gradient of Smooth Map]]
- [[Ordinary Differential Equations]]
- [[Local Flow on Smooth Manifold]]

- [[Divergence Operator of Vector Field on Riemannian Manifold]]
- [[Coordinate Representation of Gradient]]
- [[Coordinate Representation of Divergence Operator]]


## Explanation


## Gradient Descent

- [[Stochastic Gradient Descent Algorithm]]
- [[Gradient Descent Algorithm]]
- [[Langevin Dynamics and Langevin Sampling]]




-----------
##  Recommended Notes and References


- [[Wasserstein Space]]
- [[Wasserstein Distance]]
- [[Optimal Transport in Space of Measures]]




- [[Fokker–Planck Equation via Wasserstein Distance]]
- [[Fokker–Planck and Kolmogorov Forward-Backward Equation]]



- [[Gradient Flows in Metric Spaces and in Space of Probability Measures by Ambrosio]]
- [[Optimal Transport for Applied Mathematicians by Santambrogio]]
- [[Optimal Transport Old and New by Villani]]
- [[An Introduction to Optimization on Smooth Manifolds by Boumal]]
- [[Ordinary Differential Equations by Chicone]]
- Wibisono, A. (2018, July). Sampling as optimization in the space of measures: The Langevin dynamics as a composite optimization problem. In _Conference on Learning Theory_ (pp. 2093-3027). PMLR.