---
tags:
  - concept
  - natural_language_processing
  - natural_language_processing/word_representation
keywords:
  - vector_semantics
  - distributional_hypothesis_linguistic
topics:
  - natural_language_processing/wording_representation
name: Vector Semantics and Distributional Hypothesis in Linguistic
date of note: 2024-05-12
---

## Concept Definition

>[!important]
>**Name**: Vector Semantics and Distributional Hypothesis in Linguistic

>[!important] Definition
>The **distributional hypothesis** in *linguistic* states that
>> Words that occur in *similar contexts* tend to have *similar meanings*. 
>
>In other word, it assumes that there exists a link between *similarity* in how words are *distributed* and *similarity*  in what they *mean*.

>[!important] Definition
>The **vector semantics** instantiates the *distributional hypothesis* by learning representation of the meaning of words, called the **word embedding.**
>
>The *idea* of **vector semantics** is to represent a word as a point in a *multidimensional semantic space* that is derived (in ways weâ€™ll see) from the *distributions of  word neighbors*.

- [[Word Embedding]]

### Lexical Semantics and the Meaning of Words

- [[Lexical Semantics]]
- [[Lemmas and Senses]]
- [[Synonymy]]
- [[Principle of Contrast for Semantic Analysis]]
- [[Semantic Frames and Roles]]
- [[Semantic Field and Topics]]
- [[Connotations of Word and Sentiment]]

### Similarity of Words

- [[Word Association and Relatedness]]
- [[Cosine Similarity and Cosine Distance]]


### Learning of Word Embedding

- [[Skip-Gram Algorithm for Word Embedding]]
- [[Contiuous-Bag-of-Words Algorithm for Word Embedding]]




## Explanation





-----------
##  Recommended Notes and References




- [[Speech and Language Processing by Jurafsky]] pp 101, 105