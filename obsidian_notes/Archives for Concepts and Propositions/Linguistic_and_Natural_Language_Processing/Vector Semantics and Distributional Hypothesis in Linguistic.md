---
tags:
  - concept
  - natural_language_processing
  - natural_language_processing/word_representation
keywords:
  - vector_semantics
  - distributional_hypothesis_linguistic
topics:
  - natural_language_processing/wording_representation
name: Vector Semantics and Distributional Hypothesis in Linguistic
date of note: 2024-05-12
---

## Concept Definition

>[!important]
>**Name**: Vector Semantics and Distributional Hypothesis in Linguistic

>[!important] Definition
>The **distributional hypothesis** in *linguistic* states that
>> Words that occur in *similar contexts* tend to have *similar meanings*. 
>
>In other word, it assumes that there exists a link between *similarity* in how words are *distributed* and *similarity*  in what they *mean*.

>[!important] Definition
>**Vector semantics** is an approach in computational linguistics that represents the *meaning* of words, phrases, or even entire documents as *vectors* (numerical arrays) in a *high-dimensional space*.
>
>- The idea of **vector semantics** is to represent a word as a point in a *multidimensional semantic space* that is derived from the *distributions of  word neighbors*.
>- The vector semantics instantiates the *distributional hypothesis* by learning the representation of the meaning of words, called the **word embedding.**

- [[Word Embedding]]

### Lexical Semantics and the Meaning of Words

- [[Lexical Semantics]]
- [[Lemmas and Senses]]
- [[Synonymy]]
- [[Principle of Contrast for Semantic Analysis]]
- [[Semantic Frames and Roles]]
- [[Semantic Field and Topics]]
- [[Connotations of Word and Sentiment]]

### Similarity of Words

- [[Word Association and Relatedness]]
- [[Cosine Similarity and Cosine Distance]]

### Vector Space Model

- [[Vector Space Model in Information Retrieval]]


### Word2Vec Embedding

- [[Word2Vec Algorithm for Static Word Embedding]]
- [[Skip-Gram Algorithm for Word Embedding]]
- [[Contiuous-Bag-of-Words Algorithm for Word Embedding]]

### Representation Learning and LLM

- [[Representation Learning]]
- [[Auto-Encoder and Stochastic Auto-Encoder]]
- [[Bidirectional Encoder Representation from Transformer or BERT]]



## Explanation





-----------
##  Recommended Notes and References




- [[Speech and Language Processing by Jurafsky]] pp 101, 105