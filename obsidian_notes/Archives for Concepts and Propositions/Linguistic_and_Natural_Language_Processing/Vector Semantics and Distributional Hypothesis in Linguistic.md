---
tags:
  - concept
  - natural_language_processing
  - natural_language_processing/word_representation
keywords:
  - vector_semantics
  - distributional_hypothesis_linguistic
topics:
  - natural_language_processing/wording_representation
name: Vector Semantics and Distributional Hypothesis in Linguistic
date of note: 2024-05-12
---

## Concept Definition

>[!important]
>**Name**: Vector Semantics and Distributional Hypothesis in Linguistic

>[!important] Definition
>The **distributional hypothesis** in *linguistic* states that
>> Words that occur in *similar contexts* tend to have *similar meanings*. 
>
>In other word, it assumes that there exists a link between *similarity* in how words are *distributed* and *similarity*  in what they *mean*.

^849498

>[!important] Definition
>**Vector semantics** is an approach in computational linguistics that represents the *meaning* of words, phrases, or even entire documents as *vectors* (numerical arrays) in a *high-dimensional space*.
>
>- The idea of **vector semantics** is to represent a word as a point in a *multidimensional semantic space* that is derived from the *distributions of  word neighbors*.
>- The vector semantics instantiates the *distributional hypothesis* by learning the representation of the meaning of words, called the **word embedding.**
>- **vector semantics** refers to the set of NLP methods that aim to learn the word representation based on *distributional properties* of words in a large corpus.

^be40ca

- [[Word Embedding]]

### Distributional Representation and Distributed Representation


>[!important] Definition
>The **distributional representation** refers to representation based on *distribution* of *words* from the *context* in which the words appear.
>- This scheme is based on the **distributional hypothesis**.
>- A distributional representation is based on the *co-occurrence matrix.*
>- The *distributional representation* is **sparse.**

^c91885

- [[Co-Occurrence Matrix]]

![[Distributed Representation#^04ca19]]

- [[Distributed Representation]]

### Lexical Semantics and the Meaning of Words

- [[Lexical Semantics]]
- [[Lemmas and Senses]]
- [[Synonymy]]
- [[Principle of Contrast for Semantic Analysis]]
- [[Semantic Frames and Roles]]
- [[Semantic Field and Topics]]
- [[Connotations of Word and Sentiment]]

### Similarity of Words

- [[Word Association and Relatedness]]
- [[Cosine Similarity and Cosine Distance]]

### Vector Space Model

- [[Vector Space Model in Information Retrieval]]
- [[Co-Occurrence Matrix]]
- [[Term Frequency and Inverse Document Frequency or TF-IDF]]

### Word2Vec Embedding

- [[Word2Vec Algorithm for Static Word Embedding]]
- [[Noise Contrastive Estimation]]
- [[Skip-Gram Algorithm with Negative Sampling for Word Embedding]]
- [[Contiuous-Bag-of-Words Algorithm for Word Embedding]]

### Representation Learning and LLM

- [[Representation Learning]]

- [[Auto-Encoder and Stochastic Auto-Encoder]]
- [[Information Noise Contrastive Estimation as Contrastive Learning]]
- [[Transformer Network]]
- [[Bidirectional Encoder Representation from Transformer or BERT]]



## Explanation





-----------
##  Recommended Notes and References


- [[Distributed Representation]]

- [[Speech and Language Processing by Jurafsky]] pp 101, 105