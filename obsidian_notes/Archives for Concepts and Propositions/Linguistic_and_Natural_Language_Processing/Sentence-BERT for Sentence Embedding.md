---
tags:
  - concept
  - deep_learning/large_language_models
  - natural_language_processing/large_language_models
  - natural_language_processing/sentence_representation
  - information_retrieval/retrieval_augmentation
keywords: 
topics: 
name: 
date of note: 2024-12-07
---

## Concept Definition

>[!important]
>**Name**: 



- [[Bidirectional Encoder Representation from Transformer or BERT]]
- [[Cosine Similarity and Cosine Distance]]
- [[Siamese Network or Twin Tower Network for Contrastive Learning]]
- [[Triplet Loss Minimization for Contrastive Learning]]
- [[Contrastive Learning]]

![[sentence_bert.png]]


## Explanation





-----------
##  Recommended Notes and References



- [[Representation Learning]]
- [[Masked Language Modeling as Language Model Training Task]]
- [[Next Sentence Prediction as Language Model Training Task]]

- [[Large Language Model and Pretrained Language Models]]


- [[Dense Text Retrieval with Pretrained Language Models]]
- [[Retrieval Augmented Generation]]
- [[Information Retrieval]]



- [[Speech and Language Processing by Jurafsky]]
- [[reimersSentenceBERTSentenceEmbeddings2019]] Nils Reimers and Iryna Gurevych. 2019. [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://aclanthology.org/D19-1410). In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 3982â€“3992, Hong Kong, China. Association for Computational Linguistics. 