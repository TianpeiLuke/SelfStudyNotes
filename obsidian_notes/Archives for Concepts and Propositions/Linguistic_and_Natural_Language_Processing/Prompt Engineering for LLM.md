---
tags:
  - concept
  - deep_learning/architecture
  - deep_learning/models
keywords: 
topics: 
name: 
date of note: 2024-10-21
---

## Concept Definition

>[!important]
>**Name**: 


- [[In-Context Learning and Prompt Engineering for LLM]]
- [[Generative Pre-trained Transformer or GPT]]
- [[Parameter Efficient Fine Tuning or PEFT for Large Language Model]]

## Explanation





-----------
##  Recommended Notes and References


- [[Attention Mechanism in Neural Network]]
- [[Transformer Network]]
- [[Large Language Model and Pretrained Language Models]]
- [[Foundational Models for Transfer Learning]]


- [[Deep Learning Foundations and Concepts by Bishop]] pp 394
- [[Speech and Language Processing by Jurafsky]] pp 243
- Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, _55_(9), 1-35. [[liuPretrainPromptPredict2023]]