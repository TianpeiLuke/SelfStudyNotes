---
tags:
  - concept
  - optimization/algorithm
  - numerical_methods/numerical_linear_algebra
  - CGNR
  - conjugate_gradient_normal_equation_residual
keywords:
  - conjugate_gradient_normal_equation_residual
topics:
  - numerical_linear_algebra
  - optimization
  - optimization/algorithm
name: Conjugate Gradient Normal Equation Residual and CGNR
date of note: 2024-09-30
---

## Concept Definition

>[!important]
>**Name**: Conjugate Gradient Normal Equation Residual and CGNR

### Krylov Subspace

![[Krylov Subspace and Krylov Matrix#^27eea3]]

![[Nilpotent Linear Transformation and Matrix#^a1b57c]]

![[Nilpotent Linear Transformation and Matrix#^ee0cec]]

- [[Krylov Subspace and Krylov Matrix]]
- [[Nilpotent Linear Transformation and Matrix]]
- [[Nilpotent Linear Transformation Geometric Characterization]]

### CGNR as Conjugate Gradient Method over Krylov Subspaces

>[!important] Definition
>Consider the task of solving the *unsymmetric system of equations* $$Ax = b$$ where $A\in \mathbb{R}^{m\times n}$ and $b\in \mathbb{R}^{m}$.
>- We can convert this system into the **normal equations** $$A^TA x = A^Tb.$$ or $$AA^{T}y = b, \quad x= A^{T}y$$
>- Note that $$\text{Ker}(A) = \text{Ker}(A^{T}A) = \text{Ker}(AA^{T})$$ 
>
>At *each iteration* $t$,  the **conjugate gradient normal equation residual (CGNR, CGNER or CGN)** solve the *normal equation* or the *least square problem* $$A^TA x = A^Tb \quad \iff \quad \min_{x} \lVert Ax - b \rVert_{2}^2$$  over the **Krylov subspace** of degree $t$, generated by $A$, $b$, i.e. $$\mathcal{K}_{t} := \mathcal{K}(A, b, t) = \text{span}\left\{ b, \,Ab \,{,}\ldots{,}\, A^{t-1}b\right\}.$$

- [[Normal Equations and Newton System of Equations]]

>[!important] Definition
>Consider the task of solving the **unsymmetric linear system of equations** $$Ax = b$$ where $A\in \mathbb{R}^{m\times n}$ and $b\in \mathbb{R}^{m}$. 
>
>The the **conjugate gradient normal equation residual (CGNR, CGNER or CGN)**  solves above problem by iteratively solving
>-  the *normal equation* $$A^TA x = A^Tb$$ 
>- or the *least square estimation* $$\min_{x} \lVert Ax - b \rVert_{2}^2$$  on an increasing sequence of *nested* *Krylov subspaces* generated by $A$ and $b$ $$\mathcal{K}_{1} \subset \mathcal{K}_{2} \,{\subset}\ldots{\subset}\,\mathcal{K}_{t}.$$ 
>
>Specifically, the algorithm is described as below:
>- *Require*: $A\in \mathbb{R}^{m\times n}$ and $b\in \mathbb{R}^{m}$
>- *Require*: initial solution $x_{0}$
>- Initialize 
>	- $r_{0} = Ax_{0} - b$; 
>	- $z_{0} = A^{T}r_{0}$
>	- and $d_{0} = - r_{0}$, 
>	- and $k \leftarrow 0$;
>- While $r_{k} \neq 0$
>	- compute **step size** $$\alpha_{k} = - \frac{z_{k}^T\,z_{k}}{d_{k}^T\,A^{T}\,A\,d_{k}}$$
>		- or $$\hat{d}_{k} := A\,d_{k}$$
>		- and $$\alpha_{k} = - \frac{z_{k}^T\,z_{k}}{\hat{d}_{k}^{T}\hat{d}_{k}}$$
>	- generate new point via **conjugate gradient direction**: $$x_{k+1} = x_{k} + \alpha_{k}\,d_{k}$$
>	- update **residual**: $$r_{k+1} = r_{k} + \alpha_{k}\,A\,d_{k}$$
>	- update **adjoint map** of *residual* i.e. $A^{T}r$ : $$z_{k+1} = A^{T}\,r_{k+1}$$ 
>	- compute **step size** for *conjugate gradient update*: $$\beta_{k+1} = \frac{z_{k+1}^T\,z_{k+1}}{z_{k}^T\,z_{k}}$$ 
>	- generate new **conjugate gradient direction**: $$d_{k+1} = - z_{k+1} + \beta_{k+1}\,d_{k}$$
>	- $k \leftarrow k+1$

^4c3023


- [[Conjugate Gradient Algorithm Linear]]
- [[Least Square Estimation]]


## Explanation

>[!info]
>**CGNR** belongs to a family of **Krylov subspace methods**. 

- [[Krylov Subspace Methods]]
- [[GMRES as Regression on Krylov Space]]

>[!info]
>Compare to linear conjugate gradient, **CGNR**
>- replaces $$\left\langle  d_{k}\,,\,A d_{k}   \right\rangle \implies \left\langle  d_{k}\,,\,A^{T}A d_{k}  \right\rangle = \left\langle  Ad_{k}\,,\, Ad_{k}   \right\rangle$$ when computing the **step size** $\alpha_{k}$ for solution update;
>- replaces the role of **residual** $r_{k}$ by the **adjoint map of residual** $z_{k}$ $$r_{k} = Ax_{k} - b \implies z_{k} = A^{T}r_{k} = A^{T}Ax_{k} - A^{T}b$$ when the computing the the **step size** $\beta_{k}$ for gradient direction update; and also when the *new gradient* is accumulated.




-----------
##  Recommended Notes and References




- [[Conjugate Gradient Algorithm Nonlinear]]
- [[Conjugate Gradient Algorithm Convergence Analysis]]
- [[Conjugate Gradient Algorithm Nonlinear Convergence Analysis]]



- [[Least Square Estimation Solution and Geometric Interpretation]]
- [[Algorithms for Least Square Estimation Problem]]
- [[Krylov Subspace and Krylov Matrix]]


- [[System of Linear Equations or Linear System]]
- [[Existence and Uniqueness of Solution of Linear Equations]]




- [[Matrix Analysis by Horn]]
- [[Matrix Computations by Golub]] pp 636 - 638
- [[Numerical Linear Algebra by Trefethen]] pp 304 - 305