---
tags:
  - concept
  - optimization/algorithm
  - numerical_methods/numerical_linear_algebra
  - optimization/convex_optimization
keywords:
  - conjugate_vector_positive_definite
  - conjugate_gradient_linear
topics:
  - optimization/algorithm
  - numerical_linear_algebra
name: Conjugate Gradient Algorithm Linear
date of note: 2024-05-12
---

## Concept Definition

>[!important]
>**Name**: Conjugate Gradient Algorithm Linear

>[!important] Definition (Preliminary Form)
>Consider the problem of solving the following *system of linear equations*:
>$$
> Ax = b
>$$
>where $A$ is a *symmetric, positive definite matrix*. It is equivalent to solve the following *quadratic optimization problem*
>$$
>\min_{x}\; \frac{1}{2}x^T\,A\,x - b^T\,x
>$$
>
>
>Given $x_{0}$, the **preliminary form of conjugate gradient method** is described as follows:
>
>- Set $r_{0} = Ax_{0} - b$; and $d_{0} = - r_{0}$, and $k \leftarrow 0$;
>- While $r_{k} \neq 0$
>	- compute **step size** $$\alpha_{k} = - \frac{r_{k}^T\,d_{k}}{d_{k}^T\,A\,d_{k}}$$
>	- generate new point via **conjugate gradient direction**: $$x_{k+1} = x_{k} + \alpha_{k}\,d_{k}$$
>	- update **residual**: $$r_{k+1} = Ax_{k+1} - b$$
>	- compute **step size** for *conjugate gradient update*: $$\beta_{k+1} = \frac{r_{k+1}^T\,A\,d_{k}}{d_{k}^T\,A\,d_{k}}$$ 
>	- generate new **conjugate gradient direction**: $$d_{k+1} = - r_{k+1} + \beta_{k+1}\,d_{k}$$
>	- $k \leftarrow k+1$

^48ebc6

- [[Quadratic Programming]]
- [[Conjugate Vector with respect to Positive Definite Transformation]]
- [[Positive Semidefinite Transformation]]
- [[Self-Adjoint Linear Map]]
- [[Numerical Optimization by Nocedal]] pp 108

>[!important] Definition (Standard Form)
>Consider the problem of solving the following *system of linear equations*:
>$$
> Ax = b
>$$
>where $A$ is a *symmetric, positive definite matrix*. It is equivalent to solve the following *quadratic optimization problem*
>$$
>\min_{x}\; \frac{1}{2}x^T\,A\,x - b^T\,x
>$$
>
>Given $x_{0}$, the **conjugate gradient (CG) method** is described as follows:
>
>- Set $r_{0} = Ax_{0} - b$; and $d_{0} = - r_{0}$, and $k \leftarrow 0$;
>- While $r_{k} \neq 0$
>	- compute **step size** $$\alpha_{k} = - \frac{r_{k}^T\,r_{k}}{d_{k}^T\,A\,d_{k}}$$
>	- generate new point via **conjugate gradient direction**: $$x_{k+1} = x_{k} + \alpha_{k}\,d_{k}$$
>	- update **residual**: $$r_{k+1} = r_{k} + \alpha_{k}\,A\,d_{k}$$
>	- compute **step size** for *conjugate gradient update*: $$\beta_{k+1} = \frac{r_{k+1}^T\,r_{k+1}}{r_{k}^T\,r_{k}}$$ 
>	- generate new **conjugate gradient direction**: $$d_{k+1} = - r_{k+1} + \beta_{k+1}\,d_{k}$$
>	- $k \leftarrow k+1$

^e7cffe

## Explanation

>[!info]
>In conjugate gradient method, each direction $d_{k}$ is chosen to be a **linear combination** of **negative residual** $-r_{k}$ and the **previous direction** $d_{k-1}.$
>$$d_{k} = - r_{k} + \beta_{k}\,d_{k-1}.$$
>
>Note that $-r_{k}$ is the **gradient descent direction** $-\nabla f$ where $$f(x) = \frac{1}{2} x^TAx - b^Tx.$$ 

>[!info]
>The stepsize for point update $\alpha_{k}$ is chosen as the **linear coefficient** for the *displacement vector* under **conjugate direction basis**
>$$
> x^{*} - x_{0} = \alpha_{0}\,d_{0} + \alpha_{1}\,d_{1} \,{+}\ldots{+}\, \alpha_{k-1}\,d_{k-1}
>$$
>or as the **the length of orthogonal projection** of $x^{*} - x_{0}$ onto *Krylov subspace*
>$$
> \alpha_{k} =  \min\left\{\lVert  x^{*} - x_{0} - d \rVert: d\in \text{span}\left\{ d_{0}, d_{1} \,{,}\ldots{,}\, d_{k-1}\right\}   \right\}   
>$$

>[!info]
>The stepsize $\beta_{k}$ is chosen so that 
>$$
>\left\langle  Ad_{k-1}\,,\, d_{k} \right\rangle = 0.
>$$


## Conjugate Direction and Krylov Subspace

>[!important] Proposition
>Suppose that the $k$-th iterate generated by the **conjugate gradient method** is not the solution $x^{*}$.
>
>The following properties hold:
>- **Orthogonality of residual**: the new residual is *orthogonal to* residuals generated at each previous iterates $$\left\langle  r_{i}\,,\, r_{k}  \right\rangle = 0, \quad \forall i = 0 \,{,}\ldots{,}\, k-1.$$
>- **Subspace spanned by Residuals**: the *subspace spanned by all residuals* is identical to the *Krylov subspace* of degree $k$ generated by $A$ and initial residual $r_{0}$, i.e. $$\text{span}\left\{ r_{0}, r_{1} \,{,}\ldots{,}\, r_{k}\right\} = \text{span}\left\{ r_{0}, Ar_{0} \,{,}\ldots{,}\, A^{k}r_{0}\right\}.$$
>- **Subspace spanned by conjugate directions**: the *subspace spanned by all  directions* is identical to the *Krylov subspace* of degree $k$ generated by $A$ and initial residual $r_{0}$, i.e. $$\text{span}\left\{ d_{0}, d_{1} \,{,}\ldots{,}\, d_{k}\right\} = \text{span}\left\{ r_{0}, Ar_{0} \,{,}\ldots{,}\, A^{k}r_{0}\right\}.$$
>- **Conjugacy**: the new direction is *conjugate to all previous directions* with respect to $A$ $$\left\langle A\,d_{i}\,,\,d_{k} \right\rangle = 0, \quad i=0,1 \,{,}\ldots{,}\,k-1.$$

^4f0700


- [[Krylov Subspace Methods]]
- [[Conjugate Vector with respect to Positive Definite Transformation]]
- [[Krylov Subspace and Krylov Matrix]]
- [[Numerical Optimization by Nocedal]] pp 109

## Convergence Analysis

- [[Conjugate Gradient Algorithm Convergence Analysis]]

## Precondition CG

>[!important] Definition (Preconditioned Form)
>Let $A$ is a *symmetric, positive definite matrix*. 
>
>Given $x_{0}$, and a **preconditioner** $M$, the **preconditioned conjugate gradient (CG) method** is described as follows:
>
>- Set $r_{0} = Ax_{0} - b$; 
>- Solve **linear equations** for $y_{0}$, $$M y_{0} = r_{0}$$
>- Set $d_{0} = - y_{0}$, and $k \leftarrow 0$;
>- While $r_{k} \neq 0$
>	- compute **step size** $$\alpha_{k} = - \frac{r_{k}^T\,y_{k}}{d_{k}^T\,A\,d_{k}}$$
>	- generate new point via **conjugate gradient direction**: $$x_{k+1} = x_{k} + \alpha_{k}\,d_{k}$$
>	- update **residual**: $$r_{k+1} = r_{k} + \alpha_{k}\,A\,d_{k}$$
>	- solve **linear equations** for $y_{k+1}$: $$M\,y_{k+1} = r_{k+1}$$
>	- compute **step size** for *conjugate gradient update*: $$\beta_{k+1} = \frac{r_{k+1}^T\,y_{k+1}}{r_{k}^T\,y_{k}}$$ 
>	- generate new **conjugate gradient direction**: $$d_{k+1} = - y_{k+1} + \beta_{k+1}\,d_{k}$$
>	- $k \leftarrow k+1$

- [[Preconditioning to improve Numerical Stability]]

## Conjugate Gradient Normal Equation Residual 

- [[Normal Equations and Newton System of Equations]]
- [[Conjugate Gradient Normal Equation Residual and CGNR]]

## Other Krylov Subspace Methods

- [[Krylov Subspace Methods]]
- [[Lanczos Iteration for Tridiagonal Reduction of Large Matrix]]
- [[Arnoldi Iteration for Hessenberg Reduction of Large Matrix]]
- [[GMRES as Regression on Krylov Space]]

## Relation to the Eigenvalue Problem

>[!quote]
>There is a close analogy between the **CG iteration** for solving $$Ax = b$$ and the **Lanczos iteration** for finding eigenvalues $$Ax = \lambda x.$$ 
>- The eigenvalues of $A$, as discussed in Lecture 27, are the **stationary values** for $x  \in \mathbb{R}$ in of the **Rayleigh quotient**, $$r(x) = \frac{\left\langle  x\,,\, Ax \right\rangle}{\left\langle  x\,,\, x \right\rangle}.$$ 
>	- As pointed out in Exercise 36.1, the **eigenvalue estimates (Ritz values)** associated with step $n$ of the *Lanczos iteration* are the **stationary values** of the same function $r(x)$ if $x$ is restricted to the **Krylov subspace** $\mathcal{K}_{n}$. 
>- This is a perfect parallel of what we have shown in the last two pages, that the solution $x^{*}$ of $Ax = b$ is the **minimal point** in $\mathbb{R}^{m}$ of the scalar function $w(x)$, 
>	- and the **CG iterate** $x_{n}$ is the **minimal point** of the same function $w(x)$ if $x$ is *restricted to* $\mathcal{K}_{n}$.
>	  
>-- [[Numerical Linear Algebra by Trefethen]] pp 298	  

- [[Rayleigh Quotient for Eigenvalue Problem]]
- [[Convex Optimization for Eigenvalue Problem]]
- [[Lanczos Iteration for Tridiagonal Reduction of Large Matrix]]
- [[Ritz Pair for Linear Map with respect to Subspace]]


-----------
##  Recommended Notes and References

- [[Conjugate Vector with respect to Positive Definite Transformation]]
- [[Krylov Subspace and Krylov Matrix]]

- [[Conjugate Gradient Algorithm Nonlinear]]
- [[Newton Method]]
- [[System of Linear Equations or Linear System]]


- [[Numerical Optimization by Nocedal]] pp 103
- [[Nonlinear Programming by Bertsekas]] pp 130
- [[Deep Learning by Goodfellow]] pp 304
- [[Optimization by Vector Space Methods by Luenberger]] pp 291 - 294
- [[Matrix Computations by Golub]] pp 625 - 639
- [[Numerical Linear Algebra by Trefethen]] pp 294 - 301