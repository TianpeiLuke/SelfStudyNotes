---
tags:
  - concept
  - optimization/algorithm
keywords:
  - cutting_plane_method
  - outer_linearization
topics:
  - optimization
name: Cutting Plane Methods and Outer Linearization
date of note: 2024-05-12
---

## Concept Definition

>[!important]
>**Name**: Cutting Plane Methods and Outer Linearization

>[!important] Definition
>Consider the following *convex optimization problem*
>$$
> \min_{x\in \mathcal{X}} f(x)
>$$
>where $f: \mathbb{R}^{n} \to \mathbb{R}$ is a *convex function* and $\mathcal{X}$ is a *closed convex set*.
>
>At each iteration $k$, the **cutting plane method** solves an *approximate problem*
>$$
> x_{k+1} \in \arg\min_{x\in \mathcal{X}} F_{k}(x)
>$$
>where $F_{k}$ is a **polyhedral approximation** of $f$, constructed using $x_{0} \,{,}\ldots{,}\,x_{k}$ generated so far, and associated *subgradients* $$g_{i} \in \partial f(x_{i}), \quad i=1 \,{,}\ldots{,}\,k.$$ In particular, define the *polyhedral function*
>$$
>F_{k}(x) = \max\left\{ \ell(x; x_{0}) \,{,}\ldots{,}\,\ell(x; x_{k})  \right\} 
>$$
>where $\ell(x;x_{i})$ is the *first-order approximation* of $f$ at $x_{i}$ (or the *supporting hyperplane* of the *epigraph* of $f$), i.e. $$\ell(x; x_{i}) := f(x_{i}) + \left\langle  g_{i}\,,\, x - x_{i}    \right\rangle, \quad i=1 \,{,}\ldots{,}\,k$$

^6f4130

- [[Polyhedral Approximation for Optimization]]
- [[Approximation Method for Optimization]]
- [[Subdifferential of Convex Function]]


## Explanation

>[!quote]
>Cutting plane methods are rooted in the representation of a **closed convex set** as the **intersection** of its **supporting halfspaces**.
>
>The idea is to *approximate* either the **constraint set** or the **epigraph of the cost function** by the *intersection* of a **limited number of half-spaces**, and to gradually *refine the approximation* by generating additional halfspaces through the use of subgradients.
>
>-- [[Convex Optimization Algorithms by Bertsekas]] pp 183

- [[Dual Representation of Convex Set]]
- [[Supporting Hyperplane of Convex Set]]
- [[Support functional of Convex Set]]

## Convergence

>[!important] Proposition
>Each **limit point** of the sequence $\{ x_{k} \}$ generated by the **cutting plane method** is an **optimal solution**.

>[!important]
>It practice, it is common to use the **inequalities**
> $$F_{k-1}(x_{k}) \le \min_{x \in \mathcal{X}}f(x) \le \min_{0 \le j \le k}f(x_{j}), \quad k=0 \,{,}\ldots{,}\,$$ to **bound** the *optimal value* $p^{*}$. Thus the iteration stops when the **upper and lower bound difference** 
> $$
> \min_{0 \le j \le k}f(x_{j}) - F_{k-1}(x_{k}) 
> $$
> comes within a small tolerance $\epsilon$.

>[!important] Proposition (Polyhedral Objective Function)
>Assume that the cost function $f$ is **polyhedral** of the form $$f(x) = \max_{i\in I}\left\{ \left\langle a_{i}\,,\, x \right\rangle + b_{i} \right\}.$$
>
>Then the **cutting plane method**, with the *subgradient selection* and **termination rules** 
>$$\min_{0 \le j \le k}f(x_{j}) - F_{k-1}(x_{k}) < \epsilon,$$ obtains an *optimal solution* in a **finite number** of iterations.


## Limitations

>[!quote]
>Despite the finite convergence property shown in Prop. 4.1.2, the cutting plane method has **several drawbacks**:
> 
> 1. It can take **large steps away from the optimum**, resulting in *large
> cost increases*, even when it is *close to (or even at) the optimum.* This
> phenomenon is referred to as **instability**, and has another *undesirable
> effect*, namely that the *current point* $x_{k}$ **may not be a good starting
> point** for the algorithm that minimizes the new approximate cost
> function $F_{k}(x)$ over $\mathcal{X}$.
> 
> 2. The *number of subgradients* used in the cutting plane approximation
> $F_{k}$ **increases without bound** as $k \to \infty$ leading to a *potentially large
> and difficult optimization problem* to find $x_{k}$. To *remedy* this, one
> may **occasionally discard** some of the cutting planes. To guarantee
> convergence, it is essential to do so *only at times* when improvement
> in the cost is recorded, e.g., $f(x_{k}) \le \min_{j <k} f(x_{j}) - \delta$ for some small positive $\delta$. Still one has to be **judicious** about *discarding cutting
> planes*, as some of them may reappear later.
> 
>3.  The **convergence is often slow**. Indeed, for challenging problems, even when $f$ is *polyhedral*, one should base **termination** on the **upper and lower bounds** $$F_{k-1}(x_{k}) \le \min_{x \in \mathcal{X}}f(x) \le \min_{0 \le j \le k}f(x_{j})$$ rather than wait for *finite termination* to occur.
>
>--  [[Convex Optimization Algorithms by Bertsekas]] pp 187




-----------
##  Recommended Notes and References

- [[Approximation Method for Optimization]]
- [[Convex Optimization Problem]]
- [[Convex Set]]
- [[Convex Function]]

- [[Inner Linearization Methods and Simplicial Decomposition]]

- [[Convex Optimization Algorithms by Bertsekas]] pp 182
- [[Nonlinear Programming by Bertsekas]] pp 629
- [[Introduction to Linear Optimization by Bertsimas]] 