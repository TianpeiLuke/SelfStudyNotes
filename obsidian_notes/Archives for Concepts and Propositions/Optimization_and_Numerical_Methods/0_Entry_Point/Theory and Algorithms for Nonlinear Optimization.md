---
tags:
  - entry_point
  - concept
  - optimization/algorithm
  - optimization/theory
keywords: 
topics:
  - optimization
name: 
date of note: 2024-05-12
---

## Concept Definition

### Theory

- [[Concepts and Theorems in Convex Analysis and Convex Optimization Theory]]

### Concepts

- [[Constrained Optimization Problem]]
- [[Unconstrained Local Minimization]]
- [[Unconstrained Global Minimization]]
- [[Necessary and Sufficient Condition for Local Minimum of Smooth Function]]
- [[Stationary Point and Singular Point of Smooth Function]]
- [[Convex Optimization Problem]]
- [[Methods of Lagrangian Multipliers]]
- [[Lagrangian Dual Function]]
- [[Lagrange Dual Problem]]
- [[Fenchel Duality Theorem]]
- [[Saddle Point of Lagrangian Function]]
- [[Karush-Kuhn-Tucker Optimality Condition]]


### First-Order Gradient Algorithms

- [[Iterative Descent]]
- [[Gradient Descent Algorithm]]
- [[Line Search Strategies for Optimal Stepsize]]

### Adaptive Stepsize Gradient Method

- [[Momentum Algorithm]]
- [[Adam Algorithm]]

### Second-Order Gradient Algorithms

- [[Newton Method]]
- [[Secant Equation and Quasi-Newton Methods]]
- [[BFGS Algorithm]]
- [[Limited Memory BFGS]]
- [[Conjugate Gradient Algorithm Linear]]
- [[Conjugate Gradient Algorithm Nonlinear]]
- [[Newton-Conjugate Gradient and Inexact Newton Method]]

### Feasible Direction Method for Constrained Problem

- [[Feasible Direction Method]]
- [[Gradient Projection Method]]
- [[Block Coordinate Descent Algorithm]]

### Trust-Region Method

- [[Trust Region Method]]
- [[Trust Region Newton-Conjugate Gradient Method]]

### Stochastic Optimization Method

- [[Stochastic Gradient Descent Algorithm]]





### Gradient Computation





## Explanation





-----------
##  Recommended Notes and References

- [[Theory and Algorithms for Convex Optimization]]

- [[Convex Optimization by Boyd]]
- [[Numerical Optimization by Nocedal]]
- [[Nonlinear Programming by Bertsekas]]
- [[Convex Optimization Algorithms by Bertsekas]]