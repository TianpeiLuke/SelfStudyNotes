---
tags:
  - concept
  - numerical_methods/numerical_linear_algebra
  - math/matrix_analysis
  - statistics/estimation
keywords:
  - generalized_minimal_residual_method
  - gmres
  - krylov_subspace
topics:
  - numerical_linear_algebra
  - matrix_analysis
name: GMRES as Regression on Krylov Space
date of note: 2024-10-04
---

## Concept Definition

>[!important]
>**Name**: GMRES or *Generalized Minimal Residual method*

### Krylov Subspace

![[Krylov Subspace and Krylov Matrix#^27eea3]]

![[Nilpotent Linear Transformation and Matrix#^a1b57c]]

![[Nilpotent Linear Transformation and Matrix#^ee0cec]]

- [[Krylov Subspace and Krylov Matrix]]
- [[Nilpotent Linear Transformation and Matrix]]
- [[Nilpotent Linear Transformation Geometric Characterization]]

### Generalized Minimum Residual as Least Square Estimation

>[!important] Definition
>Let $A\in L(V)$ be a linear map on a $n$-dimensional finite dimensional space $V$ and $b\in V$. The **Krylov subspacae** of *degree* $t$ *generated by* $A$ and $b$ is denoted as $$\mathcal{K}_{t} := \text{span}\left\{ b, \,Ab \,{,}\ldots{,}\, A^{t-1}b\right\} $$
>
>At each iteration $t$, the **generalized minimum residual (GMRES)** method solves a *linear regression problem* on a *Krylov subspace* $\mathcal{K}_{t}$, i.e. 
>$$
>\begin{align*}
>x^{(t)} = \arg\min_{x}\;&\; \lVert Ax - b \rVert_{2}^2 \\[5pt]
>\text{s.t. }& x\in \mathcal{K}_{t} 
>\end{align*}
>$$

- [[Linear Map]]
- [[Linear Regression]]
- [[Least Square Estimation]]

![[Krylov Subspace and Krylov Matrix#^e38b83]]

>[!important] Definition
>Let $b\neq 0 \in \mathbb{R}^{n}$, $A\in \mathbb{R}^{n\times n}$.
>
>The **generalized minimum residual (GMRES)** method solves the following *equality constrained least square problem* at each iteration $t$
>$$
>\begin{align*}
>\min_{x, y}\;&\; \lVert Ax - b \rVert_{2}^2 \\[5pt]
>\text{s.t. }& K(A, b, t)\,y = x
>\end{align*}
>$$
>or equivalently,
>$$
>\begin{align*}
>\min_{y}\;&\; \lVert AK(A, b, t)\,y  - b \rVert_{2}^2 = \lVert K(A, Ab, t)\,y  - b \rVert_{2}^2 
>\end{align*}
>$$
>- This problem is a **least square estimation** problem where the coefficient matrix is $$AK(A, b, t) := K(A, Ab, t) = [Ab,\, A^2b \,{,}\ldots{,}\,A^{t}b]$$

^dc7f60

- [[KKT Matrix and KKT System for Optimization with Equality Constraints]]

### Solving GMRES via QR Factorization on Krylov Matrix

![[Hessenberg Decomposition of Matrix#^40c613]]

- [[Hessenberg Decomposition of Matrix]]

>[!info]
>Let **QR factorization** of **Krylov matrix** be $$K(A, b, t) = Q_{t}R_{t}$$ Then we see that the same $Q$ would reduce $A$ into **unreduced upper Hessenberg matrix** $$Q_{t}^{T}AQ_{t} = H_{t} \quad \iff \quad AQ_{t} = Q_{t}H_{t}$$
>

- [[QR Factorization of Matrix]]

>[!info]
>Through **Arnoldi iteration**, we can find a *sequence of orthonormal basis* $q_{1} \,{,}\ldots{,}\,q_{t},\, q_{t+1} \,{}\ldots{}\,$ so that $$H_{k+1,k}\,q_{k+1} = A q_{k} - \sum_{i=1}^{k}H_{i,k}\,q_{i}$$ where $$Q_{t} = [q_{1}\,{,}\ldots{,}\,q_{t}],\; q_{1} = \frac{b}{\lVert b \rVert }$$
>
>Then
 >$$AQ_{t} = Q_{t+1}\hat{H}_{t}$$

^398f19

- [[Arnoldi Iteration for Hessenberg Reduction of Large Matrix]]

>[!info]
>Moreover, the corresponding least square estimation can be *refomulated* as
>$$
>\begin{align*}
>\lVert AK(A, b, t)\,y  - b \rVert_{2}^2 &= \lVert A\,Q_{t}\,R_{t}\,y  - b \rVert_{2}^2 \\[5pt]
>&:= \lVert A\,Q_{t}\,z  - b \rVert_{2}^2\\[5pt]
>&= \lVert Q_{t+1}\,\hat{H}_{t}\,z  - b \rVert_{2}^2 \\[5pt]
>&= \lVert \hat{H}_{t}\,z  - Q_{t+1}^{T}b \rVert_{2}^2
\end{align*}
>$$
>where
>- $$x := K(A, b, t)y = Q_{t}z, \quad z= R_{t}\,y $$
>- $\hat{H}_{t}$ is the Hessenberg matrix  at the $t$-step of  **Arnoldi process**
>- Note that $$Q_{t+1}^{T}b  = \lVert b \rVert e_{1}$$



>[!important] 
>At iteration $t$, the **GMRES** solve the following *least square problem*
>$$
>\min_{z} \;\lVert \hat{H}_{t}\,z  - \lVert b \rVert e_{1} \rVert_{2}^2
>$$

^1b4e25

### GMRES Algorithm

>[!important] Definition
>Consider the task of solving the **linear system of equations** $$Ax = b$$ where $A\in \mathbb{R}^{n\times n}$ and $b\in \mathbb{R}^{n}$. 
>
>The **generalized minimum residual (GMRES)** method solves above problem by iteratively solving *least square estimation* on an increasing sequence of *nested* *Krylov subspaces* generated by $A$ and $b$ $$\mathcal{K}_{1} \subset \mathcal{K}_{2} \,{\subset}\ldots{\subset}\,\mathcal{K}_{t}.$$ 
>
>Specifically, the algorithm is described as follow
>- *Require*:   $A\in \mathbb{R}^{n\times n}$ and $b\in \mathbb{R}^{n}$.
>- Initialize $q_{1}$ by **normalization** $$q_{1} = \frac{b}{\lVert b \rVert_{2}}$$
>- For $k=1,\,2\,{,}\ldots{,}\,$
>	- Call the **Arnoldi process** to find *Hessenberg matrix* $\hat{H}_{k}$ and *new orthonormal basis* $q_{k+1}$
>		- Compute the vector in the *range of* $A$ $$v = Aq_{k}$$
>		- For $j=1\,{,}\ldots{,}\,k$:
>			- Compute the $(j,k)$ entry of $H$ as the *cosine similarity* between $q_{j}$ and $Aq_{k}$ $$H_{j,k} = \left\langle  q_{j}\,,\,v \right\rangle$$
>			- Compute the residual of $v$ after *projecting* onto the span of first $j$ columns of $Q$ $$v \leftarrow v - H_{j,k}\,q_{j}$$
>		- Set $H_{k+1,k}$ as the *norm* of residual $$H_{k+1,k} = \lVert v \rVert_{2}$$
>		- Collect the **Hessenberg matrix** as $$\hat{H}_{k} = \left[ \begin{array}{ccccc}H_{11} & \cdots & \cdots  &  H_{1m} \\[5pt] H_{21} & H_{22} & \cdots &  \vdots \\[5pt]   & \ddots & \ddots &   \vdots \\[5pt] 0 &    & H_{k,k-1} & H_{k, k}\\[5pt]   &    & & H_{k+1, k}\end{array} \right] = \left[ \begin{array}{cc} \hat{H}_{k-1} & H_{\{ 1:k \}, k} \\ 0 & H_{k+1,k} \end{array} \right]  \in \mathbb{R}^{(k+1)\times k}$$
>		- Set $q_{k+1}$ by *normalization* of *residual*  $$q_{k+1} = \frac{v}{H_{k+1,k}}$$
>	- Solve the **least square problem**: $$z_{k} = \arg\min_{z} \;\lVert \hat{H}_{k}\,z  - \lVert b \rVert e_{1} \rVert_{2}^2$$
>		- $z_{k}\in \mathbb{R}^{k}$
>		- There are many methods to solve above $(k+1)\times k$ least square problem.
>	- Obtain the *solution iterate* of linear system as $$x_{k} = Q_{k}\,z_{k}$$
>- *Return* $x^{*}$ as the solution of linear system. $$x_{k} \to x^{*} = A^{-1}b$$

^ee401b

 
- [[Monotone Sequence of Nested Sets]]
- [[Arnoldi Iteration for Hessenberg Reduction of Large Matrix]]
- [[Least Square Estimation with QR Factorization]]
- [[Algorithms for Least Square Estimation Problem]]

## Explanation

>[!info]
>GMRES can be used as an **iterative method** to solve the **least square problem**
>$$\min_{x} \lVert Ax - b \rVert_{2}^2$$ 
>
>- This method is equivalent to [[Forward Stagewise Additive Modeling]]




-----------
##  Recommended Notes and References

- [[Minimal Residuals Algorithm and MINRES]]
- [[Normal Equations and Newton System of Equations]]



- [[Least Square Estimation with QR Factorization]]


- [[Modified Gram-Schmidt Algorithm for QR Factorization]]
- [[Gram-Schmidt Orthogonalization]]


- [[Jordan Canonical Form Geometric Interpretation]]
- [[Matrix]]

- [[Matrix Analysis by Horn]] 
- [[Numerical Linear Algebra by Trefethen]] pp 266 -275
- [[Matrix Computations by Golub]] pp 642 - 644
- Saad, Y. (2003).Â _Iterative methods for sparse linear systems_. Society for Industrial and Applied Mathematics. pp 157 - 172