---
tags:
  - concept
  - numerical_methods/numerical_linear_algebra
  - math/matrix_analysis
  - optimization/algorithm
keywords:
  - minimal_residual_algorithm
  - minres
topics:
  - numerical_linear_algebra
  - optimization/algorithm
name: Minimal Residuals Algorithm and MINRES
date of note: 2024-10-04
---

## Concept Definition

>[!important]
>**Name**: Minimal Residuals Algorithm and MINRES

### Krylov Subspace

![[Krylov Subspace and Krylov Matrix#^27eea3]]

![[Nilpotent Linear Transformation and Matrix#^a1b57c]]

![[Nilpotent Linear Transformation and Matrix#^ee0cec]]

- [[Krylov Subspace and Krylov Matrix]]
- [[Nilpotent Linear Transformation and Matrix]]
- [[Nilpotent Linear Transformation Geometric Characterization]]

### Krylov Subspace Method

![[Krylov Subspace Methods#^d69683]]

- [[Krylov Subspace Methods]]

>[!important] Definition
>Let $b\neq 0 \in \mathbb{R}^{n}$, $A\in \mathbb{R}^{n\times n}$ be **symmetric**.
>
>The **minimum residual (MINRES)** method solves the following *equality constrained least square problem* at each iteration $t$
>$$
>\begin{align*}
>\min_{x, y}\;&\; \lVert Ax - b \rVert_{2}^2 \\[5pt]
>\text{s.t. }& K(A, b, t)\,y = x
>\end{align*}
>$$
>or equivalently,
>$$
>\begin{align*}
>\min_{y}\;&\; \lVert AK(A, b, t)\,y  - b \rVert_{2}^2 = \lVert K(A, Ab, t)\,y  - b \rVert_{2}^2 
>\end{align*}
>$$
>- This problem is a **least square estimation** problem where the coefficient matrix is $$AK(A, b, t) := K(A, Ab, t) = [Ab,\, A^2b \,{,}\ldots{,}\,A^{t}b]$$

- [[Hermitian or Symmetric Matrix]]
- [[Linear Regression]]
- [[Least Square Estimation]]
- [[Normal Equations and Newton System of Equations]]


### Solving MINRES via Lanczos Process

>[!important] Definition
>Consider the task of solving the **linear system of equations** $$Ax = b$$ where $A\in \mathbb{R}^{n\times n}$ is **symmetric** and $b\in \mathbb{R}^{n}$. 
>
>The **minimum residual (MINRES)** method solves above problem by iteratively solving *least square estimation* on an increasing sequence of *nested* *Krylov subspaces* generated by $A$ and $b$ $$\mathcal{K}_{1} \subset \mathcal{K}_{2} \,{\subset}\ldots{\subset}\,\mathcal{K}_{t}.$$ 
>
>Specifically, the algorithm is described as follow
>- *Require*:   $A\in \mathbb{R}^{n\times n}$ and $b\in \mathbb{R}^{n}$.
>- Initialize $q_{1}$ by **normalization** $$q_{1} = \frac{b}{\lVert b \rVert_{2}}$$
>- Initialize *residual* $r_{0} = 0$, off-diagonal $\beta_{0} = 1$, initial $q_{0} = 0$
>- For $k=1,\,2\,{,}\ldots{,}\,$
>	- Call **Lanczos process** to compute tridiagonal matrix $T_{k}$ and $q_{k+1}$
>		- Compute the vector in the *range of* $A$ $$v = Aq_{k}$$
>			- We can use $$v= Aq_{k} - \beta_{k-1}q_{k-1}$$ to increase *stability*.
>		- Compute the *diagonal term* $$\alpha_{k} = \left\langle  q_{k}\,,\,v    \right\rangle$$
>		- Compute the residual of $v$ after projecting onto the span of $q_{k-1}, q_{k}$ $$v \leftarrow v - \beta_{k-1}\,q_{k-1} - \alpha_{k}\,q_{k}$$
>		- Set *off-diagonal term* $\beta_{k}$ as the *norm* of residual $$\beta_{k+1} = \lVert v \rVert_{2}$$
>		- Set $q_{k+1}$ by *normalization* of residual  $$q_{k+1} = \frac{v}{\beta_{k}}$$
>		- *Terminate the Lanczos process* if $\beta_{k+1} = 0$.
>		- Obtain the **tridiagonal matrix** $$\hat{T}_{k} = \left[ \begin{array}{ccccc}\alpha_{1} & \beta_{1} & & 0 \\[5pt] \beta_{1} & \alpha_{2} & \ddots &   \\[5pt]   & \ddots & \ddots &   \beta_{k-1} \\[5pt]  &    & \beta_{k-1} & \alpha_{k}\\[5pt]  0 &    & & \beta_{k}\end{array} \right] = \left[ \begin{array}{cc} \hat{T}_{k-1} & \beta_{k-1}e_{k-1} + \alpha_{k}e_{k} \\ 0 & \beta_{k} \end{array} \right]  \in \mathbb{R}^{(k+1)\times k}$$
>	- Solve the **least square problem**: $$z_{k} = \arg\min_{z} \;\lVert \hat{T}_{k}\,z  - \lVert b \rVert e_{1} \rVert_{2}^2$$
>		- $z_{k}\in \mathbb{R}^{k}$
>		- There are many methods to solve above $(k+1)\times k$ least square problem.
>	- Obtain the *solution iterate* of linear system as $$x_{k} = Q_{k}\,z_{k}$$
>- *Return* $x^{*}$ as the solution of symmetric linear system. $$x_{k} \to x^{*} = A^{-1}b$$

^ac4c95

- [[Lanczos Iteration for Tridiagonal Reduction of Large Matrix]]




## Explanation

>[!info]
>**MIRES** is the symmetric version of **GMRES**, where **Lanczos process** is used instead of **Arnoldi process.**

- [[GMRES as Regression on Krylov Space]]

## Conjugate Gradient

![[Conjugate Gradient Algorithm Lanczos#^87babe]]

- [[Conjugate Gradient Algorithm Lanczos]]

-----------
##  Recommended Notes and References



- [[Least Square Estimation with QR Factorization]]


- [[Modified Gram-Schmidt Algorithm for QR Factorization]]
- [[Gram-Schmidt Orthogonalization]]

- [[Matrix Analysis by Horn]] 
- [[Numerical Linear Algebra by Trefethen]] pp 266 -275
- [[Matrix Computations by Golub]] pp 639 - 641