---
tags:
  - concept
  - optimization/algorithm
  - optimization/theory
keywords:
  - conjugate_vector_positive_definite
topics:
  - optimization/algorithm
name: Conjugate Gradient Algorithm Convergence Analysis
date of note: 2024-05-12
---

## Concept Definition

>[!important]
>**Name**: Conjugate Gradient Algorithm Convergence Analysis

![[Conjugate Gradient Algorithm Linear#^48ebc6]]

- [[Conjugate Gradient Algorithm Linear]]

### Convergence of Conjugate Direction Method

>[!important] Proposition
>For any $x_{0}\in \mathbb{R}^n$, the sequence $\{ x_{n} \}$ generated by the **conjugate directions** $\{ d_{k} \}$,  $$x_{k+1} = x_{k} + \alpha_{k}\,d_{k}$$ and $$\alpha_{k} = - \frac{r_{k}^T\,d_{k}}{d_{k}^T\,A\,d_{k}}$$ **converges** to the *solution* of system of linear equations $$Ax = b$$ in **at most $n$ step.** 

- [[Numerical Optimization by Nocedal]] pp 103

>[!info]
>Note that conjugate directions $\{ d_{k} \}$ are **linearly independent**. Thus it forms a basis of $\mathbb{R}^n$. Therefore the difference can be represented as a *finite linear combination* of conjugate direction vectors
>$$
>x^{*} - x_{0} = \sum_{k=0}^{n-1}\sigma_{k}\;d_{k}
>$$
>We just need to show that $\sigma_{k} = \alpha_{k}.$

- [[Conjugate Vector with respect to Positive Definite Transformation]]

### Minimization over Expanding Subspace

>[!important] Proposition
>Let $x_{0}\in \mathbb{R}^n$ be any starting point and suppose that the sequence $\{ x_{k} \}$ is generated by **conjugate direction algorithm**. Then the *residual* is **orthogonal** to all *conjugate directions*
>$$
>\left\langle  d_{i}\,,\, r_{k} \right\rangle = 0, \quad i=0, 1 \,{,}\ldots{,}\,k-1,
>$$
>and $x_{k}$ is the **minimizer** of $$\phi(x) = \frac{1}{2} x^T\,A\,x - b^T\,x$$ over the *affine subspace* $$x_{0} + \text{span}\left\{ d_{0}, d_{1} \,{,}\ldots{,}\,d_{k-1} \right\}.$$

- [[Affine Combination]]
- [[Krylov Subspace]]
- [[Numerical Optimization by Nocedal]] pp 106

## Explanation



## Rate of Convergence


>[!info]
>Note that 
>$$
>\begin{align*}
>x_{k+1} &= x_{0} + \alpha_{0}\,d_{0} + \alpha_{1}\,d_{1}  \,{+}\ldots{+}\, \alpha_{k} d_{k} \\[5pt]
>&= x_{0} + \gamma_{0}\,r_{0} + \gamma_{1}\,A\,r_{0}  \,{+}\ldots{+}\, \gamma_{k} A^{k}r_{0} \\[5pt]
>&:= x_{0} + P_{k}(A)\,r_{0}
\end{align*}
>$$
>where 
>$$
> P_{k}(A) = \gamma_{0}\,I + \gamma_{1}\,A\,  \,{+}\ldots{+}\, \gamma_{k} A^{k}
>$$

>[!info]
>The CG method can be seen as **minimizing the distance** between optimal and current direction for **all possible polynomial of degree $k$**
>
>$$
> \min_{P_{k}}\; \lVert x_{0} + P_{k}(A)\,r_{0} - x^{*}  \rVert_{A}^2 
>$$
>
>Let $\{\lambda_{k}\}$ be *eigenvalue* of $A$.  Then
>$$
>\begin{align*}
>\lVert x_{0} + P_{k}(A)\,r_{0} - x^{*}  \rVert_{A}^2 = \sum_{i=1}^{n}\lambda_{i}\left(1 + \lambda_{i}\;P_{k}(\lambda_{i})\right)^2\,\xi_{i}^2,
>\end{align*}
>$$
>where $v_{i}$ is the *eigenvector* of $A$ associated with $\lambda_{i}$ and $\{ \xi_{i} \}$ are linear coefficients for displacement vector under $\{ v_{i} \}$ $$x_{*} - x_{0} = \sum_{i=1}^{n}\xi_{i}v_{i}.$$ 
>
>We obtain that 
>$$
>\begin{align*}
>\lVert x_{k+1}- x^{*}  \rVert_{A}^2  &\le \min_{P_{k}}\max_{1 \le i \le n} \left(1 + \lambda_{i}\;P_{k}(\lambda_{i})\right)^2\, \left(\sum_{i=1}^{n}\lambda_{i}\xi_{i}^2\right)\\
>& =  \min_{P_{k}}\max_{1 \le i \le n} \left(1 + \lambda_{i}\;P_{k}(\lambda_{i})\right)^2\,\lVert x_{0} - x^{*} \rVert 
>\end{align*}
>$$

>[!important]
>Thus the key is to estimate
>$$
>\min_{P_{k}}\max_{1 \le i \le n} \left(1 + \lambda_{i}\;P_{k}(\lambda_{i})\right)^2
>$$
>where  $\{\lambda_{k}\}$ is the *eigenvalue* of $A$.

>[!important] Theorem
>If $A$ has only $r$ **distinct eigenvalues**, then the **CG iteration** will terminate at the *solution* in **at most $r$ iterations**.

- [[Eigenvalue and Eigenvector for Linear Map]]
- [[Numerical Optimization by Nocedal]] pp 115

>[!important] Theorem
>If $A$ has **eigenvalues** $\lambda_{1} \le \lambda_{2} \,{\le}\ldots{\le}\,\lambda_{n}$, then the CG iterates have that
>$$
>\lVert x_{k+1} - x^{*}  \rVert_{A}^2 \le \left(\frac{\lambda_{n-k} + \lambda_{1}}{\lambda_{n-k} - \lambda_{1}}\right)^2\;\lVert x_{0} - x^{*} \rVert_{A}^2  
>$$

- [[Contraction Function]]
- [[Contraction Map Principle]]

>[!important]
>Define the **condition number** $$\kappa(A) = \frac{\max \lambda_{i}}{\min \lambda_{i}} = \lVert A \rVert_{2}\; \lVert A^{-1} \rVert_{2}.$$
>
>We have that an approximate error
>$$
>\lVert x_{k+1} - x^{*}  \rVert_{A} \le 2\left(\frac{\sqrt{ \kappa(A) } - 1}{\sqrt{ \kappa(A) } + 1}\right)^k\;\lVert x_{0} - x^{*} \rVert_{A}.
>$$




-----------
##  Recommended Notes and References

- [[Conjugate Vector with respect to Positive Definite Transformation]]
- [[Krylov Subspace]]

- [[Conjugate Gradient Algorithm Nonlinear]]
- [[Newton Method]]

- [[Numerical Optimization by Nocedal]] pp 103
- [[Nonlinear Programming by Bertsekas]] pp 130