---
tags:
  - concept
  - statistics/monte_carlo_simulation
  - statistics/estimation
  - machine_learning/theory
  - deep_learning/algorithms
keywords:
  - partition_function
  - importance_sampling
  - annealed_importance_sampling
topics:
  - monte_carlo
  - statistics/monte_carlo
name: Estimating Partition Function via Importance Sampling
date of note: 2024-09-12
---

## Concept Definition

>[!important]
>**Name**: Estimating Partition Function via Importance Sampling

>[!important] 
>Let $X \in \mathcal{X}$ be a random variable with p.d.f. $$p(x) = \frac{\tilde{p}(x)}{Z_{p}}$$ where $Z_{p}$ is the *partition function* and $\tilde{p}(x)$ is the unnormalized density. 
>
>We wish to use an **important sampling distribution** $$q(x) = \frac{\tilde{q}(x)}{Z_{q}}$$ to estimate the *expectation* 
>$$
>\mathbb{E}_{ p }\left[  f(X) \right] = \int_{\mathcal{X}} f(x)\,p(x)\,dx.
>$$
>
>In particular, we have
>$$
>\begin{align*}
>\mathbb{E}_{ p }\left[  f(X) \right] &= \int_{\mathcal{X}} f(x)\,p(x)\,dx \\[5pt]
>&= \int_{\mathcal{X}} f(x)\,\frac{p(x)}{q(x)}\,q(x)\,dx \\[5pt]
>&= \frac{Z_{q}}{Z_{p}} \int_{\mathcal{X}} f(x)\,\frac{\tilde{p}(x)}{\tilde{q}(x)}\,q(x)\,dx \\[5pt]
>&:= \frac{Z_{q}}{Z_{p}} \int_{\mathcal{X}} f(x)\,w(x)\,q(x)\,dx 
>\end{align*}
>$$
>where the **importance weight** is defined as
>$$
>w(x) = \frac{\tilde{p}(x)}{\tilde{q}(x)}.
>$$
>
>Now let $f\equiv 1$, we have
>$$
>1 = \mathbb{E}_{ p }\left[  f(X) \right] = \frac{Z_{q}}{Z_{p}} \int_{\mathcal{X}} \,w(x)\,q(x)\,dx 
>$$
>Hence, the **ratio of partition function** can be computed as
>$$
>\frac{Z_{p}}{Z_{q}} = \int_{\mathcal{X}} \,w(x)\,q(x)\,dx 
>$$

- [[Importance Sampling]]
- [[Probability Density Function of Random Variable]]

>[!important] Definition
>The **Importance Sampling Algorithm** to estimate the **ratio of partition function** is described as below:
>- Draw $$X_1, \ldots, X_{m} \sim q(x) = \frac{\tilde{q}(x)}{Z_{q}}$$ where $q$ is **trial p.d.f.**;
>- Calculate **the importance weight** $w(x)$:
>$$
> \begin{align*}
> w_i := w(X_i) &= \frac{\tilde{p}(X_i)}{\tilde{q}(X_i)}, \quad i=1,\ldots, m
> \end{align*}
>$$ 
>- **Approximate** $$\frac{Z_{p}}{Z_{q}}$$ by the *sum of importance weights* $$\frac{Z_{p}}{Z_{q}} \approx \frac{1}{m}\sum_{i=1}^{m} w_{i}$$ 

### Annealed Importance Sampling to Approximate Partition Function


>[!important] Definition
>Consider a *sequence of distributions* $\{ p_{\eta_{i}}, i=1\,{,}\ldots{,}\,n \}$ lies in a smooth path between *proposal distribution* $p_{0}$ and *target distribution*  $p_{1}$ in *information manifold* $\mathscr{P}$. 
>- Each distribution can be generated by *weighted geometric mean* between the $p_{0}$ and $p_{1}$ as $$p_{\eta_{i}} \propto p_{1}^{\eta_{i}}\, p_{0}^{1- \eta_{i}}, \quad 0=\eta_{0} \le \eta_{1} \,{\le}\ldots{\le}\, \eta_{n} = 1$$
>- Assume that $$p_{0}(x) = \frac{\tilde{p}_{0}(x)}{Z_{0}}, \quad p_{1}(x) = \frac{\tilde{p}_{1}(x)}{Z_{1}}$$
>- Assume that $p_{\eta_{j}}$ is the **invariant measure** of a Markov chain with *transition kernel* $K_{\eta_{j}}$ i.e. $$p_{\eta_{j}}(x) = \int_{\mathcal{X}}\, p_{\eta_{j}}(x')\,K_{\eta_{j}}(x', x) dx'$$
>  
>Then the *ratio* of the *partition function* of target distribution over *partition function* of proposal distribution can be computed by
>$$
> \frac{Z_{1}}{Z_{0}} = \prod_{j=0}^{n-1}\frac{Z_{\eta_{j+1}}}{Z_{\eta_{j}}}
>$$

- [[Invariant Measure and Stationary Distribution]]
- [[e-Connection and m-Connection on Statistical Manifold]]

>[!important] Definition
>The **annealed importance sampling (AIS)** strategy to estimate the ratio of partition functions is described as below:
>- *Require*: the **target distribution** $$p_{1}(x) = \frac{\tilde{p}_{1}(x)}{Z_{1}}$$
>- *Require*: the initial proposal distribution $$p_{0}(x) = \frac{\tilde{p}_{0}(x)}{Z_{0}}$$ 
>- *Require*: a sequence of distributions $$p_{\eta_{i}} \propto p_{1}^{\eta_{i}}\, p_{0}^{1- \eta_{i}}, \quad 0=\eta_{0} \le \eta_{1} \,{\le}\ldots{\le}\, \eta_{n} = 1$$
>- *Require*: a sequence of **transition kernels** $K_{\eta_{i}}$ that have $p_{\eta_{i}}$ as *invariant measure*
>- For $t=1 \,{,}\ldots{,}\,T$
>	- **Generate** sample from initial proposal distribution $$X_{\eta_{1}}^{(t)} \sim p_{0}(x)$$
>	- For $j=1 \,{,}\ldots{,}\,n$
>		- **Generate** sample given transition kernel $K_{\eta_{i}}$ and previous sample $$X_{\eta_{j}}^{(t)} \sim K_{\eta_{j}}\left(X_{\eta_{j-1}}^{(t)}\,,\, \cdot\right)$$ 
>			- Use **markov chain Monte Carlo methods**, that generate a Markov chain $$X_{\eta_{j}, k}^{(t)} \to X_{\eta_{j}}^{(t)},\; k\to \infty $$ that induced by $K_{\eta_{i}}$ to sample
>		- Compute a **factor** of **importance weight** as   $$w_{\eta_{j}}^{(t)} := \frac{\tilde{p}_{\eta_{j}}(X_{\eta_{j}}^{(t)})}{\tilde{p}_{\eta_{j-1}}(X_{\eta_{j}}^{(t)})}$$
>	- Compute the **product of factors** to form the **importance weight** as $$w^{(t)} = \prod_{j=1}^{n}w_{\eta_{j}}^{(t)}$$
>- The **ratio** of $Z_{1}$ over $Z_{0}$ is estimated by the **sample mean** of all *weights* $$\frac{Z_{1}}{Z_{0}} \approx \frac{1}{T} \sum_{t=1}^{T}w^{(t)}.$$

- [[Annealed Importance Sampling]]

## Explanation

>[!quote]
>We have focused our presentation here on the problem of generating samples from a distribution so as to estimate the posterior. However, another task of significant practical importance is that of **computing the partition function** of an **unnormalized measure**. As we will see, this task is of particular importance in learning, since the normalizing constant is often used as a **measure of the overall quality** of a learned model. Computation of a partition function by directly sampling the distribution leads to estimates of *high variance*, since such estimates are usually dominated by a small number of samples with high unnormalized probabilities. In order to avoid this problem, a ratio of partition functions is computed; see exercise 12.8. An equivalent problem of **free energy difference**, or **partition function ratio**, has been tackled in computational physics and computation chemistry communities with a range of methods, including **free energy perturbation**, **thermodynamic integration**, **bridge sampling**, **umbrella sampling**, and Jarzynski equation; these methods are in essence *importance-sampling algorithms*.
>
>-- [[Probabilistic Graphical Models by Koller]] pp 543 - 544

>[!quote]
>In situations where $\mathbb{KL}\left( p_{0} \left\|\right. p_{1} \right)$ is large (i.e., where there is *little overlap* between $p_0$ and $p_{1}$), a strategy called **annealed importance sampling (AIS)** attempts to bridge the gap by introducing *intermediate distributions* (Jarzynski, 1997; Neal, 2001).
>
>This approach enables us to estimate the partition function of a multimodal distribution defined over a *high-dimensional space* (such as the distribution defined by a trained RBM). We begin with a **simpler** model with a **known partition function** (such as an RBM with zeros for weights) and estimate the **ratio between the two modelâ€™s partition functions**. The estimate of this ratio is based on the estimate of the ratios of a sequence of many similar distributions, such as the sequence of RBMs with weights interpolating between zero and the learned weights.
>
>-- [[Deep Learning by Goodfellow]] pp 617







-----------
##  Recommended Notes and References


- [[Annealed Importance Sampling]]
- [[Importance Sampling]]
- [[Sequential Importance Sampling]]
- [[Markov Chain Monte Carlo Methods]]


- [[Log-Partition Function of Exponential Family]]
- [[Log-Partition Function and Score Function of Graphical Models]]
- [[Partition Function for AdaBoost]]
- [[Gibbs Measure and Energy-based Model]]
- [[Probabilistic Graphical Models]]


- [[Deep Learning Foundations and Concepts by Bishop]] pp 437 - 439
- [[Deep Learning by Goodfellow]] pp 614 - 616
- [[Probabilistic Graphical Models by Koller]] pp 543 - 544