---
tags:
  - concept
  - statistics/monte_carlo_simulation
  - statistics/estimation
  - machine_learning/theory
  - deep_learning/algorithms
keywords: 
topics: 
name: Estimating Partition Function via Importance Sampling
date of note: 2024-09-12
---

## Concept Definition

>[!important]
>**Name**: Estimating Partition Function via Importance Sampling

>[!important] 
>Let $X \in \mathcal{X}$ be a random variable with p.d.f. $$p(x) = \frac{\tilde{p}(x)}{Z_{p}}$$ where $Z_{p}$ is the *partition function* and $\tilde{p}(x)$ is the unnormalized density. 
>
>We wish to use an **important sampling distribution** $$q(x) = \frac{\tilde{q}(x)}{Z_{q}}$$ to estimate the *expectation* 
>$$
>\mathbb{E}_{ p }\left[  f(X) \right] = \int_{\mathcal{X}} f(x)\,p(x)\,dx.
>$$
>
>In particular, we have
>$$
>\begin{align*}
>\mathbb{E}_{ p }\left[  f(X) \right] &= \int_{\mathcal{X}} f(x)\,p(x)\,dx \\[5pt]
>&= \int_{\mathcal{X}} f(x)\,\frac{p(x)}{q(x)}\,q(x)\,dx \\[5pt]
>&= \frac{Z_{q}}{Z_{p}} \int_{\mathcal{X}} f(x)\,\frac{\tilde{p}(x)}{\tilde{q}(x)}\,q(x)\,dx \\[5pt]
>&:= \frac{Z_{q}}{Z_{p}} \int_{\mathcal{X}} f(x)\,w(x)\,q(x)\,dx 
>\end{align*}
>$$
>where the **importance weight** is defined as
>$$
>w(x) = \frac{\tilde{p}(x)}{\tilde{q}(x)}.
>$$
>
>Now let $f\equiv 1$, we have
>$$
>1 = \mathbb{E}_{ p }\left[  f(X) \right] = \frac{Z_{q}}{Z_{p}} \int_{\mathcal{X}} \,w(x)\,q(x)\,dx 
>$$
>Hence, the **ratio of partition function** can be computed as
>$$
>\frac{Z_{p}}{Z_{q}} = \int_{\mathcal{X}} \,w(x)\,q(x)\,dx 
>$$

- [[Importance Sampling]]
- [[Probability Density Function of Random Variable]]

>[!important] Definition
>The **Importance Sampling Algorithm** to estimate the **ratio of partition function** is described as below:
>- Draw $$X_1, \ldots, X_{m} \sim q(x) = \frac{\tilde{q}(x)}{Z_{q}}$$ where $q$ is **trial p.d.f.**;
>- Calculate **the importance weight** $w(x)$:
>$$
> \begin{align*}
> w_i := w(X_i) &= \frac{\tilde{p}(X_i)}{\tilde{q}(X_i)}, \quad i=1,\ldots, m
> \end{align*}
>$$ 
>- **Approximate** $$\frac{Z_{p}}{Z_{q}}$$ by the *sum of importance weights* $$\frac{Z_{p}}{Z_{q}} \approx \frac{1}{m}\sum_{i=1}^{m} w_{i}$$ 

### Annealed Importance Sampling to Approximate Partition Function

>[!important] Definition
>Consider a *sequence of distributions* $\{ p_{\eta_{i}}, i=1\,{,}\ldots{,}\,n \}$ lies in a smooth path between *proposal distribution* $p_{0}$ and *target distribution*  $p_{1}$ in *information manifold* $\mathscr{P}$. 
>- Each distribution can be generated by *weighted geometric mean* between the $p_{0}$ and $p_{1}$ as $$p_{\eta_{i}} \propto p_{1}^{\eta_{i}}\, p_{0}^{1- \eta_{i}}, \quad 0=\eta_{0} \le \eta_{1} \,{\le}\ldots{\le}\, \eta_{n} = 1$$
>- Assume that $$p_{0}(x) = \frac{\tilde{p}_{0}(x)}{Z_{0}}, \quad p_{1}(x) = \frac{\tilde{p}_{1}(x)}{Z_{1}}$$
>- Assume that $p_{\eta_{j}}$ is the **invariant measure** of a Markov chain with *transition kernel* $K_{\eta_{j}}$ i.e. $$p_{\eta_{j}}(x) = \int_{\mathcal{X}}\, p_{\eta_{j}}(x')\,K_{\eta_{j}}(x', x) dx'$$
>  
>Then the *ratio* of the *partition function* of target distribution over *partition function* of proposal distribution can be computed by
>$$
> \frac{Z_{1}}{Z_{0}} = \prod_{j=0}^{n-1}\frac{Z_{\eta_{j+1}}}{Z_{\eta_{j}}}
>$$

- [[Invariant Measure and Stationary Distribution]]
- [[e-Connection and m-Connection]]

>[!important] Definition
>The **annealed importance sampling (AIS)** strategy to estimate the ratio of partition functions is described as below:
>- *Require*: the **target distribution** $$p_{1}(x) = \frac{\tilde{p}_{1}(x)}{Z_{1}}$$
>- *Require*: the initial proposal distribution $$p_{0}(x) = \frac{\tilde{p}_{0}(x)}{Z_{0}}$$ 
>- *Require*: a sequence of distributions $$p_{\eta_{i}} \propto p_{1}^{\eta_{i}}\, p_{0}^{1- \eta_{i}}, \quad 0=\eta_{0} \le \eta_{1} \,{\le}\ldots{\le}\, \eta_{n} = 1$$
>- *Require*: a sequence of **transition kernels** $K_{\eta_{i}}$ that have $p_{\eta_{i}}$ as *invariant measure*
>- For $t=1 \,{,}\ldots{,}\,T$
>	- **Generate** sample from initial proposal distribution $$X_{\eta_{1}}^{(t)} \sim p_{0}(x)$$
>	- For $j=1 \,{,}\ldots{,}\,n$
>		- **Generate** sample given transition kernel $K_{\eta_{i}}$ and previous sample $$X_{\eta_{j}}^{(t)} \sim K_{\eta_{j}}\left(X_{\eta_{j-1}}^{(t)}\,,\, \cdot\right)$$ 
>			- Use **markov chain Monte Carlo methods**, that generate a Markov chain $$X_{\eta_{j}, k}^{(t)} \to X_{\eta_{j}}^{(t)},\; k\to \infty $$ that induced by $K_{\eta_{i}}$ to sample
>		- Compute a **factor** of **importance weight** as   $$w_{\eta_{j}}^{(t)} := \frac{\tilde{p}_{\eta_{j}}(X_{\eta_{j}}^{(t)})}{\tilde{p}_{\eta_{j-1}}(X_{\eta_{j}}^{(t)})}$$
>	- Compute the **product of factors** to form the **importance weight** as $$w^{(t)} = \prod_{j=1}^{n}w_{\eta_{j}}^{(t)}$$
>- The **ratio** of $Z_{1}$ over $Z_{0}$ is estimated by the **sample mean** of all *weights* $$\frac{Z_{1}}{Z_{0}} \approx \frac{1}{T} \sum_{t=1}^{T}w^{(t)}.$$

- [[Metropolis-Hastings Algorithm]]
- [[Sum-Product Variable Elimination]]


## Explanation





-----------
##  Recommended Notes and References


- [[Annealed Importance Sampling]]
- [[Importance Sampling]]
- [[Sequential Importance Sampling]]


- [[Log-Partition Function of Exponential Family]]
- [[Log-Partition Function and Score Function of Graphical Models]]
- [[Partition Function for AdaBoost]]
- [[Gibbs Measure and Energy-based Model]]



- [[Deep Learning Foundations and Concepts by Bishop]] pp 437 - 439
- [[Deep Learning by Goodfellow]] pp 614 - 616