---
tags:
  - concept
  - statistics/monte_carlo_simulation
  - optimization/algorithm
keywords:
  - annealed_importance_sampling
topics:
  - statistics/monte_carlo
name: Annealed Importance Sampling
date of note: 2024-08-27
---

## Concept Definition

>[!important]
>**Name**: Annealed Importance Sampling


>[!important] Definition
>Consider a *sequence of distributions* $\{ p_{\eta_{i}}, i=1\,{,}\ldots{,}\,n \}$ lies in a smooth path between *proposal distribution* $p_{0}$ and *target distribution*  $p_{1}$ in *information manifold* $\mathscr{P}$. 
>- Each distribution can be generated by *weighted geometric mean* between the $p_{0}$ and $p_{1}$ as $$p_{\eta_{i}} \propto p_{1}^{\eta_{i}}\, p_{0}^{1- \eta_{i}}, \quad 0=\eta_{0} \le \eta_{1} \,{\le}\ldots{\le}\, \eta_{n} = 1$$
>- Assume that $$p_{0}(x) = \frac{\tilde{p}_{0}(x)}{Z_{0}}, \quad p_{1}(x) = \frac{\tilde{p}_{1}(x)}{Z_{1}}$$
>- Assume that $p_{\eta_{j}}$ is the **invariant measure** of a Markov chain with *transition kernel* $K_{\eta_{j}}$ i.e. $$p_{\eta_{j}}(x) = \int_{\mathcal{X}}\, p_{\eta_{j}}(x')\,K_{\eta_{j}}(x', x) dx'$$
>  
>Then the *ratio* of the *partition function* of target distribution over *partition function* of proposal distribution can be computed by
>$$
> \frac{Z_{1}}{Z_{0}} = \prod_{j=0}^{n-1}\frac{Z_{\eta_{j+1}}}{Z_{\eta_{j}}}
>$$

- [[Invariant Measure and Stationary Distribution]]
- [[e-Connection and m-Connection]]

>[!important] Definition
>The **annealed importance sampling (AIS)** strategy to estimate the ratio of partition functions is described as below:
>- *Require*: the **target distribution** $$p_{1}(x) = \frac{\tilde{p}_{1}(x)}{Z_{1}}$$
>- *Require*: the initial proposal distribution $$p_{0}(x) = \frac{\tilde{p}_{0}(x)}{Z_{0}}$$ 
>- *Require*: a sequence of distributions $$p_{\eta_{i}} \propto p_{1}^{\eta_{i}}\, p_{0}^{1- \eta_{i}}, \quad 0=\eta_{0} \le \eta_{1} \,{\le}\ldots{\le}\, \eta_{n} = 1$$
>- *Require*: a sequence of **transition kernels** $K_{\eta_{i}}$ that have $p_{\eta_{i}}$ as *invariant measure*
>- For $t=1 \,{,}\ldots{,}\,T$
>	- **Generate** sample from initial proposal distribution $$X_{\eta_{1}}^{(t)} \sim p_{0}(x)$$
>	- For $j=1 \,{,}\ldots{,}\,n$
>		- **Generate** sample given transition kernel $K_{\eta_{i}}$ and previous sample $$X_{\eta_{j}}^{(t)} \sim K_{\eta_{j}}\left(X_{\eta_{j-1}}^{(t)}\,,\, \cdot\right)$$ 
>			- Use **markov chain Monte Carlo methods**, that generate a Markov chain $$X_{\eta_{j}, k}^{(t)} \to X_{\eta_{j}}^{(t)},\; k\to \infty $$ that induced by $K_{\eta_{i}}$ to sample
>		- Compute a **factor** of **importance weight** as   $$w_{\eta_{j}}^{(t)} := \frac{\tilde{p}_{\eta_{j}}(X_{\eta_{j}}^{(t)})}{\tilde{p}_{\eta_{j-1}}(X_{\eta_{j}}^{(t)})}$$
>	- Compute the **product of factors** to form the **importance weight** as $$w^{(t)} = \prod_{j=1}^{n}w_{\eta_{j}}^{(t)}$$
>- The **ratio** of $Z_{1}$ over $Z_{0}$ is estimated by the **sample mean** of all *weights* $$\frac{Z_{1}}{Z_{0}} \approx \frac{1}{T} \sum_{t=1}^{T}w^{(t)}.$$

^9aace9

- [[Metropolis-Hastings Algorithm]]
- [[Sum-Product Variable Elimination]]
- [[Estimating Partition Function via Importance Sampling]]



## Explanation

>[!quote]
>In situations where $\mathbb{KL}\left( p_{0} \left\|\right. p_{1} \right)$ is large (i.e., where there is *little overlap* between $p_0$ and $p_{1}$), a strategy called **annealed importance sampling (AIS)** attempts to bridge the gap by introducing *intermediate distributions* (Jarzynski, 1997; Neal, 2001).
>
>This approach enables us to estimate the partition function of a multimodal distribution defined over a *high-dimensional space* (such as the distribution defined by a trained RBM). We begin with a **simpler** model with a **known partition function** (such as an RBM with zeros for weights) and estimate the **ratio between the two model’s partition functions**. The estimate of this ratio is based on the estimate of the ratios of a sequence of many similar distributions, such as the sequence of RBMs with weights interpolating between zero and the learned weights.
>
>-- [[Deep Learning by Goodfellow]] pp 617



-----------
##  Recommended Notes and References


- [[Estimating Partition Function via Importance Sampling]]

- [[Importance Sampling]]
- [[Simulated Annealing]]
- [[Sequential Importance Sampling]]
- [[Markov Chain Monte Carlo Methods]]
- [[Gibbs Sampling]]
- [[Metropolis-Hastings Algorithm]]


- [[Log-Partition Function and Score Function of Graphical Models]]
- [[Gibbs Measure and Energy-based Model]]


- [[Deep Learning by Goodfellow]] pp 617
- [[Probabilistic Machine Learning Advanced Topics by Murphy]]  pp 533 - 536
- [[Probabilistic Graphical Models by Koller]] pp 543, 548
- Neal, R. M. (2001). Annealed importance sampling. _Statistics and Computing_, _11_(2), 125–139. [https://doi.org/10.1023/A:1008923215028](https://doi.org/10.1023/A:1008923215028)