---
tags:
  - concept
  - statistics/monte_carlo_simulation
  - optimization/algorithm
keywords:
  - annealed_importance_sampling
topics:
  - statistics/monte_carlo
name: Annealed Importance Sampling
date of note: 2024-08-27
---

## Concept Definition

>[!important]
>**Name**: Annealed Importance Sampling

![[Importance Sampling#^1fa755]]

- [[Importance Sampling]]


>[!important] Definition
>The goal of **anneal importance sampling (AIS)** is to estimate the *expectation* of some function $f$ with respect to a **target distribution** $p_{1}$ 
>$$
> \mathbb{E}_{ p_{1} }\left[  f(X) \right]
>$$
>via a sequence of a *sequence of distributions* $\{ p_{\eta_{i}}, i=1\,{,}\ldots{,}\,n \}$, which lies in a smooth path between *a simple initial proposal distribution* $p_{0}$ and *target distribution*  $p_{1}$ in *information manifold* $\mathscr{P}$. 
>- Each distribution can be generated by *weighted geometric mean* between the $p_{0}$ and $p_{1}$ as $$p_{\eta_{i}} \propto p_{1}^{\eta_{i}}\, p_{0}^{1- \eta_{i}}, \quad 0=\eta_{0} \le \eta_{1} \,{\le}\ldots{\le}\, \eta_{n} = 1$$
>- Assume that $$p_{0}(x) = \frac{\tilde{p}_{0}(x)}{Z_{0}}, \quad p_{1}(x) = \frac{\tilde{p}_{1}(x)}{Z_{1}}$$ where $\tilde{p}_{i}$ is the **unnormalized distributions**
>- Assume that $p_{\eta_{j}}$ is the **invariant measure** of a Markov chain with *transition kernel* $K_{\eta_{j}}$ i.e. $$p_{\eta_{j}}(x) = \int_{\mathcal{X}}\, p_{\eta_{j}}(x')\,K_{\eta_{j}}(x', x) dx'$$
>	- Note that using **Markov chain Monte Carlo methods** such as *Metropolis-Hastings algorithm* with transition kernel $K_{\eta_{j}}(x, x') = K_{\eta_{j}}(x', x)\,$
>  
>  
>Consider 
>- The *time-reversal process* $\{ X_{\eta_{t}}, t=1\,{,}\ldots{,}\,n \}$ where $$X_{n-t} \sim \tilde{K}_{\eta_{t}}(X_{n-t+1}, \cdot), \quad t=1\,{,}\ldots{,}\,n.$$ where the transition kernel of reverse chain is $$\tilde{K}_{\eta_{t}}(x',x) = K_{\eta_{t}}(x, x') \frac{p_{\eta_{t}}(x)}{p_{\eta_{t}}(x')}$$
>- the stochastic process $\{ X_{t}, t=0\,{,}\ldots{,}\, n\}$ where $$X_{t} \sim K_{\eta_{t}}(X_{t-1}, \cdot), \quad t=1\,{,}\ldots{,}\,n.$$
>
>Then the *joint distribution* of Markov chain $\{ X_{t}, t=0\,{,}\ldots{,}\, n\}$ and its time reverse $\{ Y_{t}, t=0\,{,}\ldots{,}\,n \}$  are given by
>- $$p\left(x_{0} \,{,}\ldots{,}\,x_{n}\right) = p_{0}(x_{0})\prod_{j=1}^{n}K_{\eta_{j}}(x_{j-1}, x_{j})$$
>- $$q\left(x_{0} \,{,}\ldots{,}\,x_{n}\right) = p_{1}(x_{n})\prod_{k=1}^{n}\tilde{K}_{\eta_{k-1}}(x_{k}, x_{k-1})$$
>  
>The principle behind the **annealed importance sampling** is 
>$$
> \mathbb{E}_{ p_{1} }\left[  f(X) \right] = \int \frac{p\left(x_{0} \,{,}\ldots{,}\,x_{n}\right)}{q\left(x_{0} \,{,}\ldots{,}\,x_{n}\right)}f(x_{n}) dx_{0}\,{}\ldots{}\,d x_{n}
>$$

- [[Time-Reversible Markov Chain]]



>[!important] Definition
>The **annealed importance sampling (AIS)** strategy to estimate the ratio of partition functions is described as below:
>- *Require*: the **target distribution** $$p_{1}(x) = \frac{\tilde{p}_{1}(x)}{Z_{1}}$$
>- *Require*: the initial proposal distribution $$p_{0}(x) = \frac{\tilde{p}_{0}(x)}{Z_{0}}$$ 
>- *Require*: the integrand $f$
>- *Require*: a sequence of distributions $$p_{\eta_{i}} \propto p_{1}^{\eta_{i}}\, p_{0}^{1- \eta_{i}}, \quad 0=\eta_{0} \le \eta_{1} \,{\le}\ldots{\le}\, \eta_{n} = 1$$
>- *Require*: a sequence of **transition kernels** $K_{\eta_{i}}$ that have $p_{\eta_{i}}$ as *invariant measure*
>- For $t=1 \,{,}\ldots{,}\,T$
>	- **Generate** sample from initial proposal distribution $$X_{\eta_{1}}^{(t)} \sim p_{0}(x)$$
>	- For $j=1 \,{,}\ldots{,}\,n$
>		- **Generate** sample given transition kernel $K_{\eta_{i}}$ and previous sample $$X_{\eta_{j}}^{(t)} \sim K_{\eta_{j}}\left(X_{\eta_{j-1}}^{(t)}\,,\, \cdot\right)$$ 
>			- Use **markov chain Monte Carlo methods**, that generate a Markov chain $$X_{\eta_{j}, k}^{(t)} \to X_{\eta_{j}}^{(t)},\; k\to \infty $$ that induced by $K_{\eta_{i}}$ to sample
>		- Compute a **factor** of **importance weight** as   $$w_{\eta_{j}}^{(t)} := \frac{\tilde{p}_{\eta_{j}}(X_{\eta_{j}}^{(t)})}{\tilde{p}_{\eta_{j-1}}(X_{\eta_{j}}^{(t)})}$$
>	- Compute the **product of factors** to form the **importance weight** as $$w^{(t)} = \prod_{j=1}^{n}w_{\eta_{j}}^{(t)}$$
>	- Compute the function value $$f^{(t)} = f(X_{\eta_{n}}^{(t)})$$
>- The expectation $\mathbb{E}_{ p_{1} }\left[  f(X) \right]$ is estimated by the **weighted sample mean** $$\mathbb{E}_{ p_{1} }\left[  f(X) \right] \approx \frac{1}{T} \sum_{t=1}^{T}w^{(t)}f^{(t)}.$$

- [[Metropolis-Hastings Algorithm]]
- [[Sum-Product Variable Elimination]]
- [[Estimating Partition Function via Importance Sampling]]



## Explanation

>[!quote]
>In situations where $\mathbb{KL}\left( p_{0} \left\|\right. p_{1} \right)$ is large (i.e., where there is *little overlap* between $p_0$ and $p_{1}$), a strategy called **annealed importance sampling (AIS)** attempts to bridge the gap by introducing *intermediate distributions* (Jarzynski, 1997; Neal, 2001).
>
>This approach enables us to estimate the partition function of a multimodal distribution defined over a *high-dimensional space* (such as the distribution defined by a trained RBM). We begin with a **simpler** model with a **known partition function** (such as an RBM with zeros for weights) and estimate the **ratio between the two model’s partition functions**. The estimate of this ratio is based on the estimate of the ratios of a sequence of many similar distributions, such as the sequence of RBMs with weights interpolating between zero and the learned weights.
>
>-- [[Deep Learning by Goodfellow]] pp 617



-----------
##  Recommended Notes and References


- [[Estimating Partition Function via Importance Sampling]]

- [[Importance Sampling]]
- [[Simulated Annealing]]
- [[Sequential Importance Sampling]]
- [[Markov Chain Monte Carlo Methods]]
- [[Gibbs Sampling]]
- [[Metropolis-Hastings Algorithm]]


- [[Log-Partition Function and Score Function of Graphical Models]]
- [[Gibbs Measure and Energy-based Model]]


- [[Deep Learning by Goodfellow]] pp 617
- [[Probabilistic Machine Learning Advanced Topics by Murphy]]  pp 533 - 536
- [[Probabilistic Graphical Models by Koller]] pp 543, 548
- Neal, R. M. (2001). Annealed importance sampling. _Statistics and Computing_, _11_(2), 125–139. [https://doi.org/10.1023/A:1008923215028](https://doi.org/10.1023/A:1008923215028)