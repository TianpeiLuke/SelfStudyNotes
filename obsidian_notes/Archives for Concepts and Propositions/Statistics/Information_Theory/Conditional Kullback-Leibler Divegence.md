---
tags:
  - concept
  - math/information_theory
keywords:
  - conditional_kl_divergence
topics:
  - information_theory
name: Conditional Kullback-Leibler Divegence
date of note: 2024-06-01
---

## Concept Definition

>[!important]
>**Name**: Conditional Kullback-Leibler Divegence

>[!important] Definition
>Let $(\Omega, \mathscr{F})$ be measurable space. Let $X$ and $Y$ be *random variables* taking value in  $\mathcal{X}$ and $\mathcal{Y}$. 
>
>Denote 
>- $\mathcal{F}_{X} = \sigma(X)$ as the $\sigma$-algebra generated by random variable $X$ and  
>- $\mathcal{F}_{Y} = \sigma(Y)$ as the  $\sigma$-algebra generated by random variable$Y$
>- $\mathscr{F}_{X} \times \mathscr{F}_{Y}$ as the product $\sigma$-algebra on $\mathcal{X} \times \mathcal{Y}$ 
>
>Let $\mathcal{P}$ and $\mathcal{Q}$ be two *joint probability measures* on $\mathscr{F}_{X} \times \mathscr{F}_{Y}$ where 
>$$
>\mathcal{P} \ll \mathcal{Q}.
>$$
>Moreover, define 
>-  $\mathcal{P}[Y | \mathscr{F}_{X}]$ as *conditional probability measure* on $Y$  given $\mathscr{F}_{X}$ *induced* by $\mathcal{P}$;
>- define $\mathcal{Q}[Y | \mathscr{F}_{X}]$ as *conditional probability measure* on $Y$  given $\mathscr{F}_{X}$ *induced* by $\mathcal{Q}$.
>
>The **conditional Kullback-Leibler divergence** (**conditional relative entropy**) from $\mathcal{Q}[Y | \mathscr{F}_{X}]$ to $\mathcal{P}[Y | \mathscr{F}_{X}]$ is defined as
>$$
>\begin{align*}
>\mathbb{KL}\left( \mathcal{P}[Y | \mathscr{F}_{X}] \left\|\right. \mathcal{Q}[Y | \mathscr{F}_{X}] \right) &:= \int_{\mathcal{X} \times \mathcal{Y}} \log \left(\frac{\mathcal{P}[Y | \mathscr{F}_{X}]}{\mathcal{Q}[Y | \mathscr{F}_{X}]}\right)\; d\mathcal{P} = \int_{\mathcal{X} \times \mathcal{Y}} \frac{d\mathcal{P}}{d\mathcal{Q}}\; \log \left(\frac{\mathcal{P}[Y | \mathscr{F}_{X}]}{\mathcal{Q}[Y | \mathscr{F}_{X}]}\right)\; d\mathcal{Q}\\
>&= \mathbb{E}_{ \mathcal{P}_{X, Y} }\left[  \; \log \left(\frac{\mathcal{P}[Y | \mathscr{F}_{X}]}{\mathcal{Q}[Y | \mathscr{F}_{X}]}\right)\;\right] 
\end{align*}
>$$

- [[Kullback-Leibler Divergence]]
- [[Conditional Probability]]
- [[Radon-Nikodym Derivative]]

## Explanation

>[!important]
>By [[Fubini Theorem]], we can represent the conditional relative entropy as 
>$$
>\begin{align*}
>\mathbb{KL}\left( \mathcal{P}[Y | \mathscr{F}_{X}] \left\|\right. \mathcal{Q}[Y | \mathscr{F}_{X}] \right) &:= \int_{\mathcal{X} \times \mathcal{Y}} \log \left(\frac{\mathcal{P}[Y | \mathscr{F}_{X}]}{\mathcal{Q}[Y | \mathscr{F}_{X}]}\right)\; d\mathcal{P} \\
>&=  \int_{\mathcal{X}} \left[    \int_{\mathcal{Y}} \log \left(\frac{\mathcal{P}[ \,y\, | \mathscr{F}_{X}]}{\mathcal{Q}[ \,y\, | \mathscr{F}_{X}]}\right)\;\mathcal{P}[\,dy\, |\, \mathscr{F}_{X}\,] \right] d\mathcal{P}|_{\mathscr{F}_{X}} 
\end{align*}
>$$



-----------
##  Recommended Notes and References

- [[Kullback-Leibler Divergence]]
- [[Conditional Shannon Entropy]]


- [[Elements of Information Theory by Cover]]