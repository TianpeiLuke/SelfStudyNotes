---
tags:
  - concept
  - multi-modal_learning
  - CLIP
  - contrastive_learning
  - contrastive_language_image_pre-training
  - transformers
keywords:
  - CLIP
  - contrastive_language_image_pre-training
topics:
  - multi-modal_learning
  - machine_learning_paradigm
name: Contrastive Language Image Pre-training or CLIP
date of note: 2025-05-05
---

## Concept Definition

>[!important]
>**Name**: Contrastive Language Image Pre-training or CLIP



- [[Vision Transformer or ViT]]
- [[Transformer Network]]
- [[Attention Mechanism in Neural Network]]


![[CLIP.png]]


## Explanation


- [[SimCLR as Visual Representation Learning]]
- [[Information Noise Contrastive Estimation as Contrastive Learning]]




-----------
##  Recommended Notes and References


- [[Large Language Model and Pretrained Language Models]]
- [[radfordLearningTransferableVisual2021]]  Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. _Proceedings of the 38th International Conference on Machine Learning_, 8748â€“8763. 