---
tags:
  - concept
  - machine_learning/strategy
  - deep_learning/representation_learning
keywords:
  - representation_learning
  - distributed_representation
topics:
  - machine_learning_strategy
  - deep_learning
  - representation_learning
name: Distributed Representation
date of note: 2024-07-07
---

## Concept Definition

>[!important]
>**Name**: Distributed Representation


>[!important] Definition
>The **distributed representation** is a principle for system representation that 
>- *each input* to a system should be *represented by many features*, and 
>- *each feature* should be involved in the *representation of many possible inputs*. 

^04ca19


- [[Vector Semantics and Distributional Hypothesis in Linguistic]]

### Why Distributed Representation

>[!important]
>Compared to **one-hot representation**, *distributed representations* are powerful because they can use $n$ features with $k$ values to describe $k^n$ *different concepts*.
>
>Thus *distributed representation* provides a **compact representation** for complicated structure with small number of parameters.


>[!important]
>Another benefit of using *distributed representation* is to mitigate the **curse of dimensionality**:
>-  to learn a *target function* that increases and decreases many times in many different regions, we may need a number of examples that is *at least as large as* the *number of distinguishable regions*.
>  
>  
>The argument in favor of distributed representation is that it improves the **generalization power**:
>- if a *parametric transformation* with $k$ parameters can learn about $r$ regions in input space, with $k \ll r$, and if obtaining such a representation was useful to the task of interest, then we could potentially **generalize much better** in this way than in a nondistributed setting, where we would need $O(r)$ examples to obtain the same features and associated *partitioning of the input space* into $r$ regions. 
>- Using *fewer parameters* to represent the model means that we have *fewer parameters to fit*, and thus require far *fewer training examples* to generalize well.   

- [[Curse of Dimensionality]]
- [[Occam Razor]]
- [[Minimum Description Length]]

>[!important]
>A further part of the argument for why models based on distributed representations generalize well is that their **capacity** remains **limited** despite being able to distinctly encode so many different regions.

- [[VC Dimension]]



## Explanation

### Distributed Representation

>[!quote]
>Many information processing tasks can be very easy or very difficult depending on **how the information is represented**.
>
>-- [[Deep Learning by Goodfellow]] pp 517

- [[Three Components of Intelligence System]]

>[!quote]
>Several key concepts arose during the connectionism movement of the 1980s that remain central to todayâ€™s deep learning.  
>
>One of these concepts is that of **distributed representation** (Hinton et al., 1986). This is the idea that *each input* to a system should be *represented by many features*, and *each feature* should be involved in the *representation of many possible inputs*. 
>
>-- [[Deep Learning by Goodfellow]] pp 16


>[!quote]
>Many deep learning algorithms are motivated by the assumption that the *hidden units* can learn to *represent the underlying causal factors* that explain the data. Distributed representations are natural for this approach, because each *direction* in **representation space** can correspond to the value of a *different underlying configuration variable*.
>
>-- [[Deep Learning by Goodfellow]] 536


>[!quote]
>Let us examine a special case of a *distributed representation learning algorithm*, which extracts **binary features** by *thresholding linear functions of the input*. Each binary feature in this representation divides $\mathbb{R}^d$ into a pair of half-spaces,
>
>- How many regions are generated by an arrangement of $n$ hyperplanes in $\mathbb{R}^d$? By applying a general result concerning the *intersection of hyperplanes* (Zaslavsky, 1975), one can show (Pascanu et al., 2014b) that the **number of regions** this **binary feature representation** can distinguish is $$\sum_{j=0}^{d}{n \choose j} = \mathcal{O}(n^d)$$ Therefore, we see a *growth* that is **exponential** in the *input size* and **polynomial** in the number of *hidden units*.
>  
>  
>-- [[Deep Learning by Goodfellow]] 540  


### Curse of Dimensionality

>[!quote]
>When and why can there be a **statistical advantage** from using a *distributed representation* as part of a learning algorithm? Distributed representations can have a statistical advantage when an apparently complicated structure can be **compactly represented** using a small number of parameters.


### VC Dimension and Induction Bias


>[!quote]
>For example, the **VC dimension** of a neural network of linear threshold units is only $O(w \log w)$, where $w$ is the number of weights (Sontag, 1998). This limitation *arises* because, while we can assign very *many unique codes* to representation space, we *cannot use absolutely all the code space*, nor can we learn arbitrary functions mapping from the representation space $h$ to the output $y$ using a linear classifier. The use of a **distributed representation** combined with a linear classifier thus expresses a *prior belief* that the classes to be recognized are linearly separable as a function of the underlying causal factors captured by $h$. We will typically want to learn categories such as the set of all images of all green objects or the set of all images of cars, but not categories that require nonlinear XOR logic. For example, we typically do not want to partition  the data into the set of all red cars and green trucks as one class and the set of all green cars and red trucks as another class.
>
>- [[Deep Learning by Goodfellow]] 541

- [[VC Dimension]]



-----------
##  Recommended Notes and References

- [[Supervised Learning and Unsupervised Learning]]
- [[Representation Learning]]
- [[Word Embedding]]


- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 173, 991
- [[Deep Learning Foundations and Concepts by Bishop]] pp 188 - 189
- [[Probabilistic Graphical Models by Koller]]
- [[Deep Learning by Goodfellow]] pp 3, 16, 147, 517 - 549