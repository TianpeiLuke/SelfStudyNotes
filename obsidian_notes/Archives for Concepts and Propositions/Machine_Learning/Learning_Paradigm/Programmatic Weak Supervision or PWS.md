---
tags:
  - concept
  - machine_learning/strategy
  - programmatic_weak_supervision
  - PWS
keywords:
  - programmatic_weak_supervision
  - pws
topics:
  - machine_learning_strategy
name: Programmatic Weak Supervision of PWS
date of note: 2025-04-24
---

## Concept Definition

>[!important]
>**Name**: Programmatic Weak Supervision or PWS

>[!important] Definition
>**Programmatic Weak Supervision** is a machine learning paradigm where 
>- large labeled datasets are created automatically by writing 
>	- *heuristic labeling functions*, 
>	- *rules*, 
>	- *external models*, 
>	- or other *noisy sources* of supervision, rather than manually annotating each example. 

| Component              | Role                                                                    |
| ---------------------- | ----------------------------------------------------------------------- |
| **Labeling Functions** | Programmatically assign weak, noisy labels to unlabeled data            |
| **Label Model**        | Combines noisy LF outputs to infer denoised probabilistic labels        |
| **End Model**          | Trains a final classifier using the probabilistic labels and raw inputs |


### Labeling Functions (LF)

>[!info]
>Instead of relying on *expensive, time-consuming human labeling*, programmatic weak supervision leverages domain knowledge codified into **labeling functions** that provide noisy and potentially conflicting labels.

>[!important] Definition
>A **Labeling Function (LF)** is a *programmatic heuristic* that assigns a noisy, potentially abstaining label to a data point based on domain knowledge or external information sources.
>
>- Each labeling function $$\lambda_j: \mathcal{X} \to \{-1, 0, +1\}$$ maps an input $x$ to a label:
> 	 - $+1$ (positive class),
> 	 - $-1$ (negative class),
> 	 - $0$ (**abstain**, no label).
>- Labeling functions are typically **imperfect**: 
>	- they can be noisy, biased, or conflicting.
>- A collection of multiple labeling functions provides **weak supervision signals** over unlabeled data.


>[!example]
>
>  - Keyword matching rules.
>  - Regular expression patterns.
>  - Predictions from external models.
>  - Distant supervision from knowledge bases.


### Label Model

>[!info]
>A probabilistic model, such as a **label model**, is often used to 
>- *denoise* 
>- and *aggregate* these **weak signals** into high-quality training labels, which are then used to train a final discriminative model. 

>[!important] Definition
>A **Label Model** is a *probabilistic model* that aggregates the *noisy, conflicting* outputs of multiple labeling functions to infer the most likely latent true labels.
>
>- The label model learns parameters representing:
> 	 - **Accuracy** of each labeling function,
> 	 - **Correlations** between labeling functions,
> 	 - **Biases** in labeling functions.
>
>- The label model is trained **without access to ground truth labels** by maximizing the likelihood of the observed noisy labels.
>
>- Output:
> 	 - For each data point, the label model produces a **probabilistic soft label** $p(y \mid \lambda(x))$.


>[!info]
>**Common implementations**:
>  - Generative modeling using factor graphs (e.g., Snorkel Label Model).
>  - Graphical models with latent variables.

- [[Probabilistic Graphical Models]]

### End Model

>[!important] Definition
>An **End Model** is a *discriminative* machine learning model trained on the *probabilistic labels* generated by the *label model* to perform the final supervised learning task.
>
>- Input:
>	- Raw input features $x$,
>	- *Probabilistic labels* $\hat{p}(y \mid \lambda(x))$ from the label model.
>
>- The end model learns a direct mapping from inputs $x$ to predictions $y$:
>  $$
>  f_{\theta}: \mathcal{X} \to \mathcal{Y}
>  $$
>  using standard supervised learning methods (e.g., deep neural networks, gradient-boosted trees).


>[!info]
>**Purpose**:
>- To generalize beyond the *noisy patterns* captured by labeling functions.
>- To learn powerful representations from raw data.
>- To *correct biases and limitations* of the weak supervision sources.

>[!info]
>The end model is evaluated against **true held-out labeled data** or used directly in deployment.



## Explanation


>[!info]
> This approach enables **rapid, scalable creation** of labeled datasets, making it practical to supervise complex tasks with minimal direct human annotation effort, especially in domains like information extraction, medical imaging, and abuse detection.

- [[Snorkel for Programmatic Weak Supervision]]





-----------
##  Recommended Notes and References


- [[ratnerDataProgrammingCreating2016]]
- [[ratnerSnorkelRapidTraining2020]]
- [[yuAlfredSystemPrompted2023]]
- [[zhangSurveyProgrammaticWeak2022]]