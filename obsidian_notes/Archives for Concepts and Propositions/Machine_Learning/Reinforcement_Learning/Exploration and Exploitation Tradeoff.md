---
tags:
  - concept
  - machine_learning/algorithms
  - reinforcement_learning/theory
keywords:
  - exploration_exploitation
topics:
  - reinforcement_learning/theory
name: Exploration and Exploitation Tradeoff
date of note: 2024-05-12
---

## Concept Definition

>[!important]
>**Name**: Exploration and Exploitation Tradeoff

>[!important] Definition
>The **exploration-exploitation tradeoff** describes the balancing act between two opposing strategies.
>- **Exploitation** involves choosing the *best option* based on *current knowledge* of the system (which may be *incomplete* or misleading), 
>- while **exploration** involves trying out new options that may lead to *better outcomes* in the future at the expense of an *exploitation opportunity*.


>[!quote] 
>The **exploration-exploitation tradeoff** refers to the fact that 
>- the agent needs to try multiple state/action combinations (this is known as **exploration**) in order to collect enough data so it can reliably learn the reward function $R(s, a)$; 
>- it can then **exploit** its knowledge by picking the predicted best action for each state. 
>- If the agent starts *exploiting* an *incorrect* model too *early*, it will collect suboptimal data, and will get stuck in a **negative feedback loop**.
>  
>-- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 1111 - 1112  


## Explanation

>[!important]
>The **exploration–exploitation trade‑off** is the fundamental dilemma faced by any learning or decision‑making agent that interacts with an uncertain environment: 
>- it must **exploit** its current knowledge to choose actions that yield the highest immediate reward, yet it must also **explore** unfamiliar actions so it can discover potentially better long‑term strategies. 
>- **Pure exploitation** risks converging on sub‑optimal behavior because the agent never samples alternatives, while **excessive exploration** wastes opportunities by repeatedly trying low‑value options. 
>- Effective algorithms
>	- $\epsilon$‑greedy
>	- UCB in multi‑armed bandits, 
>	- $\epsilon$‑decay and entropy bonuses in deep RL,  
>	- Bayesian methods that track posterior uncertainty
>
>carefully balance the two, typically by quantifying uncertainty or allocating a fraction of interaction budget to probing less‑known actions. The challenge is to minimize cumulative regret: explore just enough to learn accurate value estimates, then exploit that knowledge as quickly and safely as possible.

### Multi-Armed Bandit

![[Explore-Then-Commit Bandit Algorithm#^37b26f]]

![[Explore-Then-Commit Bandit Algorithm#^3387d1]]

- [[Multi-Armed Adversarial Bandit]]
- [[Explore-Then-Commit Bandit Algorithm]]
- [[epsilon-Greedy Algorithm]]
- [[Upper Confidence Bound Algorithm]]


### Monte Carlo Prediction and Control for Reinforcement Learning

- [[Monte Carlo Prediction for Value Estimation]]
- [[Monte Carlo Control with Exploring Starts]]


### Temporal Difference Learning

- [[Temporal Difference Learning]]
- [[SARSA Algorithm and On-Policy Temporal Difference Control]]
- [[Q Learning Algorithm and Off-Policy Temporal Difference Control]]
- [[Expected SARSA Algorithm]]


### Policy Gradient Learning with REINFORCE

![[REINFORCE Algorithm for Monte Carlo Policy Gradient#^2e06da]]

- [[REINFORCE Algorithm for Monte Carlo Policy Gradient]]




-----------
##  Recommended Notes and References



- [[Markov Decision Process]]


- [[Algorithms to Live By Book Summary]]
- [[Reinforcement Learning An Introduction by Sutton]] pp 3, 103, 472
- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 1111 - 1112, 1148
