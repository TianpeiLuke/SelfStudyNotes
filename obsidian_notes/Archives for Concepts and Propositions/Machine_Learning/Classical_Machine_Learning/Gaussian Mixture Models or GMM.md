---
tags:
  - concept
  - machine_learning/models
  - probabilistic_graphical_models/models
  - machine_learning/latent_variable_model
  - GMM
  - gaussian_mixture_models
keywords:
  - gaussian_mixture_models
topics:
  - machine_learning_models
  - probabilistic_graphical_model
name: Gaussian Mixture Models
date of note: 2024-09-04
---

## Concept Definition

>[!important]
>**Name**: Gaussian Mixture Models

>[!important] Definition  
>A **Gaussian Mixture Model (GMM)** assumes each data point $x\in\mathbb R^{d}$ is generated by 
>- first sampling a **latent component label** $z\in\{1,\dots,K\}$ 
>- and then drawing $x$ from a component‑specific multivariate normal distribution.  $$x \sim \mathcal{N}(\mu_{k}; \Sigma_{k})$$

- [[Latent Variable Models]]

### Complete Data log-likelihood

![[Expectation-Maximization Algorithm#^0c7718]]

>[!important] Definition
>*Let*  
>- $X=\{x_1,\dots,x_N\}\subset\mathbb R^{d}$ as observed data)  
>- $Z=\{z_1,\dots,z_N\},\; z_i\in\{1,\dots,K\}$ as latent component indices) 
>- model parameters be $$\theta=\{\boldsymbol\pi,\boldsymbol\mu_{1:K},\boldsymbol\Sigma_{1:K}\}$$
>
>The **complete‑data log‑likelihood**  
>$$
>    \log p_\theta(X,Z)
>      =\sum_{i=1}^{N}\log\!\Bigl[\,
>        \pi_{z_i}\,
>        \mathcal N\bigl(x_i\mid\boldsymbol\mu_{z_i},\boldsymbol\Sigma_{z_i}\bigr)
>      \Bigr].
>$$



>[!important] Definition
>The **Q–function**  
>$$ \begin{align*}
>   Q\!\bigl(\theta;\,\theta^{\text{old}}\bigr)
>     & =\mathbb E_{Z\mid X,\theta^{\text{old}}}
>        \bigl[\log p_\theta(X,Z)\bigr] \\
>     & =\sum_{i=1}^{N}\sum_{k=1}^{K}
>        \gamma_{i,k}\,
>        \log\!\Bigl[
>          \pi_{k}\,
>          \mathcal N\bigl(x_i\mid\boldsymbol\mu_{k},\boldsymbol\Sigma_{k}\bigr)
>        \Bigr]
>\end{align*}
>$$
> where 
> - the *soft counts* or  **Posterior responsibility** $\gamma_{i,k}$ weight each sample’s contribution to component $k$.  
>$$
>    \gamma_{i,k}
>    =P\!\bigl(z_i=k\mid x_i,\theta^{\text{old}}\bigr)
>    =\frac{
>        \pi_{k}^{\text{old}}
>        \,\mathcal N(x_i\mid\boldsymbol\mu_{k}^{\text{old}},\boldsymbol\Sigma_{k}^{\text{old}})
>      }{
>        \sum_{j=1}^{K}\pi_{j}^{\text{old}}
>        \,\mathcal N(x_i\mid\boldsymbol\mu_{j}^{\text{old}},\boldsymbol\Sigma_{j}^{\text{old}})
>      }.
>$$

- [[Majorization-Minimization Algorithm]]
- [[Evidence Lower Bound for Exponential Family]]
- [[Evidence Lower Bound]]

### EM Algorithm

>[!important]
>**EM algorithm for GMM** (iterate until convergence)  
>- **E‑step**: Compute *posterior responsibilities* for every data point $x_i$:  
>$$\begin{align*}
>\gamma_{i,k} &= P(z=k\mid x_i,\theta^{(t)}) \\
>       &= \frac{\pi_k^{(t)}\,
>              \mathcal N(x_i\mid\boldsymbol\mu_k^{(t)},\boldsymbol\Sigma_k^{(t)})}
>              {\sum_{j=1}^{K}\pi_{j}^{(t)}\,
>               \mathcal N(x_i\mid\boldsymbol\mu_{j}^{(t)},\boldsymbol\Sigma_{j}^{(t)}) }.
>\end{align*}$$
>- **M‑step**: Update parameters using the responsibilities  
>	- $$N_k  \gets \sum_{i=1}^{N}\gamma_{i,k}$$
>	- $$\pi_k \gets \frac{N_k}{N} $$
>	- $$\mu_k \gets \frac1{N_k}\sum_{i=1}^{N}\gamma_{i,k}\,x_i$$
>	- $$\boldsymbol\Sigma_k \gets
>         \frac1{N_k}\sum_{i=1}^{N}\gamma_{i,k}\,
>           (x_i-\boldsymbol\mu_k)(x_i-\boldsymbol\mu_k)^{\!\top}$$
>-  Set $$t\leftarrow t+1$$ and repeat.  
>
>- *Output*
>	- Maximum‑likelihood estimates $\hat\theta$ 
>	- and *soft cluster memberships* $\gamma_{i,k}$.  
>

- [[Expectation-Maximization Algorithm]]
- [[Expectation-Maximization Algorithm for Exponential Family]]

## Explanation

>[!info]
>*Remarks*  
>- EM *monotonically increases* the log‑likelihood but may converge to a *local maximum*; 
>	- multiple random initialisations are recommended.  
>- Covariance can be *constrained* (diagonal, tied, spherical) to reduce variance and speed computation.  




-----------
##  Recommended Notes and References


- [[Gaussian Scale Mixtures]]
- [[k-Means Clustering]]
- [[Expectation-Maximization Algorithm]]



- [[Exponential Family of Distributions]]
- [[Gaussian Random Vector]]

- [[Hidden Markov Model]]

- [[Optimal Transport in Discrete Setting]]
- [[Entropic Regularized Optimal Transport in Discrete Settings]]


- [[Elements of Statistical Learning by Hastie]] pp 273, 463, 492, 509
- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 920 - 922
- [[Graphical Models Exponential Families and Variational Inference by Wainwright and Jordan]] pp 46 - 47, 156 - 157
- [[Probabilistic Graphical Models by Koller]] pp 616 - 626, 685 - 688
- [[Deep Learning Foundations and Concepts by Bishop]] pp 478 - 484
- [[Computational Optimal Transport by Peyre]]