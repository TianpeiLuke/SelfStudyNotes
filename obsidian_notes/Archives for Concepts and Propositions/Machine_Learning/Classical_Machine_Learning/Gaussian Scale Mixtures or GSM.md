---
tags:
  - concept
  - machine_learning/models
  - probabilistic_graphical_models/models
  - machine_learning/latent_variable_model
  - gaussian_scale_mixtures
  - GSM
keywords:
  - gaussian_scale_mixtures
  - GSM
topics:
  - machine_learning_models
name: Gaussian Scale Mixtures or GSM
date of note: 2024-09-04
---

## Concept Definition

>[!important]
>**Name**: Gaussian Scale Mixtures or GSM

>[!important] Definition  
>A **Gaussian Scale Mixture (GSM)** is a continuous-mixture generalisation of a Gaussian: 
>- each observation is drawn from a Gaussian whose *covariance is rescaled by a positive latent variable*.  
>- The resulting marginal distribution is *heavy-tailed* while remaining analytically tractable.  
>

- [[Sub-Gamma Random Variables]]
- [[Sub-Exponential Random Variables]]

### Generative Model

>[!important] Definition
>For every data vector $x\in\mathbb R^{d}$, the **Gaussian Scale Mixture model (GSM)** is generated by the following  
>$$\begin{align*}
>s &\sim p(s\mid\alpha)\\
>x \mid s &\sim\; \mathcal N\!\bigl(\,\boldsymbol\mu,\; s\,\boldsymbol\Sigma\bigr) 
>\end{align*}
>$$
>where
>- **Latent scale** $s>0$ (*scalar* or *diagonal-matrix*) is drawn from a mixing distribution $$p(s\mid\alpha)$$ with *hyper-parameter(s)* $\alpha$.  
>- **Parameters**
>	- $\theta=\{\boldsymbol\mu,\boldsymbol\Sigma,\alpha\}$.  

- [[Gaussian Random Vector]]
- [[Latent Variable Models]]

>[!info]
>Common choices  of prior distribution $$p(s|\alpha)$$
>- **Inverse-Gamma**  when marginal $x$ follows a **multivariate Student-t.**  
>- **Discrete set** $\{s_1,\dots,s_K\}$ with weights $\pi_k$ when the marginal distribution $x$ is a finite mixture.  

- [[Student-t Distribution]]

### Complete-data log-likelihood  

>[!important] Definition
>Given 
>- dataset $X=\{x_i\}_{i=1}^{N}$ 
>- and latent scales $S=\{s_i\}$
>  
>the **complete-data log-likelihood** is given by  
>$$
>\begin{align*}
>  &\; \log p_\theta(X,S) \\
>  &= \sum_{i=1}^{N}
>  \bigl[
>    \log p(s_i\mid\alpha)\;
>    -\frac12
>    \bigl(
>      d\log(2\pi)+\log|s_i\boldsymbol\Sigma|
>      +(x_i-\boldsymbol\mu)^\top(s_i\boldsymbol\Sigma)^{-1}(x_i-\boldsymbol\mu)
>    \bigr)
>  \bigr].
>\end{align*}
>$$

- [[Gaussian Mixture Models or GMM]]
- [[Evidence Lower Bound]]
- [[Gaussian Random Vector]]

### EM estimation (conjugate Inverse-Gamma prior)  

>[!important] Definition
>For every data vector $x\in\mathbb R^{d}$, the **Gaussian Scale Mixture model (GSM)** is generated by the following  
>$$\begin{align*}
>s &\sim p(s\mid\alpha)\\
>x \mid s &\sim\; \mathcal N\!\bigl(\,\boldsymbol\mu,\; s\,\boldsymbol\Sigma\bigr) 
>\end{align*}
>$$
>with parameters
>- $\theta=\{\boldsymbol\mu,\boldsymbol\Sigma,\alpha\}$.  
>  
>The **EM algorithm for GSM** is given as below
>- Iterate the following steps until the *observed-data log-likelihood* converges.  
>	- **E-step**
>		- Compute *posterior* of $s_i$ given current parameters  $$\begin{aligned}p(s_i\mid x_i,\theta^{\text{old}}) &=\; \text{Inv-Gamma}\!\bigl(\tilde a,\;\tilde b_i \bigr),\\[5pt] \tilde a &= a + \frac{d}{2}, \\ \tilde b_i &= b+\frac12 (x_i-\mu)^\top\Sigma^{-1}(x_i-\mu).\end{aligned}$$
>	- Store *expected scale* and its *inverse*  $$\langle s_i\rangle,\;\langle 1/s_i\rangle.$$
>	- **M-step**
>		- Update *mean* and *covariance*  $$\begin{aligned}\boldsymbol\mu &\leftarrow \frac{\sum_i \langle 1/s_i\rangle\,x_i}{\sum_i \langle 1/s_i\rangle},\\[6pt] \boldsymbol\Sigma &\leftarrow \frac1{N}\sum_{i=1}^{N}\langle 1/s_i\rangle\,(x_i-\boldsymbol\mu)(x_i-\boldsymbol\mu)^\top.\end{aligned}$$
>		- Update $a, b$ via maximum-likelihood or fixed
>
>- Outputs  
>	- *Estimated parameters* $$\hat{\theta} :=  (\hat{\boldsymbol\mu}, \hat{\boldsymbol\Sigma}, \hat{\alpha}).$$  
>	- *Posterior mean scales* $$\langle s_i\rangle$$ can act as **per-sample noise estimates**.  

- [[Expectation-Maximization Algorithm]]
- [[Expectation-Maximization Algorithm for Exponential Family]]


## Explanation

>[!important]
>- The marginal density $p(x)$ is **heavy-tailed**, making *GSMs* suitable for **robust modelling** of natural images, financial returns, or any data with occasional large outliers.  

>[!important]
>- If $p(s)$ is **discrete**, *GSM* collapses to a **heteroscedastic GMM** where all components share *the same mean* and *orientation* but *differ in scale*.


>[!info]
>Setting $$p(s)=\delta(s-1)$$ retrieves an **ordinary Gaussian**.  



-----------
##  Recommended Notes and References



- [[k-Means Clustering]]
- [[Expectation-Maximization Algorithm]]



- [[Exponential Family of Distributions]]

- [[Hidden Markov Model]]
- [[Linear Stochastic Differential Equation Explicit Solution]]
- [[Linear Stochastic Differential Equation]]


- [[Elements of Statistical Learning by Hastie]]
- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 922 - 923
- [[Graphical Models Exponential Families and Variational Inference by Wainwright and Jordan]]
- [[Probabilistic Graphical Models by Koller]]