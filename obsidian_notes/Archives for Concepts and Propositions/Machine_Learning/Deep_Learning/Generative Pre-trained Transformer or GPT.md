---
tags:
  - concept
  - deep_learning/architecture
  - deep_learning/models
  - deep_learning/generative_models
  - deep_learning/large_language_models
keywords:
  - generative_pretrained_transformer
  - gpt
  - gpt_network
topics:
  - deep_learning/generative_models
  - deep_learning/large_language_models
name: Generative Pre-trained Transformer or GPT
date of note: 2024-10-21
---

## Concept Definition

>[!important]
>**Name**: Generative Pre-trained Transformer or GPT


### Decoder-Only Transformer Architecture


- [[Autoregressive Models]]

#### Causal Masked Attention

![[Attention Mechanism in Neural Network#^bb5243]]

![[causal_and_masked_matrix.png]]

- [[Attention Mechanism in Neural Network]]


![[gpt_architecture.png]]


![[gpt_transformer.png]]


### Byte-Pair Encoding Tokenization

![[Byte-Pair Encoding or BPE Tokenization#^98ba57]]

- [[Byte-Pair Encoding or BPE Tokenization]]


### Greedy Decoding via Beam Search



- [[Beam Search as Greedy Decoding]]


## Explanation





-----------
##  Recommended Notes and References


- [[Attention Mechanism in Neural Network]]
- [[Transformer Network]]

- [[Large Language Model and Pretrained Language Models]]
- [[Bidirectional Encoder Representation from Transformer or BERT]]
- [[Text-to-Text Transfer Transformer or T5 for Translation]]
- [[Artificial Neural Network and Deep Learning]]

- [[Deep Learning Foundations and Concepts by Bishop]] pp 382 - 388