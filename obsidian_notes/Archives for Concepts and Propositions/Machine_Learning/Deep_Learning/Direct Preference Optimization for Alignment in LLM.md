---
tags:
  - concept
  - deep_learning/architecture
  - deep_learning/models
keywords: 
topics: 
name: 
date of note: 2024-10-21
---

## Concept Definition

>[!important]
>**Name**: 



## Explanation


- [[Reinforcement Learning with Human Feedbacks or RLHF for LLM]]
- [[Supervised Fine-Tuning and Preference Alignment for LLM]]


-----------
##  Recommended Notes and References

- [[rafailovDirectPreferenceOptimization2023]] Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., & Finn, C. (2024). Direct preference optimization: Your language model is secretly a reward model. _Advances in Neural Information Processing Systems_, _36_.
- [[Attention Mechanism in Neural Network]]
- [[Transformer Network]]
- [[Large Language Model and Pretrained Language Models]]
- [[Foundational Models for Transfer Learning]]


- [[Valued-based and Policy-based Reinforcement Learning]]
- [[Markov Decision Process]]