---
tags:
  - concept
  - machine_learning/models
  - deep_learning/generative_models
  - math/game_theory
keywords:
  - generative_adversarial_network
  - wasserstein_distance
  - wasserstein_gan
topics:
  - deep_learning/generative_models
  - game_theory
name: Wasserstein Generative Adversarial Network
date of note: 2024-05-12
---

## Concept Definition

>[!important]
>**Name**: Wasserstein Generative Adversarial Network

### Training Objective

![[Variational Representation of Wasserstein Distance#^7ba1e7]]

![[Principle of Learning by Comparison for Implicit Generative Models#^a1fa42]]

>[!important] Definition
>Let $p^{*}$ be the unknown true data distribution, and $q_{\theta}$ is a distribution from **generative model** $\mathcal{P}_{\Theta}.$
>
>Let $X$ and $X'$ be samples generated by true data distribution $p^{*}$ and generative model* $q_{\theta}$
>$$
>X\sim p^{*}, \quad X' = g_{\theta}(Z) \sim q_{\theta}(Z),\; Z\sim p(z)
>$$
>
>At iteration $t$, the *training* for **Wasserstein-Generative Adversarial Network (Wasserstein-GAN)** consists of two steps
>- Given $\theta^{(t-1)}$,  training of **discriminator** or **critic** $D_{\psi}$ via 
>$$
>\begin{align*}
>\psi &= \arg\max_{\psi}\left\{ \mathbb{E}_{p^{*}}\left[ D_{\psi}(X)   \right] -  \mathbb{E}_{Z\sim p(z)}\left[D_{\psi}(g_{\theta^{(t-1)}}(Z))  \right]  \right\} 
> \end{align*}
>$$ 
>where $D_{\psi}$ need to be *Lipschitz continuous* with *Lipschitz norm* $1$
>- Given $\psi^{(t)}$,  training of **generator** or *generative model* $g_{\theta}$ via 
>$$
>\begin{align*}
>\theta &= \arg\min_{\theta}\left\{ \mathbb{E}_{p^{*}}\left[ D_{\psi^{(t)}}(X)   \right] -  \mathbb{E}_{Z\sim p(z)}\left[ D_{\psi^{(t)}}(g_{\theta}(Z))  \right]  \right\}
> \end{align*}
>$$
>

- [[Wasserstein Distance]]
- [[Variational Representation of Wasserstein Distance]]
- [[Principle of Learning by Comparison for Implicit Generative Models]]
- [[Generative Adversarial Network]]
- [[Lipschitz Continuous Function]]
- [[Monte Carlo and Applications]]

### Game Perspective

>[!important] Defintion
>The *training objective* for **Wasserstein GAN** can be seen as a **two-player zero-sum game** where the two players are
>1. the **discriminator** or **critic**, whose objective is 
>$$
>\begin{align*}
>\psi &= \arg\max_{\psi}\left\{\mathbb{E}_{p^{*}}\left[ D_{\psi}(X)   \right] -  \mathbb{E}_{Z\sim p(z)}\left[  D_{\psi}(g_{\theta}(Z)) \right]    \right\} 
> \end{align*}
>$$
>2. the **generator**, whose objective is 
>$$
>\begin{align*}
>\theta &= \arg\min_{\theta}\left\{\mathbb{E}_{p^{*}}\left[ D_{\psi}(X)   \right] -  \mathbb{E}_{Z\sim p(z)}\left[  D_{\psi}(g_{\theta}(Z))  \right]   \right\} 
> \end{align*}
>$$
>
>The overall **value** of **min-max game** is bounded above by the **minimal Wasserstein-1 distance**
>$$
>\begin{align*}
>V(\theta^{*}, \psi^{*})&:= \min_{\theta}\max_{\psi}\left\{ \mathbb{E}_{p^{*}}\left[ D_{\psi}(X)   \right] -  \mathbb{E}_{Z\sim p(z)}\left[ D_{\psi}(g_{\theta}(Z))  \right]  \right\} \\[5pt]
>&\le \min_{\theta}\mathcal{W}_{1}(p^{*}, q_{\theta})
> \end{align*}
>$$
>- Thus the $\theta^{*}$ that *minimize the $\mathcal{W}_{1}$ distance* is the **Nash equilibrium** $\theta^{*}$

^ebb70c

- [[Von Neumann Min-Max Theorem]]
- [[Two-Player Finite Game and Matrix Representation]]
- [[Jensen-Shannon Divergence]]


## Explanation

![[Density Ratio Estimation via Binary Classifiers#^4f1f3a]]

- [[Density Ratio Estimation via Binary Classifiers]]





-----------
##  Recommended Notes and References


- [[f-Generative Adversarial Network]]
- [[Artificial Neural Network and Deep Learning]]


- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 883 - 914
- [[Deep Learning Foundations and Concepts by Bishop]] pp 533 - 544
- [[Deep Learning by Goodfellow]] pp 679, 690
- [[Foundations of Computer Vision by Torralba]] pp 487 - 489