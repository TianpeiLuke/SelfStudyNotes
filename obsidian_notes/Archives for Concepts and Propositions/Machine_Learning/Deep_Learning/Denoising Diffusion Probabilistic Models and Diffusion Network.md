---
tags:
  - concept
  - machine_learning/models
  - deep_learning/generative_models
  - deep_learning/models
  - deep_learning/architecture
  - probabilistic_graphical_models/sequential_models
keywords:
  - denoising_diffusion_probabilistic_models
  - ddpm
  - diffusion_network
topics:
  - deep_learning/models
  - deep_learning/algorithm
  - probabilistic_graphical_model
name: Denoising Diffusion Probabilistic Models and Diffusion Network
date of note: 2024-05-12
---

## Concept Definition

>[!important]
>**Name**: Denoising Diffusion Probabilistic Models and Diffusion Network

>[!important] Definition
> The **denoising diffusion probabilistic model (DDPM)** consists of two processes:
> - The **forward pass** corresponds to a *multi-step noise process* i.e. a **diffusion process** that transforms data into white noise.
> - The **reverse pass** uses the *learned neural network* to *invert* the noise process, which involves a *multi-step time-reversed Markov chain* that transforms white noise to new sample.
>   
>The goal of **DDPM** is to learn a *neural network* that *approximate* the **reversed Markov transition kernel**.   


- [[Diffusion Process]]
- [[Markov Chain and Markov Process]]
- [[Time-Reversible Markov Chain]]
- [[Markov Transition Kernel and Transition Function]]
- [[Stochastic Differential Equations]]


## Forward Diffusion Chain and Encoder

>[!important] Definition
> At iteration $t$, the intermediate latent variable $z_{t}$ is generated by *corrupting* previous iteration $z_{t-1}$ with an **independent additive Gaussian white noise** $\epsilon_{t}$, i.e.
> $$
> \begin{align}  
> z_{t} &= \sqrt{ 1 - \beta_{t} }\,z_{t-1} + \sqrt{ \beta_{t} }\epsilon_{t} \quad t= 1\,{,}\ldots{,}\,
>\end{align}
>$$ 
>where 
>- $z_{0} = x$ is *input data*;
>- $\epsilon_{t} \sim \mathcal{N}(0, I)$ are *i.i.d. standard normal distribution*
>- The noise variance parameters $\{\beta_{t}\}$ are *fixed* with $$\beta_{1} < \beta_{2} \,{<}\ldots{<}\,\beta_{T} < 1$$
>
>The **transition kernel** is defined by a *conditional Gaussian distribution* $$K(z_{t-1}, z_{t}) = \mathcal{N}(z_{t}\;|\; \sqrt{ 1 - \beta_{t} }\,z_{t-1},\; \beta_{t}I)$$
>- The sequence $$x_{0}\,\to\, z_{1} \,{\to}\ldots{\to}\,z_{T-1}\, \to\, z_{T}$$ is called the **forward pass** or **forward diffusion**, which is a **Markov chain** with **Gaussian transition function**.
>- It can be seen as a discretized version of *continuous time* **diffusion process**

^de66ca

- [[Gaussian Random Vector]]
- [[Markov Transition Kernel and Transition Function]]
- [[Markov Chain and Markov Process]]
- [[Diffusion Process]]

### Diffusion Kernel

>[!important] Definition
>The **diffusion kernel** for *DDPM* is defined as the *$t$-step forward transition kernel* of the *Markov chain* $\{ z_{t} \}_{t=1}^{T}$, i.e.
>$$
>K^{t}(x, z_{t}) := q(z_{t}\,|\,x)
>$$
>
>- Note that the joint distribution of  $\{ z_{t} \}_{t=1}^{T}$ given input $x$ can be *factorized* by diffusion kernel $$q(z_{1} \,{,}\ldots{,}\,z_{T}\,|\,x) = q(z_{1}\,|\,x)\,\prod_{t=2}^{T}q(z_{t}\,|\,z_{t-1})$$ where each potential function is *Gaussian*.
>- The diffusion kernel can be computed by *marginalization* of $q(z_{1} \,{,}\ldots{,}\,z_{T}\,|\,x)$ i.e. $$K^{t}(x, z_{t}) := q(z_{t}\,|\,x) = \mathcal{N}(z_{t}\;|\; \sqrt{ \alpha_{t} }x,\, (1- \alpha_{t})I)$$ where $$\alpha_{t} = \prod_{s=1}^{t}(1 - \beta_{s})$$
>- That is, the **diffusion kernel** is also a **Gaussian kernel** in closed-form
>- In other word, *$t$-step* *intermediate latent variable*  $z_{t}$ can be obtained **directly** from *input* $x$ via formula $$z_{t} = \sqrt{ \alpha_{t} }x + \sqrt{ 1 - \alpha_{t} }\epsilon_{t}$$
>
>- As $t\to \infty$, the **limit transition kernel** $$\lim_{ t \to \infty }  K^{t}(x, z_{t}) = \mathcal{N}(z_{T}\,|\,0, I).$$

^e5bd56

- [[Gaussian Bayesian Network]]
- [[Dynamic Bayesian Network]]
- [[Chapman-Kolmogorov Equation]]
- [[Invariant Measure and Stationary Distribution]]
- [[Ergodic Markov Chain and Asymptotic of Transition Kernel]]

>[!important]
>The simple formulation of 1-step transition kernel in *forward diffusion* allows us to directly **jump** to $t$-step of the process *without simulating the entire process*.
>
>From **autoencoder** point of view, the **encoder step** is a fixed mapping where $$z_{t} = \sqrt{ \alpha_{t} }x + \sqrt{ 1 - \alpha_{t} }\epsilon_{t}$$
>
>Unlike **variational autoencoder**, **DDPM** requires **multiple encoder at different time** while each has **different variance** $\alpha_{t}$

>[!important] Definition
>The **marginal distribution** of $t$-step intermediate representation $z_{t}$ is given by 
>$$
>q(z_{t}) = \int_{\mathcal{D}}\,q(z_{t}\,|\,x)\,p_{\text{data}}(x)\,dx
>$$
>- We can view this as the **noise perturbed data distribution** under **different noise scale**.
>
>- This unconditional marginal corresponds to the **Gaussian convolution** in **image filter and reconstruction**:
>	- It will remove the *high frequency component* such as the *low-level details*, *textures* etc.
>	- Then it will remove the *low frequency component* i.e. the *high-level semantic information*.
>  
>- The **forward pass** in DDPM is equivalent to apply *multiple stages* of **band-pass filters** in image processing.  

- [[Wavelet]]

 >[!info] Proof
 >Note that 
>$$
>\begin{align*}
>z_{t} &= \sqrt{ 1 - \beta_{t} }\,z_{t-1} + \sqrt{ \beta_{t} }\epsilon_{t} \\[5pt]
>&= \sqrt{ 1 - \beta_{t} }\,\left(\sqrt{ 1 - \beta_{t-1} }\,z_{t-2} + \sqrt{ \beta_{t-1} }\epsilon_{t-1}\right) + \sqrt{ \beta_{t} }\epsilon_{t}\\[5pt]
>&= \ldots \\
>&= \prod_{s=1}^{t}\left(\sqrt{ 1 - \beta_{s} }\right)x + \sum_{s=1}^{t}\left(\prod_{r=s+1}^{t}\sqrt{ 1 - \beta_{r} }\right)\sqrt{ \beta_{s} }\epsilon_{s}\\[5pt]
>&= \sqrt{\prod_{s=1}^{t}(1 - \beta_{s})}\,x \,+\, \hat{\epsilon}_{t}\\[5pt]
>&= \sqrt{ \alpha_{t} }\,x \,+\, \hat{\epsilon}_{t}
>\end{align*}
>$$
>where $$\alpha_{t} := \prod_{s=1}^{t}(1 - \beta_{s})$$
>
>Since $\epsilon_{i} \sim \mathcal{N}(0, I)$ are *independent standard Gaussian*, so 
>$$
>\begin{align*}
>\hat{\epsilon}_{t} &= \sum_{s=1}^{t}\sqrt{\beta_{s} \prod_{r=s+1}^{t}\left(1 - \beta_{r}\right) }\;\epsilon_{s} \sim \mathcal{N}(0, \hat{\sigma}_{t}^2I) \\[5pt]
>\hat{\sigma}_{t}^2&= \sum_{s=1}^{t}\beta_{s} \prod_{r=s+1}^{t}\left(1 - \beta_{r}\right)
>\end{align*}
>$$
>
>Also 
>$$
>\begin{align*}
> \alpha_{t} &:= \prod_{s=1}^{t}(1 - \beta_{s})\\[5pt]
> &=  \prod_{s=2}^{t}(1 - \beta_{s}) - \beta_{1} \prod_{s=2}^{t}(1 - \beta_{s})\\[5pt]
> &= \prod_{s=3}^{t}(1 - \beta_{s}) - \beta_{2}\prod_{s=3}^{t}(1 - \beta_{s}) - \beta_{1} \prod_{s=2}^{t}(1 - \beta_{s})\\[5pt]
> &= \ldots \\
> &= (1 - \beta_{t}) - \beta_{t-1}(1 - \beta_{t}) \,{-}\ldots{-}\,\beta_{1} \prod_{s=2}^{t}(1 - \beta_{s})\\[5pt]
> &= 1 - \sum_{s=1}^{t}\beta_{s} \prod_{r=s+1}^{t}\left(1 - \beta_{r}\right)
>\end{align*}
>$$
>So $$\hat{\sigma}_{t}^2 = 1- \alpha_{t}$$


## Reverse Diffusion Chain and Diffusion Network

### Reversed Transition Kernel and Conditional Reversed Transition Kernel

>[!info]
>We now consider the **reverse diffusion** which is a *time-reversed Markov chain* $$z_{T} \to z_{T-1} \,{\to}\ldots{\to}\,z_{1} \to x$$
>Although the *forward transition kernel* is simple, the *reversed transition kernel*  $$\begin{align} Q(z_{t}, z_{t-1}) := q(z_{t-1} | z_{t}) &= \frac{q(z_{t}| z_{t-1})q(z_{t-1})}{q(z_{t})} \end{align}$$ is in general **intractable**, since the marginal distribution $$q(z_{t})= \int q(z_{t}\,|\,x)\,p_{\text{data}}(x)\,dx$$ depends on the *data distribution* $p(x)$, which is high dimensional and complex.

- [[Time-Reversible Markov Chain]]
- [[Bayes Theorem]]

>[!important] Definition
>Define the **conditional reversed kernel** as the *posterior distribution* of past state $z_{t-1}$ **conditioned on input** $x$ and present state $z_{t}$, $$\begin{align}Q_{x}(z_{t}, z_{t-1}) := q(z_{t-1} | z_{t}, x) &= \frac{q(z_{t}| z_{t-1}, x)q(z_{t-1}|x)}{q(z_{t}|x)} \end{align}$$
> - Note that the LHS involves the *ratio* of two *Gaussian diffusion kernels* $$q(z_{s}|x) = K^{s}(x, z_{s}) = \mathcal{N}(z_{s}\;|\;\sqrt{ \alpha_{s} }x,\; (1-\alpha_{s})I)$$
> - And the *Markov property* holds for $q$, which allows us to drop $x$ $$q(z_{t}| z_{t-1}, x) = q(z_{t}| z_{t-1})$$
> - Thus the *conditional reverse kernel* given data $x$ is given by $$Q_{x}(z_{t}, z_{t-1}) = \frac{K(z_{t-1}, z_{t})\,K^{t-1}(x, z_{t-1})}{K^{t}(x, z_{t})}$$ 
> - The **conditional reversed kernel** is **Gaussian** which is in closed-form $$Q_{x}(z_{t}, z_{t-1}) := q(z_{t-1} | z_{t}, x) = \mathcal{N}(z_{t-1}\;|\; m_{t}(x, z_{t}),\,\sigma_{t}^2I)$$
> 	- the **posterior mean function** $$m_{t}(x, z_{t}) := \frac{1}{1- \alpha_{t}}\left[ (1- \alpha_{t-1})\sqrt{ 1- \beta_{t} }\,z_{t} + \sqrt{ \alpha_{t-1} }\,\beta_{t}\,x \right] $$
> 	- the **posterior variance** $$\sigma_{t}^2 := \frac{1 - \alpha_{t-1}}{1- \alpha_{t}}\,\beta_{t}$$
> - In other word, **given input** $x$, the **time-reversed Markov chain** can be generated by $$z_{t-1} = \sqrt{ \beta_{t} }\left(\frac{1 - \alpha_{t-1}}{1- \alpha_{t}}\right)\,z_{t} + \beta_{t}\frac{\sqrt{ \alpha_{t-1} }}{1 - \alpha_{t}}\,x + \sqrt{ \beta_{t}\left(\frac{1 - \alpha_{t-1}}{1- \alpha_{t}}\right) }\;\epsilon_{t} $$

- [[Marginal and Conditional Distribution of Gaussian]]

![[diffusion_network_ddpm.png]]


###  Diffusion Network as Approximation of Reverse Kernel 

>[!important]
>The intractable **time-reversed kernel** $Q(z_{t}, z_{t-1})$ can be *approximated* by a **diffusion network**
>$$
>\begin{align*}
>Q(z_{t}, z_{t-1}) &:= q(z_{t-1}|z_{t})\\[5pt] 
>&\approx p(z_{t-1}|z_{t}, w) \\[5pt]
>&:= \mathcal{N}(z_{t-1}\;|\; \mu(z_{t}, w, t),\, \beta_{t}I)
>\end{align*}
>$$
>where
>- the **diffusion network** is the mean function $$\mu(z_{t}, w, t)$$ 
>	- Note that it takes *time-step* $t$ as *input* to account for the *change of variance* $\beta_{t}$ at *different time-step* of Markov chain.
>	- We can consider $\mu(z_{t}, w,t)$ as the **noise conditional score network** which conditioned on *different noise scale, indexed by* $t$
>	- This allows us to use a *single network* to invert *all the steps* in the Markov chain, instead of having to learn a *separate network for each step*.
>- **Intuition**: Even if the posterior distribution $q(z_{t-1}|z_{t})$ is *multi-modal*, if we let $\beta_{t} \ll 1$, we would expect it can be approximated by *single-modal Gaussian distribution.*
>  
>  
>The *time-reverse Markov chain* $(z_{T}\,{,}\ldots{,}\,z_{1}, x)$ has a factorized joint distribution that can be *approximated* as
>$$
>\begin{align*}
>q(z_{T}\,{,}\ldots{,}\,z_{1}, x) &= q(z_{T})\prod_{t=2}^{T}q(z_{t-1}|z_{t})\,q(x|z_{1}) \\[5pt]
>&\approx p(z_{T})\prod_{t=2}^{T}p(z_{t-1}|z_{t}, w)\,p(x|z_{1}, w) \\[5pt]
>\end{align*}
>$$  
>- Assume that the terminal state has standard normal distribution $$q(z_{T}) = p(z_{T}) = \mathcal{N}(z_{T}\,|\, 0, I)$$

^e4a1ab

- [[Artificial Neural Network and Deep Learning]]

## Variational Training

![[Evidence Lower Bound#^c74005]]
![[Evidence Lower Bound#^fbc1e1]]

>[!important] Definition
>Given 
>- the *forward chain* $x, z_{1}\,{,}\ldots{,}\,z_{T}$, 
>	- with a *fixed* forward *conditional* distribution $$q_{\text{forward}}(z_{1} \,{,}\ldots{,}\,z_{T}\,|\,x) = q(z_{1}\,|\,x)\,\prod_{t=2}^{T}q(z_{t}\,|\,z_{t-1})$$
>- and the *reverse chain* $z_{T}, z_{T-1} \,{,}\ldots{,}\,z_{1},x$
>	- with an *approximated joint distribution*  $$q_{\text{reverse}}(z_{T}\,{,}\ldots{,}\,z_{1}, x) \approx p(z_{T})\prod_{t=2}^{T}p(z_{t-1}|z_{t}, w)\,p(x|z_{1}, w)$$  
>
>the **evidence lower bound (ELBO)** or **variational free energy** for **DDPM** is given by 
>$$
>\begin{align*}
>\mathcal{L}(q, w; x) &:=  \mathbb{E}_{q }\left[ \log \left(\frac{q_{\text{reverse}}(Z_{T}\,{,}\ldots{,}\,Z_{1}, x)}{q_{\text{forward}}(Z_{1}\,{,}\ldots{,}\,Z_{T}|x)}\right) \right] \\[5pt]
>&=  \mathbb{E}_{q }\left[ \log \left(\frac{q(z_{T})\prod_{t=2}^{T}q(z_{t-1}\,|\,z_{t})\,q(x\,|\,z_{1})}{q(z_{1}\,|\,x)\,\prod_{t=2}^{T}q(z_{t}\,|\,z_{t-1})}\right) \right] \\[5pt]
>&\approx  \mathbb{E}_{q }\left[ \log \left(\frac{p(z_{T})\prod_{t=2}^{T}p(z_{t-1}\,|\,z_{t}, w)\,p(x\,|\,z_{1}, w)}{q(z_{1}\,|\,x)\,\prod_{t=2}^{T}q(z_{t}\,|\,z_{t-1})}\right) \right]\\[5pt]
>&= \mathbb{E}_{q }\left[\log p(z_{T}) + \sum_{t=2}^{T}\log \left(\frac{p(z_{t-1}\,|\,z_{t}, w)}{q(z_{t}\,|\,z_{t-1}, x) }\right) - \log q(z_{1}|x) + \log p(x|z_{1}, w) \right]
>\end{align*}
>$$
>where 
>- the first term $p(z_{T})$ is a *fixed distribution* $\mathcal{N}(z_{T}|0, I)$,
>- the third term $q(z_{1}|x)$ is *independent* from $w$,
>- the fourth term is the **reconstruction term** from *Variational Autoencoder (VAE)*  $$\mathbb{E}_{q }\left[\log p(x|z_{1}, w) \right] = \int q(z_{1}|x)\,\log p(x|z_{1}, w)\,dz_{1}.$$
>- the second term depends on *ratio of forward and reverse kernel* $$\sum_{t=2}^{T}\log \left(\frac{p(z_{t-1}\,|\,z_{t}, w)}{q(z_{t}\,|\,z_{t-1}, x) }\right)$$
>  

- [[Markov Chain and Markov Process]]
- [[Evidence Lower Bound]]
- [[Variational Auto-Encoder]]
- [[Metropolis-Hastings Algorithm]]


>[!info]
>By Bayes theorem, we can *invert the forward conditional kernel* 
>$$
>\begin{align}
>q(z_{t} | z_{t-1}, x) &= \frac{q(z_{t-1}| z_{t}, x)q(z_{t}|x)}{q(z_{t-1}|x)} 
>\end{align}
>$$
>And the second term is 
>$$
>\begin{align}
>\sum_{t=2}^{T}\log \left(\frac{p(z_{t-1}\,|\,z_{t}, w)}{q(z_{t}\,|\,z_{t-1}, x) }\right) &= \sum_{t=2}^{T}\log \left(\frac{p(z_{t-1}\,|\,z_{t}, w)q(z_{t-1}|x)}{q(z_{t-1}| z_{t}, x)q(z_{t}|x)}\right) \\[5pt] 
>&=  \sum_{t=2}^{T}\log\left(\frac{p(z_{t-1}\,|\,z_{t}, w)}{q(z_{t-1}| z_{t}, x)}\right)   + \sum_{t=2}^{T}\log \left(\frac{q(z_{t-1}|x)}{q(z_{t}|x)}\right) 
>\end{align}
>$$
>and the last term is a **telescope sum**, $$\sum_{t=2}^{T}\log \left(\frac{q(z_{t-1}|x)}{q(z_{t}|x)}\right) = \sum_{t=2}^{T}\left(\log q(z_{t-1}|x) - \log q(z_{t}|x)\right) = \log q(z_{1}|x) - \log q(z_{T}|x)$$ which is independent from $w$.
>

>[!info]
>Finally, 
>$$
>\begin{align*}
>\mathbb{E}_{q }\left[ \sum_{t=2}^{T}\log\left(\frac{p(z_{t-1}\,|\,z_{t}, w)}{q(z_{t-1}| z_{t}, x)}\right) \right] &=   \sum_{t=2}^{T}\mathbb{E}_{ Z_{t}\sim q(\cdot|x) }\left[  \mathbb{E}_{Z_{t-1} \sim q(\cdot|Z_{t},x) }\left[\log\left(\frac{p(Z_{t-1}\,|\,Z_{t}, w)}{q(Z_{t-1}| Z_{t}, x)}\right) \right] \right] \\[5pt]
>&=  -\sum_{t=2}^{T}\mathbb{E}_{ Z_{t}\sim q(\cdot|x) }\left[  \mathbb{E}_{Z_{t-1} \sim q(\cdot|Z_{t},x) }\left[\log\left(\frac{q(Z_{t-1}| Z_{t}, x)}{p(Z_{t-1}\,|\,Z_{t}, w)}\right) \right] \right]  \\[5pt]
>&= -\sum_{t=2}^{T}\mathbb{E}_{ Z_{t}\sim q(\cdot|x) }\left[ \mathbb{KL}\left( q(z_{t-1}| Z_{t}, x) \left\|\right. p(z_{t-1}\,|\,Z_{t}, w) \right) \right]
>\end{align*}
>$$


>[!important] Definition
>The **ELBO training objective function** for *(DDPM)* is proportional to 
>$$
>\begin{align*}
>\mathcal{L}(q, w; x) &= \mathbb{E}_{q }\left[\log p(x|Z_{1}, w) + \sum_{t=2}^{T}\log \left(\frac{p(Z_{t-1}\,|\,Z_{t}, w)}{q(Z_{t}\,|\,Z_{t-1}, x) }\right)\right] \\[5pt]
>&= \mathbb{E}_{Z_{1} \sim q(\cdot|x) }\left[\log p(x|Z_{1}, w) \right] \\[5pt]
>&\quad - \sum_{t=2}^{T}\mathbb{E}_{Z_{t}\sim q(\cdot|x) }\left[\mathbb{KL}\left( q(z_{t-1}| Z_{t}, x) \left\|\right. p(z_{t-1}\,|\,Z_{t}, w) \right)\right] - \mathbb{KL}\left( q(z_{T}| x) \left\|\right. p(z_{T}) \right)\\[5pt]
>&\propto \mathbb{E}_{Z_{1} \sim q(\cdot|x) }\left[\log p(x|Z_{1}, w) \right] - \frac{1}{2\beta_{t}}\sum_{t=2}^{T}\mathbb{E}_{Z_{t} \sim q(\cdot|x)  }\left[\lVert m_{t}(x, Z_{t}) - \mu(Z_{t}, w, t) \rVert_{2}^2\right] 
>\end{align*}
>$$  
>where
>- the first term is the **reconstruction term** $$\mathbb{E}_{Z_{1} \sim q(\cdot|x) }\left[  \log  p(x|Z_{1}, w)  \right]$$
>- And the second term is the **consistency term** $$- \sum_{t=2}^{T}\mathbb{E}_{Z_{t}\sim q(\cdot|x) }\left[\mathbb{KL}\left( q(z_{t-1}| Z_{t}, x) \left\|\right. p(z_{t-1}\,|\,Z_{t}, w) \right)\right] $$ 
>	- This term measures the discrepancy between the **conditional reversal kernel** $q(z_{t-1}| z_{t}, x)$ and the **diffusion network** $p(z_{t-1}\,|\,Z_{t}, w)$ over each steps of Markov chain 
>	- Note that both probabilities are *Gaussian* $$\begin{align*}q(z_{t-1} | z_{t}, x) &= \mathcal{N}(z_{t-1}\;|\; m_{t}(x, z_{t}),\,\frac{1 - \alpha_{t-1}}{1- \alpha_{t}}\,\beta_{t}I)\\[5pt] p(z_{t-1}|z_{t}, w) &= \mathcal{N}(z_{t-1}\;|\; \mu(z_{t}, w, t),\, \beta_{t}I)\end{align*}$$
>	- The *KL-divergence of Gaussian* can be written in closed form $$\mathbb{KL}\left( q(z_{t-1}| Z_{t}, x) \left\|\right. p(z_{t-1}\,|\,Z_{t}, w) \right) = \frac{1}{2\beta_{t}}\lVert m_{t}(x, z_{t}) - \mu(z_{t}, w, t) \rVert_{2}^2 + \text{ const.} $$
>


- [[Chain Rule of Kullback-Leibler Divergence]]
- [[Conditional Kullback-Leibler Divegence]]
- [[Kullback-Leibler Divergence of Gaussian Distributions]]
- [[Kullback-Leibler Divergence]]


>[!info]
>$$
>\begin{align*}
>&\mathbb{KL}\left( q(z_{t-1}| Z_{t}, x) \left\|\right. p(z_{t-1}\,|\,Z_{t}, w) \right) \\[5pt]
>&= \frac{1}{2}(m_{t}(x, z_{t}) - \mu(z_{t}, w,t))^{T}\left(\beta_{t}I\right)^{-1}(m_{t}(x, z_{t}) - \mu(z_{t}, w,t)) + \frac{1}{2}\log \left(\frac{\det(\beta_{t}I)}{\det(\sigma^2_{t}I)}\right)- \frac{d}{2} + \frac{1}{2}\text{tr}\left(\left(\beta_{t}I\right)^{-1}(\sigma^2_{t}I)\right)\\[5pt]
>&= \frac{1}{2\beta_{t}}\lVert  m_{t}(x, z_{t}) - \mu(z_{t}, w,t)\rVert^2 + \frac{d}{2}\log \left( \frac{1-\alpha_{t} }{1- \alpha_{t-1}}\right) - \frac{d}{2} + \frac{d}{2} \left( \frac{1-\alpha_{t-1} }{1- \alpha_{t}}\right) 
>\end{align*}
>$$


 >[!important] Definition
>Finally the **training objective** for **DDPM** can be approximated using **Monte Carlo Estimator**
>$$
>\begin{align*}
>\mathcal{L}(q, w; x) &\propto \mathbb{E}_{Z_{1} \sim q(\cdot|x) }\left[\log p(x|Z_{1}, w) \right] - \frac{1}{2\beta_{t}}\sum_{t=2}^{T}\mathbb{E}_{Z_{t} \sim q(\cdot|x)  }\left[\lVert m_{t}(x, Z_{t}) - \mu(Z_{t}, w, t) \rVert_{2}^2\right] \\[5pt]
>&\approx \frac{1}{L_{1}}\sum_{k_{1}=1}^{L_{1}}\log p(x|Z_{1}^{(k_{1})}, w) - \frac{1}{2\beta_{t}}\sum_{t=2}^{T}\frac{1}{L_{t}}\sum_{k_{t}=1}^{L_{t}} \lVert m_{t}(x, Z_{t}^{(k_{t})} - \mu(Z_{t}^{(k_{t})}, w, t) \rVert_{2}^2
>\end{align*}
>$$
>where
>- at **each stage** $t$ of **forward pass**, denote the *batch of random samples of latent representation* as 
>  $$\{ Z_{t}^{(k_{t})} \}_{k_{t}=1}^{L_{t}} \sim \mathcal{N}(z_{t}\;|\; \sqrt{ \alpha_{t} }x,\, (1- \alpha_{t})I)$$ where $$\alpha_{t} = \prod_{s=1}^{t}(1 - \beta_{s})$$
>- and the **mean function** for **forward pass** $$\begin{align*}m_{t}(x, z_{t}) &:= \frac{1}{1- \alpha_{t}}\left[ (1- \alpha_{t-1})\sqrt{ 1- \beta_{t} }\,z_{t} + \sqrt{ \alpha_{t-1} }\,\beta_{t}\,x \right] \end{align*}$$
>- We can sample $\{ Z_{t}^{(k_{t})} \}_{k_{t}=1}^{L_{t}}$ using **Langevin dynamics.**

- [[Monte Carlo and Applications]]
- [[Markov Chain Monte Carlo Methods]]
- [[Langevin Dynamics and Langevin Sampling]]


>[!info]
>This objective is close to *VAE* but we have **multiple encoder-decoder stages**.

### Noise Prediction Formulation

>[!info]
>From $t$-step forward pass update $$z_{t} = \sqrt{ \alpha_{t} }x + \sqrt{ 1- \alpha_{t} }\epsilon_{t},$$ we can *recover* $x$ as $$x = \frac{1}{\sqrt{ \alpha_{t} }}z_{t} - \frac{\sqrt{ 1 - \alpha_{t} }}{\sqrt{ \alpha_{t} }}\epsilon_{t}$$
>
>Substitute above into mean function for **forward pass** 
>$$
>\begin{align*}
>m_{t}(x, z_{t}) &:= \frac{1}{1- \alpha_{t}}\left[ (1- \alpha_{t-1})\sqrt{ 1- \beta_{t} }\,z_{t} + \sqrt{ \alpha_{t-1} }\,\beta_{t}\,x \right] \\[5pt]
>&= \frac{1}{1- \alpha_{t}}\left[ (1- \alpha_{t-1})\sqrt{ 1- \beta_{t} }\,z_{t} + \sqrt{ \alpha_{t-1} }\,\beta_{t}\,\frac{1}{\sqrt{ \alpha_{t} }}z_{t} - \sqrt{ \alpha_{t-1} }\,\beta_{t}\,\frac{\sqrt{ 1 - \alpha_{t} }}{\sqrt{ \alpha_{t} }}\epsilon_{t}\right]\\[5pt]
>&= \frac{1}{1- \alpha_{t}}\left[ (1- \alpha_{t-1})\sqrt{ 1- \beta_{t} }\,z_{t} + \,\beta_{t}\,\frac{1}{\sqrt{ 1 - \beta_{t} }}z_{t} - \,\beta_{t}\,\frac{\sqrt{ 1 - \alpha_{t} }}{\sqrt{ 1 - \beta_{t} }}\epsilon_{t}\right]\\[5pt]
>&= \frac{1}{1- \alpha_{t}}\left[ (1- \alpha_{t-1})(1- \beta_{t}) \frac{1}{\sqrt{ 1 - \beta_{t} }}\,z_{t} + \,\beta_{t}\,\frac{1}{\sqrt{ 1 - \beta_{t} }}z_{t} - \,\beta_{t}\,\frac{\sqrt{ 1 - \alpha_{t} }}{\sqrt{ 1 - \beta_{t} }}\epsilon_{t}\right]\\[5pt]
>&= \frac{1}{1- \alpha_{t}}\left[ (1 - \alpha_{t-1} + \alpha_{t-1}\beta_{t})  \frac{1}{\sqrt{ 1 - \beta_{t} }}\,z_{t} - \,\beta_{t}\,\frac{\sqrt{ 1 - \alpha_{t} }}{\sqrt{ 1 - \beta_{t} }}\epsilon_{t}\right]\\[5pt]
>&= \left[ \frac{1}{\sqrt{ 1 - \beta_{t} }}\,z_{t} - \,\beta_{t}\,\frac{1}{\sqrt{ 1 - \alpha_{t} }\sqrt{ 1 - \beta_{t} }}\epsilon_{t}\right]\\[5pt]
>&= \frac{1}{\sqrt{ 1 - \beta_{t} }}\left[ z_{t} - \frac{\beta_{t}}{\sqrt{ 1 - \alpha_{t} }}\epsilon_{t}\right]
>\end{align*}
>$$
>note that $$1 - \alpha_{t} = 1- \alpha_{t-1}(1-\beta_{t}) = 1- \alpha_{t-1} + \alpha_{t-1}\beta_{t}$$



>[!important] Definition
>Let $g(z_{t}, w, t)$ be a *neural network* that predicts the **total noise component** that was added to the original data $x$ to create the *noisy data* $z_{t}$ at that step. 
>
>That is, we can *reparameterize* mean network as 
>$$\mu(z_{t}, w,t) = \frac{1}{\sqrt{ 1 - \beta_{t} }}\left[ z_{t} - \frac{\beta_{t}}{\sqrt{ 1 - \alpha_{t} }} g(z_{t}, w, t) \right] $$
>- The network $g$ corresponds to **Stein score function** $$\begin{align*} s(z_{t}, w, t) :=  -\frac{1}{\sqrt{ 1 - \alpha_{t} }} g(z_{t}, w, t) &= - \frac{z_{t} -\sqrt{1- \beta_{t}}\,\mu(z_{t}, w,t)}{\beta_{t}} \\[5pt] &= \nabla_{z_{t}} \log \mathcal{N}(z_{t}\;|\; \sqrt{1- \beta_{t}}\,\mu(z_{t}, w,t),\, \beta_{t}I) \\[5pt] &= \nabla_{z} \log p(z_{t} | \sqrt{1- \beta_{t}}\,\hat{z}_{t-1}, \beta_{t}I) \end{align*}$$ where $$\hat{z}_{t-1} = \mu(z_{t}, w,t)$$
>- The network $$s(z_{t}, w, t)$$ corresponds to the **noise conditional score network** in **Score-baesd Generative Model (SGM).**

^03693f

- [[Score Matching and Denoising Score Matching]]

>[!important] 
>Since the **mean function** of *forward pass* is of the form $$m_{t}(x, z_{t}) = \frac{1}{\sqrt{ 1 - \beta_{t} }}\left[ z_{t} - \frac{\beta_{t}}{\sqrt{ 1 - \alpha_{t} }}\epsilon_{t}\right],$$ the **KL-divergence** in the **consistent term** becomes 
>$$
>\begin{align*}
>\mathbb{KL}\left( q(z_{t-1}| Z_{t}, x) \left\|\right. p(z_{t-1}\,|\,Z_{t}, w) \right) &= \frac{1}{2\beta_{t}}\lVert m_{t}(x, z_{t}) - \mu(z_{t}, w, t) \rVert_{2}^2 + \text{ const.} \\[5pt]
>&= \frac{\beta_{t}}{2(1- \alpha_{t})(1- \beta_{t})}\lVert g(Z_{t}, w, t) - \epsilon_{t} \rVert_{2}^2 + \text{ const.} \\[5pt]
>\end{align*}
>$$

>[!info]
>Note that $$p(z_{t-1}\,|\,Z_{t}, w) = \mathcal{N}(z_{t-1}|\; \mu(z_{t}, w,t),\, \beta_{t}I)$$
>and
>$$q(z_{t-1} | z_{t}, x) = \mathcal{N}(z_{t-1}\;|\; m_{t}(x, z_{t}),\,\sigma_{t}^2I)$$
> where $$m_{t}(x, z_{t}) := \frac{1}{1- \alpha_{t}}\left[ (1- \alpha_{t-1})\sqrt{ 1- \beta_{t} }\,z_{t} + \sqrt{ \alpha_{t-1} }\,\beta_{t}\,x \right] $$ and$$\sigma_{t}^2 := \frac{1 - \alpha_{t-1}}{1- \alpha_{t}}\,\beta_{t}$$


>[!important]
>The **reconstruction term** becomes
>$$
>\begin{align*}
>\mathbb{E}_{Z_{1} \sim q(\cdot|x) }\left[  \log  p(x|Z_{1}, w)  \right]&= -\frac{1}{2(1 - \beta_{1})}\mathbb{E}_{Z_{1} \sim q(\cdot|x) }\left[  \lVert g(Z_{1}, w, 1) - \epsilon_{1} \rVert_{2}^2 \right] \\[5pt]
>&= -\frac{\beta_{1}}{2(1- \alpha_{1})(1- \beta_{1})}\lVert g(Z_{1}, w, 1) - \epsilon_{1} \rVert_{2}^2 + \text{ const.}
>\end{align*}
>$$
>since $1- \alpha_{1} = \beta_{1}.$

>[!info]
>This is obtained by noting that $$\log p(x|z_{1}, w) = \log \mathcal{N}(x|\, \mu(z_{1}, w, 1), \beta_{1}I) = -\frac{1}{2\beta_{1}} \lVert x -  \mu(z_{1}, w, 1)\rVert_{2}^2 + \text{ const.} $$ 
>
>We substitute 
>- $$\mu(z_{t}, w,1) = \frac{1}{\sqrt{ 1 - \beta_{1} }}\left[ z_{1} - \frac{\beta_{1}}{\sqrt{ 1 - \alpha_{1} }} g(z_{1}, w, 1) \right]$$ 
>- and $$x = \frac{1}{\sqrt{ \alpha_{1} }}z_{1} - \frac{\sqrt{ 1 - \alpha_{1} }}{\sqrt{ \alpha_{1} }}\epsilon_{1}$$
>- and use the fact that $$\alpha_{1} =  1- \beta_{1}.$$

>[!important] Definition
>The **ELBO training objective function** for *DDPM* with respect to **noise network** $g(z_{t}, w, t)$ is given by 
>$$
>\begin{align*}
>\mathcal{L}(q, w; x) &\propto \mathbb{E}_{q }\left[\log p(x|Z_{1}, w) \right] - \sum_{t=2}^{T}\mathbb{E}_{Z_{t}\sim q(\cdot|x) }\left[\mathbb{KL}\left( q(Z_{t-1}| Z_{t}, x) \left\|\right. p(Z_{t-1}\,|\,Z_{t}, w) \right)\right] \\[5pt]
>&=-\sum_{t=1}^{T}\frac{\beta_{t}}{2(1- \alpha_{t})(1- \beta_{t})}\mathbb{E}_{Z_{t} \sim q(\cdot|x) }\left[\lVert g(Z_{t}, w, t) - \epsilon_{t} \rVert_{2}^2\right] \\[5pt]
>&=-\sum_{t=1}^{T}\frac{\beta_{t}}{2(1- \alpha_{t})(1- \beta_{t})}\mathbb{E}_{\epsilon_{t} \sim \mathcal{N}(0, I)}\left[\lVert g(\sqrt{ \alpha_{t} }x + \sqrt{ 1- \alpha_{t} }\epsilon_{t}, w, t) - \epsilon_{t} \rVert_{2}^2\right] \\[5pt]
>\end{align*}
>$$  
>
>We can further omit the factor $\beta_{t} / (2(1- \alpha_{t})(1- \beta_{t}))$, and obtain the **regression loss function** for **noise recontruction**
>$$
>\mathcal{L}(w) = \sum_{t=1}^{T}\mathbb{E}_{x\sim \mathcal{D}, \epsilon_{t} \sim \mathcal{N}(0, I) }\left[\lVert g\left(\sqrt{ \alpha_{t} }x + \sqrt{ 1- \alpha_{t} }\epsilon_{t}, w, t\right) - \epsilon_{t} \rVert_{2}^2\right] 
>$$
>which is approximated with *Monte Carlo sampling*
>$$
>\hat{\mathcal{L}}(w) = \sum_{t=1}^{T}\sum_{k=1}^{L}\lVert g(Z_{t}^{(k)}, w, t) - \epsilon_{t} \rVert_{2}^2
>$$

^bf4525

- [[Minimum Mean Square Estimation]]
- [[Regression Problem]]
- [[Least Square Estimation]]
- [[Score Matching and Denoising Score Matching]]

### Training Algorithm

>[!important] Definition
>The **training algorithm** for  **denoising diffusion probabilistic model (DDPM)** solves the noise regression problem 
>$$
>\min_{w}\hat{\mathcal{L}}_{T}(w) = \min_{w}\sum_{t=1}^{T}\sum_{k=1}^{L}\lVert g(Z_{t}^{(k)}, w, t) - \epsilon_{t} \rVert_{2}^2
>$$
>which is described as below:
>- *Require*: training data $\mathcal{D}$
>- *Require*: a sequence of **noise schedule** $\{ \beta_{t} \}_{t=1}^{T}$ where  $$\beta_{1} < \beta_{2} \,{<}\ldots{<}\,\beta_{T} < 1$$
>- Set $\alpha_{0} = 1$
>- For $t=1\,{,}\ldots{,}\,T$:
>	- Compute the parameter $\alpha_{t}$ for **$t$-step transition kernel** $q(z_{t}|x)$ as $$\alpha_{t} = (1 - \beta_{t})\,\alpha_{t-1}$$
>- While not *convergence*:
>	- Sample *input data* $x\sim \mathcal{D}$
>	- Sample the **time-step of diffusion chain** $$t \sim \text{Uniform}\{ 1\,{,}\ldots{,}\, T\}$$
>	- Sample **noise vector** from standard normal distribution $$\epsilon_{t} \sim \mathcal{N}(\epsilon|\,0, I)$$
>	- Generate **latent variable** $Z_{t}$ from $x$ based on the *$t$-step transition kernel* $q(z_{t}|x)$, i.e. $$Z_{t} = \sqrt{ \alpha_{t} }\,x + \sqrt{ 1- \alpha_{t} }\,\epsilon_{t}$$
>	- Set the *loss function* as $$\hat{\mathcal{L}}(w) = \lVert g(Z_{t}, w, t) - \epsilon_{t} \rVert_{2}^2$$ where $g(z_{t}, w, t)$ is the *decoder network*
>	- Take optimization step based on *stochastic gradient descent* $$w \leftarrow w - \eta \nabla \hat{\mathcal{L}}(w)$$
>- *Return*: decoder network parameter $w$


>[!info]
>Although both *forward pass* and *reverse pass* are based on **Markov chain**,  the *DDPM training* be **paralleled**. This is because
>- the **$t$-step transition kernel**  $q(z_{t}|x)$ can be obtained in closed form, which allows us to **"jump"** to any intermediate state $z_{t}$ of Markov chain
>- and the training objective *aggregates all times* together
>
>This requires the algorithm to first **sample time-stamp** $t$ of Markov chain  and then to directly generates intermediate states from **initial state**

>[!info]
>For general Markov chain, only when the **chain is mixed** i.e. the *state distribution* is **stationary** can we jump to any intermediate state without knowing proceding state.

- [[Invariant Measure and Stationary Distribution]]
- [[Ergodic Markov Chain and Asymptotic of Transition Kernel]]

## Reversed Pass and Sampling Procedure

>[!important] Definition
>The **reverse pass** or **sampling procedure** from **DDPM** is described as below:
>- *Require*: learned network parameter $w$ for noise network $g(z_{t}, w, t)$
>- *Require*: *transition parameters* $\{ \beta_{t} \}_{t=1}^{T}$
>- Sample $z_{T}$ from *standard normal distribution* $$z_{T} \sim \mathcal{N}(z_{T}|0, I)$$
>- For $t= T,T-1,\,{,}\ldots{,}\,1$:
>	- Compute $\alpha_{t}$ as $$\alpha_{t} = \prod_{s=1}^{t}(1- \beta_{s})$$
>	- **Generate** $z_{t-1}$ given $z_{t}$ in three steps:
>		- Evaluate the output of **noise neural network** $g(z_{t}, w,t)$
>		- Evaluate the output of **decoder network** given by $$\mu(z_{t}, w,t) = \frac{1}{\sqrt{ 1 - \beta_{t} }}\left[ z_{t} - \frac{\beta_{t}}{\sqrt{ 1 - \alpha_{t} }} g(z_{t}, w, t) \right] $$
>		- Generate $z_{t-1}$ according to *Gaussian distribution* $\mathcal{N}(z_{t-1}\,|\,\mu(z_{t}, w,t), \,\beta_{t}I  )$ by $$z_{t-1} = \mu(z_{t}, w,t) + \sqrt{ \beta_{t} }\epsilon$$ where $\epsilon \sim \mathcal{N}(0, I)$
>- *Return*: 
>	- the reverse Markov chain $$z_{T},\, z_{T-1}\,{,}\ldots{,}\,z_{1},\,x.$$  
>	- and a *new sample* $x$


## Explanation

>[!quote]
>The central idea is to take each training image and to **corrupt** it using a **multi-step noise process** to transform it into a sample from a *Gaussian distribution*. This is illustrated in Figure 20.1. A deep neural network is then trained to **invert this process**, and once trained the network can then *generate new images* starting with samples from a Gaussian as input.
>— [[Deep Learning Foundations and Concepts by Bishop]] pp 582

![[diffusion_network_process.png]]

- [[Langevin Equation]]
- [[Fokker–Planck and Kolmogorov Forward-Backward Equation]]
- [[Diffusion Process]]
- [[Stochastic Differential Equations]]

>[!info]
>The sequence of conditional distributions forms a **Markov chain** and can be expressed as a **probabilistic graphical model**

- [[Gaussian Bayesian Network]]
- [[Dynamic Bayesian Network]]
- [[Markov Chain and Markov Process]]

>[!info]
>A **diffusion model** is a **decoder-only architecture.** 

>[!info]
>For **each stage** $t$, there is an encoding and decoding step in DDPM
>- *encoding*: $$z_{t} = \sqrt{ 1 - \beta_{t} }\,z_{t-1} + \sqrt{ \beta_{t} }\epsilon_{t}$$
>- *decoding*:  $$\hat{z}_{t-1} = \mu(z_{t}, w,t) + \sqrt{ \beta_{t} }\epsilon_{t}$$
>
>Thus $$\hat{z}_{t-1} = \mu\left(\sqrt{ 1 - \beta_{t} }\,z_{t-1} + \sqrt{ \beta_{t} }\epsilon_{t},\; w,\;t\right) + \sqrt{ \beta_{t} }\epsilon_{t}$$  



### Which part is Exact and Which part is Approximation?

>[!info]
>The **exact formulation** in **DDPM**
>- The **forward transition kernel** and **forward 1-step transition**: $$z_{t} = \sqrt{ 1 - \beta_{t} }\,z_{t-1} + \sqrt{ \beta_{t} }\epsilon_{t}$$ and $$q(z_{t}|z_{t-1}) = \mathcal{N}(z_{t}\;|\; \sqrt{ 1 - \beta_{t} }\,z_{t-1},\; \beta_{t}I)$$
>- The **forward diffusion kernel** or the **forward $t$-step transition** from input $x$: $$q(z_{t}\,|\,x) = \mathcal{N}(z_{t}\;|\; \sqrt{ \alpha_{t} }x,\, (1- \alpha_{t})I)$$ and $$z_{t} = \sqrt{ \alpha_{t} }x + \sqrt{ 1 - \alpha_{t} }\epsilon_{t}$$
>- The **forward joint distribution** given $x$ $$q_{\text{forward}}(z_{1} \,{,}\ldots{,}\,z_{T}\,|\,x) = q(z_{1}\,|\,x)\,\prod_{t=2}^{T}q(z_{t}\,|\,z_{t-1})$$
>- The **conditional reversed kernel** given input $x$, i.e. $$q(z_{t-1} | z_{t}, x) = \mathcal{N}(z_{t-1}\;|\; m_{t}(x, z_{t}),\,\sigma_{t}^2I)$$ 
>	-  the **posterior mean function** $$\begin{align} m_{t}(x, z_{t}) &:= \frac{1}{1- \alpha_{t}}\left[ (1- \alpha_{t-1})\sqrt{ 1- \beta_{t} }\,z_{t} + \sqrt{ \alpha_{t-1} }\,\beta_{t}\,x \right] \\[5pt] m_{t}(z_{t}, \epsilon_{t}) &= \frac{1}{\sqrt{ 1 - \beta_{t} }}\left[ z_{t} - \frac{\beta_{t}}{\sqrt{ 1 - \alpha_{t} }}\epsilon_{t}\right]\end{align}$$
> 	- the **posterior variance** $$\sigma_{t}^2 := \frac{1 - \alpha_{t-1}}{1- \alpha_{t}}\,\beta_{t}$$

>[!info]
>The **approximated formulation** in **DDPM**:
>- The **time-reversed $1$-step transition kernel**: $$\begin{align*} q(z_{t-1}|z_{t}) \approx p(z_{t-1}|z_{t}, w) := \mathcal{N}(z_{t-1}\;|\; \mu(z_{t}, w, t),\, \beta_{t}I) \end{align*}$$
>	- the *posterior mean function* is the **diffusion network** which is called in each reversed transition $$\mu(z_{t}, w, t)$$
>	- Define the **noise network** $g(z_{t}, w, t)$ which is given by $$\mu(z_{t}, w,t) = \frac{1}{\sqrt{ 1 - \beta_{t} }}\left[ z_{t} - \frac{\beta_{t}}{\sqrt{ 1 - \alpha_{t} }} g(z_{t}, w, t) \right] $$
>	- The **time-revserd transition** $$z_{t-1} \approx  \hat{z}_{t-1} = \mu(z_{t}, w,t) + \sqrt{ \beta_{t} }\epsilon_{t}$$
>- The **time-reversed joint distribution**: $$q_{\text{reverse}}(z_{T}\,{,}\ldots{,}\,z_{1}, x) \approx p(z_{T})\prod_{t=2}^{T}p(z_{t-1}|z_{t}, w)\,p(x|z_{1}, w)$$  
>- The **training objective function**: $$\mathcal{L}(q, w; x) \propto \mathbb{E}_{q }\left[\log p(x|z_{1}, w) + \sum_{t=2}^{T}\log\left(\frac{p(z_{t-1}\,|\,z_{t}, w)}{q(z_{t-1}| z_{t}, x)}\right) \right] $$  
>	- It is further estimated using **Monte Carlo estimator** $$\begin{align*}\mathcal{L}(q, w; x) &\propto \mathbb{E}_{Z_{1} \sim q(\cdot|x) }\left[\log p(x|Z_{1}, w) \right] - \frac{1}{2\beta_{t}}\sum_{t=2}^{T}\mathbb{E}_{Z_{t} \sim q(\cdot|x)  }\left[\lVert m_{t}(x, Z_{t}) - \mu(Z_{t}, w, t) \rVert_{2}^2\right] \\[5pt] &\approx \frac{1}{L_{1}}\sum_{k_{1}=1}^{L_{1}}\log p(x|Z_{1}^{(k_{1})}, w) - \frac{1}{2\beta_{t}}\sum_{t=2}^{T}\frac{1}{L_{t}}\sum_{k_{t}=1}^{L_{t}} \lVert m_{t}(x, Z_{t}^{(k_{t})}) - \mu(Z_{t}^{(k_{t})}, w, t) \rVert_{2}^2 \end{align*}$$
>	- Using the *noise network*, the reformulation of training objective as $$\begin{align*}\mathcal{L}(w) &= \sum_{t=1}^{T}\mathbb{E}_{Z_{t} \sim q(\cdot|x) }\left[\lVert g(Z_{t}, w, t) - \epsilon_{t} \rVert_{2}^2\right] \\[5pt] &\approx \sum_{t=1}^{T}\sum_{k=1}^{L}\lVert g(Z_{t}^{(k)}, w, t) - \epsilon_{t} \rVert_{2}^2\end{align*}$$ 


## Variational Auto-encoder

>[!quote]
>Diffusion models can be viewed as a form of **hierarchical variational autoencoder** in which the *encoder distribution is fixed*, and defined by the noise process, and *only the generative distribution is learned* (Luo, 2022). They are easy to train, they scale well on parallel hardware, and they avoid the challenges and instabilities of adversarial training while producing results that have quality comparable to, or better than, **generative adversarial networks**. However, generating new samples can be *computationally expensive* due to the need for multiple forward passes through the decoder network (Dhariwal and Nichol, 2021).
>— [[Deep Learning Foundations and Concepts by Bishop]] pp 582

>[!info]
>Compare the **variational autoencoder (VAE)**, and **denoising diffusion probabilistic network (DDPM)**:
>- **Architecture**:
>	- *VAE*: one pair of **encoder network** + **decoder network**; 
>	- *DDPM*:    a **fixed forward diffusion kernel**  + one **diffusion network** to approximate *reversed transition kernel*
>- **Training process**
>	- *VAE*: 
>		- *forward pass*: choose data $\to$ encoder $\to$ generate internal representation $\to$ decoder $\to$ predict denoised data
>		- *backward pass*:  predict denoised data $\to$ accumulate loss from internal samples $\to$ adjust decoder $\to$ adjust encoder
>	- *DDPM*: 
>		- *forward pass*: choose data **+ time** $\to$ directly transition to latent state  **+ time** $\to$ decoder $\to$ predict denoised data  **+ time**
>		- *reverse pass*: predict denoised data **+ time** $\to$ accumulate loss from internal samples *over multiple stages* $\to$ adjust decoder
>-  **Training Objective**
>	- Both *VAE* and *DDPM* uses **ELBO** to derive the training objective
>	- *VAE*: $$\mathcal{L}(\psi, \theta; x) = \mathbb{E}_{ q(z|x, \psi) }\left[\log p(x | z, \theta)\, + \log \left(\frac{p(z) }{q(z|x, \psi)}\right) \right].$$
>		- The  distribution $q_{\psi}$ for expectation is **variational** at training stage.
>		- *Both* $(\psi, \theta)$ are trainable
>		- Updating from *both sides* of KL divergence
>		- Requires multiple rounds with coordinate ascent on $\psi$ and $\theta$, respectively
>	- *DDPM*:  $$\mathcal{L}(q, w; x) \propto \mathbb{E}_{q }\left[\log p(x|z_{1}, w) + \sum_{t=2}^{T}\log\left(\frac{p(z_{t-1}\,|\,z_{t}, w)}{q(z_{t-1}| z_{t}, x)}\right) \right] $$  
>		- The distribution $q$ for expectation is **fixed** at training stage.
>		- *Only* $w$ is trainable.
>		- Updating only on *right side* of KL divergence
>		- *Score matching loss*
>- **Encoding step**
>	- *VAE*: 
>		- $$Z^{j} = \mu_{j}(x; \psi)\, + \sigma_{j}(x; \psi)\,\epsilon^{j}$$
>		- The *variational encoder* need to be **learned**
>		- *One step* transition 
>	- *DDPM*:
>		- $$Z_{t} = \sqrt{ \alpha_{t} }\,x + \sqrt{ 1-\alpha_{t} }\epsilon_{t}$$
>		- The transition is via *additive Gaussian white noise*
>		- The *forward pass*  is **not learned**
>		- *Multiple step* transitions 
>-  **Encoder**
>	- *VAE*: A **encoder network** $$q(z|x; \psi) := \mathcal{N}(z\;|\; \mu(x, \psi)\,,\, \text{diag}(\sigma_{j}^2(x, \psi))\,I)$$
>	- *DDPM*: A **multi-step diffusion process** with a *Gaussian  transition kernel* $$q(z_{t}|x; \alpha_{t}) = \mathcal{N}(z_{t} | \sqrt{ \alpha_{t} }x, (1-\alpha_{t})I)$$ 
>-  **Decoder**
>	- *VAE*: direct map from latent space to data space based on the  *decoder network* $$g(z; \theta) := p(x|z; \theta)$$
>	- *DDPM*: need to run through entire *time-reversed diffusion process* from latent space to data space based on the  *diffusion network* transition $$q(z_{t-1}|z_{t}) \approx p(z_{t-1}|z_{t}, w) := \mathcal{N}(z_{t-1}\;|\; \mu(z_{t}, w, t),\, \beta_{t}I)$$
>- **Generation / Decoding process**
>	- *VAE*: sample from white noise $\to$ decoder $\to$ new sample; $$z \to g(z; \theta) \to x$$
>	- *DDPM*: sample via a **time-reversed Markov chain**; $$z_{T}  \to \mu(z_{T}, w, T) \to z_{T-1} \to \mu(z_{T-1}, w, T-1) \,{\to}\ldots{\to}\,z_{1} \to \mu(z_{1}, w, 1) \to x$$

^90afc8

- [[Variational Auto-Encoder]]
- [[Generative Adversarial Network]]

## Denoising Score Matching

![[Diffusion Network Score Matching Equivalence#^af368e]]

- [[Diffusion Network Score Matching Equivalence]]
- [[Score Matching and Denoising Score Matching]]


## Stochastic Differential Equation for DDPM

>[!quote]
>The **main drawback** of *diffusion models* for generating data is that they require **multiple sequential inference passes** through the *trained network*, which can be computationally expensive. One way to speed up the sampling process is first to convert the **denoising process** to a **differential equation over continuous time** and then to use alternative efficient *discretization methods* to solve the equation efficiently.
>
>-- [[Deep Learning Foundations and Concepts by Bishop]] pp 593

![[Continuous-Time Diffusion Network via Stochastic Differential Equations#^82e0e4]]


- [[Continuous-Time Diffusion Network via Stochastic Differential Equations]]


-----------
##  Recommended Notes and References


- [[Score-based Generative Models or SGMs]]
- [[Probabilistic Graphical Models]]
- [[Stochastic Process]]
- [[Wasserstein Distance]]
- [[Gaussian Process]]


- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 857
- [[Deep Learning Foundations and Concepts by Bishop]] pp 581 - 603
- Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. _Advances in neural information processing systems_, _33_, 6840-6851.
- Song, J., Meng, C., & Ermon, S. (2020.) Denoising Diffusion Implicit Models. In _International Conference on Learning Representations_. 
- Kawar, B., Elad, M., Ermon, S., & Song, J. (2022). Denoising diffusion restoration models. _Advances in Neural Information Processing Systems_, _35_, 23593-23606.