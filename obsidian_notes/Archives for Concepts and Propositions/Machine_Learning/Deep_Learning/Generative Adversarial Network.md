---
tags:
  - concept
  - machine_learning/models
  - deep_learning/generative_models
  - math/game_theory
keywords:
  - generative_adversarial_network
topics:
  - deep_learning/generative_models
  - game_theory
name: Generative Adversarial Network
date of note: 2024-05-12
---

## Concept Definition

>[!important]
>**Name**: Generative Adversarial Network

### Implicit Probabilistic Models

>[!quote]
>To develop a probabilistic formulation for GANs, it is useful to first distinguish between two types of probabilistic models: “**prescribed probabilistic models**” and “**implicit probabilistic models**” [DG84]. 
>- **Prescribed probabilistic models**, which we will call **explicit probabilistic models**, provide an explicit parametric specification of the distribution of an observed random variable $x$, specifying a *log-likelihood function* $\log q_{\theta}(x)$ with parameters $\theta$. Most models we encountered in this book thus far are of this form, whether they be state-of-the-art classifiers, large-vocabulary sequence models, or fine-grained spatio-temporal models. 
>- Alternatively, we can specify an **implicit probabilistic model** that defines a *stochastic procedure* to *directly generate data*. Such models are the natural approach for problems in climate and weather, population genetics, and ecology, since the mechanistic understanding of such systems can be used to *directly describe the generative model*. We illustrate the difference between implicit and explicit models in Figure 26.1.
>
>-- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 883

>[!important] Definition
>**Implicit generative models** do not include a *likelihood function* or *observation model*. 
>- Instead, the *generating procedure* defines a valid *density* on the *output space* that forms an effective likelihood function: $$X = G_{\theta}(Z), \quad Z \sim \mathcal{P}$$ where $G_{\theta}$ is the *cumulative distribution function* of $X$ with respect to measure $\mathcal{P}$.
>- The p.d.f. of $X$ is given by $$p_{X}(x) = \frac{\partial}{ \partial x^{1}}\,{}\ldots{}\,\frac{\partial}{ \partial x^{d}}\int_{z: G_{\theta}(z) \le x}\,p_{Z}(z)\,dz$$
>	- We can use neural network to approximate $G_{\theta}$. 
>	- Such models are sometimes called **generator networks** or **generative neural samplers**; they can also be throught of as **differentiable simulators**.
>- The problem of learning an intractable implicit generative model is called **likelihood-free inference** or **simulation-based inference**.

- [[Fundamental Theorem of Simulation]]
- [[Cumulative Distribution Function of Random Variable]]
- [[Quantile Function]]

### Principle of Learning by Comparison

![[Principle of Learning by Comparison for Implicit Generative Models#^d5e48f]]

- [[Principle of Learning by Comparison for Implicit Generative Models]]

### Training Objective

![[Principle of Learning by Comparison for Implicit Generative Models#^b00731]]

>[!important] Definition
>Let $p^{*}$ be the unknown true data distribution, and $q_{\theta}$ is a distribution from **generative model** $\mathcal{P}_{\Theta}.$
>
>Let $X$ and $X'$ be samples generated by true data distribution $p^{*}$ and generative model* $q_{\theta}$
>$$
>X\sim p^{*}, \quad X' \sim q_{\theta}(Z),\, \; Z\sim p(z)
>$$
>The **training objective** for **Generative Adversarial Network (GAN)** is given by 
>$$
>\min_{\theta}\,\max_{\psi}\,V(\theta, \psi)
>$$
>where the loss is given by the *binary cross-entropy*
>$$
>V(\theta, \psi) := \mathbb{E}_{p^{*}}\left[ \log \left(  D_{\psi}(X) \right)  \right] +  \mathbb{E}_{Z\sim p(z)}\left[ \log \left( 1 - D_{\psi}(g_{\theta}(Z)) \right) \right]  
>$$
>
>At iteration $t$, the *training* for *GAN* consists of two steps
>- Given $\theta^{(t-1)}$,  training of **discriminator** or **critic** $D_{\psi}$ via 
>$$
>\begin{align*}
>\psi^{(t)} &= \arg\max_{\psi}\left\{ \mathbb{E}_{p^{*}}\left[ \log \left(  D_{\psi}(X) \right)  \right] +  \mathbb{E}_{Z\sim p(z)}\left[ \log \left( 1 - D_{\psi}(g_{\theta^{(t-1)}}(Z)) \right) \right]  \right\} 
> \end{align*}
>$$
>- Given $\psi^{(t)}$, training of **generator** or *generative model* $g_{\theta}$ via 
>$$
>\begin{align*}
>\theta^{(t)} &= \arg\min_{\theta}\left\{ \mathbb{E}_{p^{*}}\left[ \log \left(  D_{\psi^{(t)}}(X) \right)  \right] +  \mathbb{E}_{Z\sim p(z)}\left[ \log \left( 1 - D_{\psi^{(t)}}(g_{\theta}(Z)) \right) \right]  \right\} 
> \end{align*}
>$$
>
>Note that both expectations are estimated using *Monte Carlo estimator.*

- [[Monte Carlo and Applications]]

### Game Perspective

>[!important] Defintion
>The *training objective* for **Generative Adversarial Network (GAN)** can be seen as a **two-player zero-sum game** where the two players are
>1. the **discriminator** or **critic**, whose objective is 
>$$
>\begin{align*}
>\psi &= \arg\max_{\psi}\left\{ \mathbb{E}_{p^{*}}\left[ \log \left(  D_{\psi}(X) \right)  \right] +  \mathbb{E}_{Z\sim p(z)}\left[ \log \left( 1 - D_{\psi}(g_{\theta}(Z)) \right) \right]  \right\} 
> \end{align*}
>$$
>2. the **generator**, whose objective is 
>$$
>\begin{align*}
>\theta &= \arg\min_{\theta}\left\{ \mathbb{E}_{p^{*}}\left[ \log \left(  D_{\psi}(X) \right)  \right] +  \mathbb{E}_{Z\sim p(z)}\left[ \log \left( 1 - D_{\psi}(g_{\theta}(Z)) \right) \right]  \right\} 
> \end{align*}
>$$
>
>The optimal solution of generative training is given by the **min-max problem**
>$$
>\begin{align*}
>V(\theta^{*}, \psi^{*}) &= \min_{\theta}\max_{\psi}\left\{ \mathbb{E}_{p^{*}}\left[ \log \left(  D_{\psi}(X) \right)  \right] +  \mathbb{E}_{Z\sim p(z)}\left[ \log \left( 1 - D_{\psi}(g_{\theta}(Z)) \right) \right]  \right\} \\[5pt]
>&= \min_{\theta}\mathbb{D}_{JSD}\left( p^{*} \left\|\right. q_{\theta} \right) 
> \end{align*}
>$$
>- This solution $\theta^{*}$ is the **Nash equilibrium** of the game
>- The **value** of the **game** is measured via the **minimal Jensen-Shannon divergence.**
>- The **minimizer** of *Jensen-Shannon divergence* is the solution $\theta^{*}$.

- [[Von Neumann Min-Max Theorem]]
- [[Two-Player Finite Game and Matrix Representation]]
- [[Jensen-Shannon Divergence]]


>[!quote]
>Intuitively 
>- the *discriminator training criteria* needs to ensure that the discriminator **can distinguish** between **data** and **model samples**, 
>- while the *generator loss function* needs to ensure that **model samples** are **indistinguishable** from data according to the *discriminator*.
>
>-- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 895

### Generative Adversarial Networks 

![[generative_adversarial_network.png]]

>[!important] Defintion
>The **training algorithm** for **generative adversarial network (GAN)** is given by followings
>- *Require*: training data $X\sim p^{*}$
>- *Require*: $\alpha_{\theta} >0$ and $\alpha_{\psi} >0$
>- Initial parameter $\psi, \theta$
>- For $t=1\,{,}\ldots{,}\,$:
>	- For $k=1\,{,}\ldots{,}\,K$
>		- Sample **real** training data $X\sim p^{*}$
>		- Sample *noise* $Z_{k}\sim \mathcal{N}(0,I)$
>		- **Generate** *synthetic data* $$X_{k}' = g_{\theta^{(t-1)}}(Z_{k})$$
>		- Update parameter $\psi$ of **discriminator** or **critic** $D_{\psi}$ via *SGD* $$\begin{align*}\psi^{(t,k)} &= \psi^{(t, k-1)} + \alpha_{\psi} \nabla_{\psi}\,L_{D}(\psi^{(t, k-1)}, \theta^{(t-1)})\end{align*}$$ where
>			- $$\nabla_{\psi}\,L_{D}(\psi, \theta^{(t-1)})  = \nabla_{\psi}\,\log \left(  D_{\psi}(X) \right) + \nabla_{\psi}\,\log \left( 1 - D_{\psi}(X_{k}') \right)$$
>	- Sample *noise* $Z \sim \mathcal{N}(0,I)$
>	- Update parameter $\theta$ of **generator** $g_{\theta}$ via *SGD* $$\begin{align*}\theta^{(t)} &= \theta^{(t-1)} + \alpha_{\theta} \nabla_{\theta}\,L_{G}(\psi^{(t, K)}, \theta^{(t-1)})\end{align*}$$ where
>		- $$\nabla_{\theta}\,L_{G}(\psi^{(t, K)}, \theta)  =  \nabla_{\theta}\log \left( 1 - D_{\psi^{(t, K)}}(g_{\theta}(Z)) \right) $$

- [[Stochastic Gradient Descent Algorithm]]


## Explanation

>[!info]
>Compared to **explicit generative models** such as 
>- [[Gaussian Process]]
>- [[Factor Analysis]]
>- [[Variational Auto-Encoder]]
>- [[Denoising Diffusion Probabilistic Models and Diffusion Network]]
>- [[Normalizing Flows]]
>  
>the *Generative Adversarial Network (GAN)* do *not* need to know the **likelihood function** class before. Instead, it generate samples based on **simulation**.

- [[Monte Carlo and Applications]]

>[!info]
>*Likelihood-free inference* also forms the basis of the field known as **approximate Bayesian computation** or **ABC**.
>- Both approximate Bayesian and GAN rely on a learning principle based on **_comparing real and simulated data_**.
>  
>-- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 884  


## Variants

### $f$-GAN

![[f-Generative Adversarial Network#^ebb70c]]

- [[f-Generative Adversarial Network]]

### Wasserstein GAN

![[Wasserstein Generative Adversarial Network#^ebb70c]]

- [[Wasserstein Generative Adversarial Network]]



### Conditional GAN

- [[Conditional Generative Adversarial Network]]


### Deep Convolutional GAN or DCGAN


- [[Convolutional Neural Network]]

### Self-Attention GAN or SAGAN


- [[Attention Mechanism in Neural Network]]
- 





-----------
##  Recommended Notes and References



- [[Artificial Neural Network and Deep Learning]]


- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 883 - 914
- [[Deep Learning Foundations and Concepts by Bishop]] pp 533 - 544
- [[Deep Learning by Goodfellow]] pp 679, 690
- [[Foundations of Computer Vision by Torralba]] pp 487 - 489