---
tags:
  - concept
  - deep_learning/architecture
  - deep_learning/models
  - natural_language_processing/large_language_models
keywords:
  - large_language_models
  - pretrained_language_models
topics:
  - deep_learning/large_language_models
  - deep_learning/generative_models
  - deep_learning/sequential_networks
  - deep_learning/discriminative_models
  - natural_language_processing/large_language_models
name: Large Language Model and Pretrained Language Models
date of note: 2024-10-21
---

## Concept Definition

>[!important]
>**Name**: Large Language Model and Pretrained Language Models



### Encoder-Only Transformer Architecture

- [[Bidirectional Encoder Representation from Transformer or BERT]]

### Decoder-Only Transformer Architecture

- [[Generative Pre-trained Transformer or GPT]]
- [[Beam Search as Greedy Decoding]]

### Encoder-Decoder Transformer Architecture

- [[Text-to-Text Transfer Transformer or T5 for Translation]]
- [[Beam Search as Greedy Decoding]]



## Explanation





-----------
##  Recommended Notes and References

- [[Attention Mechanism in Neural Network]]
- [[Transformer Network]]
- [[Scaling Law of Large Language Model]]


- [[Encoder-Decoder Sequence-to-Sequence Architecture]]
- [[Bidirectional Encoder Representation from Transformer or BERT]]
- [[Generative Pre-trained Transformer or GPT]]
- [[Text-to-Text Transfer Transformer or T5 for Translation]]
- [[liuRoBERTaRobustlyOptimized2019]]
- [[touvronLlamaOpenFoundation2023]]
