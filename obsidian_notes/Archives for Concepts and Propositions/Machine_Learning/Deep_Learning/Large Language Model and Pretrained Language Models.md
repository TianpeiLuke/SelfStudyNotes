---
tags:
  - concept
  - deep_learning/architecture
  - deep_learning/models
  - natural_language_processing/large_language_models
keywords:
  - large_language_models
  - pretrained_language_models
topics:
  - deep_learning/large_language_models
  - deep_learning/generative_models
  - deep_learning/sequential_networks
  - deep_learning/discriminative_models
  - natural_language_processing/large_language_models
name: Large Language Model and Pretrained Language Models
date of note: 2024-10-21
---

## Concept Definition

>[!important]
>**Name**: Large Language Model and Pretrained Language Models



### Encoder-Only Transformer Architecture

- [[Bidirectional Encoder Representation from Transformer or BERT]]

### Decoder-Only Transformer Architecture

- [[Generative Pre-trained Transformer or GPT]]
- [[Greedy Decoding for Language Model]]
- [[Beam Search as Greedy Decoding]]

### Encoder-Decoder Transformer Architecture

- [[Text-to-Text Transfer Transformer or T5 for Translation]]
- [[Greedy Decoding for Language Model]]
- [[Beam Search as Greedy Decoding]]


## Explanation




## LLM Generation

- [[Autoregressive or Causal Language Model Generation]]
- [[Top-k Sampling for Large Language Model Generation]]
- [[Top-p Sampling or Nucleus Sampling for Large Language Model Generation]]



-----------
##  Recommended Notes and References

- [[Attention Mechanism in Neural Network]]
- [[Transformer Network]]
- [[Scaling Law of Large Language Model]]


- [[Encoder-Decoder Sequence-to-Sequence Architecture]]
- [[Bidirectional Encoder Representation from Transformer or BERT]]
- [[Generative Pre-trained Transformer or GPT]]
- [[Text-to-Text Transfer Transformer or T5 for Translation]]
- [[liuRoBERTaRobustlyOptimized2019]]
- [[touvronLlamaOpenFoundation2023]]

- [[Speech and Language Processing by Jurafsky]] pp 203 - 220
- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 390 - 394
- [[Deep Learning Foundations and Concepts by Bishop]]