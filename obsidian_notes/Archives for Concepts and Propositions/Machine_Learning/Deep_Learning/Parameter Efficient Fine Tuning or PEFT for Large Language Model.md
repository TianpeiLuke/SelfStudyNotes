---
tags:
  - concept
  - deep_learning/architecture
  - deep_learning/models
  - natural_language_processing/large_language_models
keywords: 
topics: 
name: Parameter Efficient Fine Tuning or PEFT for Large Language Model
date of note: 2024-11-24
---

## Concept Definition

>[!important]
>**Name**: Parameter Efficient Fine Tuning or PEFT for Large Language Model




### Adapter Tuning

- [[Adapter-Tuning for Large Language Model]]

### Prefix Tuning

- [[Prefix-Tuning for Large Language Model]]

### Prompt Tuning

- [[Prompt-Tuning or Soft-Prompting for Large Language Model]]


### Low-Rank Adaptation

- [[Low Rank Adaptation or LoRA for Large Language Model]]


## Explanation

![[parameter_efficient_fine_tuning_methods.png]]

![[parameter_efficient_fine_tuning_methods_blocks.png]]

## Codes

- [[PEFT LLM script  - entry point]]



-----------
##  Recommended Notes and References

- [[Attention Mechanism in Neural Network]]
- [[Transformer Network]]
- [[Large Language Model and Pretrained Language Models]]


- [[Low Rank Adaptation or LoRA for Large Language Model]]
- [[Speech and Language Processing by Jurafsky]] pp 214, 217
- [[Deep Learning Foundations and Concepts by Bishop]]
- Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., & Gao, J. (2024). Large language models: A survey. _arXiv preprint arXiv:2402.06196_.