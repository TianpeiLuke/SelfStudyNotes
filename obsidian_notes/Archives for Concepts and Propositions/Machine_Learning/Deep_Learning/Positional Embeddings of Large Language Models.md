---
tags:
  - concept
  - natural_language_processing/large_language_models
  - deep_learning/large_language_models
keywords:
  - positional_embedding_llm
topics:
  - deep_learning/large_language_models
  - natural_language_processing/large_language_models
name: Positional Embeddings of Large Language Models
date of note: 2024-11-26
---

## Concept Definition

>[!important]
>**Name**: Positional Embeddings of Large Language Models

![[Transformer Network#^ce4816]]

![[Transformer Network#^aad689]]

### Absolute Positional Embedding

![[Transformer Network#^d964b5]]

- [[Transformer Network]]
- [[Large Language Model and Pretrained Language Models]]

![[position_encoding.png]]

### Relative Positional Embedding


- P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative position representations,” *arXiv preprint* arXiv:1803.02155, 2018.


### Rotary Positional Embedding


- [[touvronLlamaOpenFoundation2023]]
- J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu, “Roformer: Enhanced transformer with rotary position embedding,” *arXiv preprint* arXiv:2104.09864, 2021 [[suRoFormerEnhancedTransformer2024]]

### Relative Positional Bias

- O. Press, N. A. Smith, and M. Lewis, “Train short, test long: Attention with linear biases enables input length extrapolation,” arXiv preprint arXiv:2108.12409, 2021.

## Explanation

![[llm_positional_embedding.png]]


- Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., & Gao, J. (2024). Large language models: A survey. _arXiv preprint arXiv:2402.06196_.



-----------
##  Recommended Notes and References


- [[Deep Learning Foundations and Concepts by Bishop]]
- [[Probabilistic Machine Learning Advanced Topics by Murphy]]
- [[Speech and Language Processing by Jurafsky]]

- Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., & Gao, J. (2024). Large language models: A survey. _arXiv preprint arXiv:2402.06196_.