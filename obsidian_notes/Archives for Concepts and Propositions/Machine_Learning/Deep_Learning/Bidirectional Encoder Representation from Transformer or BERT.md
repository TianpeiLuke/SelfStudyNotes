---
tags:
  - concept
  - deep_learning/architecture
  - deep_learning/models
  - deep_learning/large_language_models
  - natural_language_processing/large_language_models
keywords:
  - bert_transformer
topics:
  - deep_learning/models
  - natural_language_processing/large_language_models
name: Bidirectional Encoder Representation from Transformer or BERT
date of note: 2024-10-21
---

## Concept Definition

>[!important]
>**Name**: Bidirectional Encoder Representation from Transformer or BERT


### Encoder-Only Transformer Architecture


![[bert_input_represntation.png]]





![[bert_architecture.png]]



### Tokenization

![[WordPiece Tokenization#^7acc64]]

- [[WordPiece Tokenization]]


### Pre-Training

#### Task 1: Masked Language Model (MLM)









#### Task 2: Next Sentence Prediction (NSP)



### Supervised Fine-Tuning


![[bert.png]]
## Explanation


## Dataset in Experiment

### General Language Understanding Evaluation (GLUE) benchmark



### Stanford Question Answering Dataset (SQuAD v1.1 and v2.0)



### Situations with Adversarial Generations (SWAG)






-----------
##  Recommended Notes and References


- [[Attention Mechanism in Neural Network]]
- [[Transformer Network]]
- [[Large Language Model and Pretrained Language Models]]
- [[Generative Pre-trained Transformer or GPT]]

- [[Bidirectional Recurrent Neural Network]]
- [[Artificial Neural Network and Deep Learning]]

- [[devlinBERTPretrainingDeep2019]]
- [[liuRoBERTaRobustlyOptimized2019]]

- [[Deep Learning Foundations and Concepts by Bishop]] pp 388 - 390
- [[Speech and Language Processing by Jurafsky]] pp 