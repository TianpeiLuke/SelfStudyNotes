---
tags:
  - concept
  - machine_learning/models
  - deep_learning/generative_models
  - deep_learning/models
  - deep_learning/architecture
  - probabilistic_graphical_models/sequential_models
keywords:
  - denoising_diffusion_probabilistic_models
  - ddpm
topics:
  - deep_learning/models
  - deep_learning/algorithm
  - probabilistic_graphical_model
name: Denoising Diffusion Probabilistic Models
date of note: 2024-05-12
---

## Concept Definition

>[!important]
>**Name**: Denoising Diffusion Probabilistic Models

>[!important] Definition
> The **denoising diffusion probabilistic model (DDPM)** consists of two processes:
> - The **forward pass** corresponds to a *multi-step noise process* i.e. a **diffusion process**
> - The **reverse pass** learns a neural network that invert the noise process, which is used as the *generator* of new samples.


- [[Diffusion Process]]
- [[Stochastic Differential Equations]]

### Forward Pass

>[!important] Definition
> At iteration $t$, the intermediate latent variable $z_{t}$ is generated by *corrupting* previous iteration $z_{t-1}$ with an *independent additive Gaussian white noise* $\epsilon_{t}$, i.e.
> $$
> \begin{align}  
> z_{t} &= \sqrt{ 1 - \beta_{t} }\,z_{t-1} + \sqrt{ \beta_{t} }\epsilon_{t} \quad t= 1\,{,}\ldots{,}\,
>\end{align}
>$$ 
>where 
>- $z_{0} = x$ is *input data*;
>- $\epsilon_{t} \sim \mathcal{N}(0, I)$
>- The parameter $$\beta_{1} < \beta_{2} \,{<}\ldots{<}\,\beta_{T} < 1$$
>- The *Markov transition kernel* is defined by conditional Gaussian distribution $$K(z_{t-1}, z_{t}) = \mathcal{N}(z_{t}\;|\; \sqrt{ 1 - \beta_{t} }\,z_{t-1},\; \beta_{t}I)$$
>- This is called the **forward pass**

- [[Gaussian Process]]
- [[Gaussian Random Vector]]
- [[Markov Transition Kernel and Transition Function]]

### Diffusion Kernel





### Reverse Pass

>[!info]
> By Bayes theorem, we can computer the posterior distribution of past $z_{t-1}$ *conditioned on input* $x$ and present $z_{t}$
> 
> $$\begin{align}q(z_{t-1} | z_{t}, x) &= \frac{q(z_{t}| z_{t-1}, x)q(z_{t-1}|x)}{q(z_{t}|x)} \end{align}$$

>[!info]
> Note that *without conditioning* on $x$, the *reverse kernel* is **intractable** $$\begin{align}q(z_{t-1} | z_{t}) &= \frac{q(z_{t}| z_{t-1})q(z_{t-1})}{q(z_{t})} \end{align}$$ since the marginal distribution $$q(z_{t})= \int q(z_{t}|x)p(x)dx$$ depends on the *data distribution* $p(x)$, which is high dimensional and complex.

- [[Bayes Theorem]]

### Variational Decoder 

>[!important]
>The **reverse process** is *approximated* by a **decoder network**
>
>$$q(z_{t-1}|z_{t}) \approx p(z_{t-1}|z_{t}, w)$$



- [[Evidence Lower Bound]]
- [[Variational Auto-Encoder]]

## Explanation

>[!quote]
>The central idea is to take each training image and to **corrupt** it using a **multi-step noise process** to transform it into a sample from a *Gaussian distribution*. This is illustrated in Figure 20.1. A deep neural network is then trained to **invert this process**, and once trained the network can then *generate new images* starting with samples from a Gaussian as input.
>— [[Deep Learning Foundations and Concepts by Bishop]] pp 582

![[diffusion_network_process.png]]

- [[Langevin Equation]]
- [[Fokker–Planck and Kolmogorov Forward-Backward Equation]]
- [[Diffusion Process]]
- [[Stochastic Differential Equations]]

>[!info]
>The sequence of conditional distributions forms a **Markov chain** and can be expressed as a **probabilistic graphical model**

- [[Gaussian Bayesian Network]]
- [[Dynamic Bayesian Network]]
- [[Markov Chain and Markov Process]]

## Other Generative Models

>[!quote]
>Diffusion models can be viewed as a form of **hierarchical variational autoencoder** in which the *encoder distribution is fixed*, and defined by the noise process, and *only the generative distribution is learned* (Luo, 2022). They are easy to train, they scale well on parallel hardware, and they avoid the challenges and instabilities of adversarial training while producing results that have quality comparable to, or better than, **generative adversarial networks**. However, generating new samples can be *computationally expensive* due to the need for multiple forward passes through the decoder network (Dhariwal and Nichol, 2021).
>— [[Deep Learning Foundations and Concepts by Bishop]] pp 582

- [[Variational Auto-Encoder]]
- [[Generative Adversarial Network]]



-----------
##  Recommended Notes and References

- [[Diffusion Network]]
- [[Artificial Neural Network and Deep Learning]]
- [[Probabilistic Graphical Models]]


- [[Wasserstein Distance]]
- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 857
- [[Deep Learning Foundations and Concepts by Bishop]] pp 581 - 603