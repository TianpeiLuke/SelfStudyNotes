---
tags:
  - concept
  - deep_learning/architecture
  - deep_learning/models
  - natural_language_processing/large_language_models
keywords: 
topics: 
name: Prefix-Tuning for Large Language Model
date of note: 2024-11-24
---

## Concept Definition

>[!important]
>**Name**: Prefix-Tuning for Large Language Model


- [[Large Language Model and Pretrained Language Models]]
- [[In-Context Learning for LLM]]


## Explanation

![[parameter_efficient_fine_tuning_methods.png]]

## Codes





-----------
##  Recommended Notes and References

- [[Attention Mechanism in Neural Network]]
- [[Transformer Network]]


- [[Parameter Efficient Fine Tuning or PEFT for Large Language Model]]
- [[Speech and Language Processing by Jurafsky]] 
- Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., & Gao, J. (2024). Large language models: A survey. _arXiv preprint arXiv:2402.06196_.
- Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _ACM Computing Surveys_, _55_(9), 1-35. [[liuPretrainPromptPredict2023]]