---
tags:
  - concept
  - deep_learning/large_language_models
  - natural_language_processing/large_language_models
keywords: 
topics: 
name: Preference Alignment for LLM
date of note: 2024-10-21
---

## Concept Definition

>[!important]
>**Name**: Preference Alignment for LLM

>[!important] Definition
>The **preference alignment** refers to a technique that improve the quality of response from Large Language Model (LLM) by training a separate model that decide how much a candidate response *aligns with human preferences*.
>- The **preference alignment** uses (human) feedback to *align model behavior* to what is preferred in the application *environment*, which is outside the training dataset.

^5fe77a

- [[Foundational Models for Transfer Learning]]


### Reinforcement Learning with Human Feedbacks

- [[Reinforcement Learning with Human Feedbacks or RLHF for LLM]]

### Direct Preference Optimization

- [[Direct Preference Optimization for Alignment in LLM]]


## Explanation

### Why Preference Alignment?

![[In-Context Learning for LLM#^937794]]

>[!info]
>In order to achieve better performance for **in-context learning**, it is necessary to have **instruction fine-tuning**.

- [[In-Context Learning for LLM]]


## Compare to Instruction Fine-Tuning

- [[Supervised Fine-Tuning and Preference Alignment for LLM]]
- [[Supervised Fine-Tuning or Instruction Fine-Tuning of LLM]]



-----------
##  Recommended Notes and References



- [[Large Language Model and Pretrained Language Models]]



- [[Valued-based and Policy-based Reinforcement Learning]]
- [[Markov Decision Process]]

- [[Speech and Language Processing by Jurafsky]] pp 214