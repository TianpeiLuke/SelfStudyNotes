---
tags:
  - concept
  - deep_learning/architecture
  - deep_learning/algorithms
  - deep_learning/generative_models
  - deep_learning/large_language_models
  - deep_learning/ensemble_learning
  - machine_learning/ensemble_methods
  - mixture_of_experts
  - MoE
keywords:
  - mixture_of_experts
topics:
  - deep_learning/models
  - deep_learning/ensemble_learning
  - deep_learning/large_language_models
name: Mixture of Experts as Deep Ensemble Learning
date of note: 2024-10-24
---

## Concept Definition

>[!important]
>**Name**: Mixture of Experts as Deep Ensemble Learning



![[mixture_of_expert_transformer_encoder.png]]

- [[Sparsely-Gated Mixture-of-Experts or MoE Layer]]

## Explanation



- [[Switch Transformer via Mixture of Expert Layer]]


-----------
##  Recommended Notes and References


- [[Gaussian Mixture Models]]
- [[Bagging and Model Averaging]]
- [[Ensemble Learning]]
- [[Dropout for Deep Learning]]

- [[Transformer Network]]
- [[Expert Parallelism]]

- [[Artificial Neural Network and Deep Learning]]

- [[Probabilistic Machine Learning Advanced Topics by Murphy]] pp 649 - 657
- [[shazeerOutrageouslyLargeNeural2017]] Shazeer, N., Mirhoseini, Azalia, Maziarz, Krzysztof, Davis, A., Le, Q., Hinton, G., & Dean, J. (2017, April 24). *Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer*. _5th International Conference on Learning Representations_. International Conference on Learning Representations (ICLR 2017), Palais des Congrès Neptune, Toulon, France. [https://openreview.net/forum?id=B1ckMDqlg](https://openreview.net/forum?id=B1ckMDqlg)
- [[lepikhinGShardScalingGiant2020]] Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., & Chen, Z. (2020, October 2). _GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding_. International Conference on Learning Representations. [https://openreview.net/forum?id=qrwe7XHTmYb](https://openreview.net/forum?id=qrwe7XHTmYb)
- [[fedusSwitchTransformersScaling2022]] Fedus, W., Zoph, B., & Shazeer, N. (2022). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. _Journal of Machine Learning Research_, _23_(120), 1–39.
- Rajbhandari, S., Li, C., Yao, Z., Zhang, M., Aminabadi, R. Y., Awan, A. A., Rasley, J., & He, Y. (2022). _DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale_ (arXiv:2201.05596). arXiv. [https://doi.org/10.48550/arXiv.2201.05596](https://doi.org/10.48550/arXiv.2201.05596)
