---
tags:
  - concept
  - deep_learning/architecture
  - deep_learning/models
  - natural_language_processing/large_language_models
keywords: 
topics: 
name: Low Rank Adaptation for Large Language Model
date of note: 2024-11-24
---

## Concept Definition

>[!important]
>**Name**: Parameter Efficient Fine Tuning or PEFT for Large Language Model



## Explanation


## Codes

- [[PEFT LLM script  - entry point]]
- [[PEFT LLM script 2 - construct LoRA]]



-----------
##  Recommended Notes and References

- [[Attention Mechanism in Neural Network]]
- [[Transformer Network]]
- [[Large Language Model and Pretrained Language Models]]


- [[Parameter Efficient Fine Tuning or PEFT for Large Language Model]]
- [[Speech and Language Processing by Jurafsky]] pp 218
- Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., ... & Chen, W. (2021). Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_. [[huLoRALowRankAdaptation2022]]