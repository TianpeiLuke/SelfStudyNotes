---
tags:
  - concept
  - deep_learning/architecture
  - deep_learning/models
keywords: 
topics: 
name: Supervised Fine-Tuning and Preference Alignment for LLM
date of note: 2024-10-21
---

## Concept Definition

>[!important]
>**Name**: Supervised Fine-Tuning and Preference Alignment for LLM

![[Supervised Fine-Tuning or Instruction Fine-Tuning of LLM#^e2e7b8]]

![[Preference Alignment for LLM#^5fe77a]]

>[!important] Definition
>Together we refer to *instructing tuning* and *preference alignment* as **model alignment**. 
>- The intuition is that we want the learning objectives of models to be *aligned* with the *goals* of the *humans* that use them.
>- The *model alignment*  also helps to avoid LLM response to be **harmful**.

- [[Foundational Models for Transfer Learning]]
- [[Supervised Fine-Tuning or Instruction Fine-Tuning of LLM]]
- [[Preference Alignment for LLM]]
- [[Reinforcement Learning with Human Feedbacks or RLHF for LLM]]
- [[Direct Preference Optimization for Alignment in LLM]]




## Explanation








-----------
##  Recommended Notes and References



- [[Large Language Model and Pretrained Language Models]]



- [[Valued-based and Policy-based Reinforcement Learning]]
- [[Markov Decision Process]]

- [[Speech and Language Processing by Jurafsky]] pp 214, 249 - 259