---
tags:
  - concept
  - deep_learning/large_language_models
  - natural_language_processing/large_language_models
  - rotary_positional_embedding
keywords:
  - rotary_positional_embedding
topics:
  - deep_learning/large_language_models
name: Rotary Positional Embedding for Large Language Models
date of note: 2025-01-03
---

## Concept Definition

>[!important]
>**Name**: Rotary Positional Embedding for Large Language Models


- [[Attention Mechanism in Neural Network]]
- [[Transformer Network]]
- [[Positional Embeddings of Large Language Models]]

## Explanation





-----------
##  Recommended Notes and References

- [[touvronLlamaOpenFoundation2023]]
- J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu, “Roformer: Enhanced transformer with rotary position embedding,” *arXiv preprint* arXiv:2104.09864, 2021 [[suRoFormerEnhancedTransformer2024]]