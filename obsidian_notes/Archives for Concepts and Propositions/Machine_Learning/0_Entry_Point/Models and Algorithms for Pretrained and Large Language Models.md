---
tags:
  - entry_point
  - concept
  - machine_learning/metrics
  - deep_learning/large_language_models
  - natural_language_processing/large_language_models
keywords: 
topics:
  - deep_learning
  - natural_language_processing/large_language_models
name: 
date of note: 2024-06-01
---

## List of Concepts

### Foundations in Math, Statistics and Optimization

- [[Concepts and Theorems in Finite Dimensional Vector Space]]
- [[Concepts and Theorems for Matrix Theory]]
- [[Concepts and Theorems in Probability Theory]]
- [[Concepts and Theorems for Markov Process]]
- [[Theory and Algorithms for Nonlinear Optimization]]
- [[Concepts and Algorithms for Monte Carlo Methods]]
- [[Concepts and Theorems in Statistical Inference and Decision Theory]]
- [[Concepts and Theorems in Information Theory]]

### Machine Learning

- [[Concepts and Algorithms in Machine Learning]]
- [[Models and Algorithms for Deep Learning]]
- [[Models and Algorithms for Reinforcement Learning]]

### Attention and Transformer Network

- [[Attention Mechanism in Neural Network]]
- [[Transformer Network]]
- [[Positional Embeddings of Large Language Models]]
- [[Rotary Positional Embedding for Large Language Models]]
- [[Flash Attention Mechanism for Large Language Model]]

### Mixture-of-Experts

- [[Sparsely-Gated Mixture-of-Experts or MoE Layer]]
- [[Mixture of Experts or MoE as Deep Ensemble Learning]]
- [[Switch Transformer via Mixture of Expert Layer]]


### Pretrained Language Model Architecture

- [[Large Language Model and Pretrained Language Models]]
- [[Bidirectional Encoder Representation from Transformer or BERT]]
- [[Generative Pre-trained Transformer or GPT]]
- [[Text-to-Text Transfer Transformer or T5 for Translation]]

### Decoding and Text Generation of Language Models

- [[Decoding and Sampling from Large Language Models]]
- [[Greedy Decoding for Causal Language Model Generation]]
- [[Beam Search for Causal Decoding of Language Model]]
- [[Top-k Sampling for Large Language Model Generation]]
- [[Top-p Sampling or Nucleus Sampling for Large Language Model Generation]]
- [[Temperature Sampling for Large Language Model Generation]]

### Pre-Training of LLM and PLM

- [[Self-Supervised Training of Large Language Model and Teacher Forcing]]
- [[Masked Language Modeling as Language Model Training Task]]
- [[Next Sentence Prediction as Language Model Training Task]]
- [[Scaling Law of Large Language Model]]
- [[Key-Value Cache for Training of Large Language Models]]
- [[Pre-Training and Fine-Tuning Paradigm for Transfer Learning]]

![[pretrain_finetuning_llm.png]]

### Fine-Tuning of PLM and LLM

- [[Foundational Models for Transfer Learning]]

#### Fine-Tuning of Encoder-Only LMs

- [[Fine-Tuning for Sequence or Sequence-Pair Classification via BERT]]
- [[Fine-Tuning for Sequence Labeling via BERT]]

#### Fine-Tuning of LLM

- [[Continued Pre-training of Large Language Models]]
- [[Parameter Efficient Fine Tuning or PEFT for Large Language Model]]
	- [[Adapter-Tuning for Large Language Model]]
	- [[Prefix-Tuning for Large Language Model]]
	- [[Prompt-Tuning or Soft-Prompting for Large Language Model]]
	- [[Low Rank Adaptation or LoRA for Large Language Model]]


### Model Alignment of LLM

- [[Supervised Fine-Tuning and Preference Alignment for LLM]]

#### Instruction Tuning or Supervised Fine-Tuning for LLM

- [[Supervised Fine-Tuning or Instruction Fine-Tuning of LLM]]

#### Preference Alignment Methods of LLM

- [[Preference Alignment for LLM]]
- [[Reinforcement Learning with Human Feedbacks or RLHF for LLM]]
- [[Direct Preference Optimization for Alignment in LLM]]


### In-Context Learning

- [[In-Context Learning for LLM]]
- [[Prompt Engineering for LLM]]
- [[Chain-of-Thought Prompting]]
- [[ReAct as LLM Agent for Reasoning and Acting]]
- [[Retrieval Augmented Generation]]

### Efficient Training, Inference and Compression

- [[Concepts and Algorithms for Concurrent and Parallel Programming]]

- [[Parallel Pretraining of Large Language Models]]
- [[Data Parallelism]]
- [[Model Parallelism]]
- [[Pipeline Parallelism]]

- [[Fully Sharded Data Parallel or FSDP for LLM Training]]
- [[Zero Redundancy Optimizer or ZeRO for Optimized Training of LLM]]
- [[Knowledge Distillation for Efficient Training]]
- [[Quantization of Large Language Models]]



### Multi-Modal Transformer Networks

- [[Multi-Modal Learning]]
- [[Vision Tokenization]]
- [[Vision Transformer or ViT]]
- [[Contrastive Language Image Pre-training or CLIP]]
- [[Vision-Language Foundational Models]]



## Explanation

![[llm_components.png]]

## Applications

- [[Concepts and Algorithms in Natural Language Processing]]
- [[Concepts and Algorithms in Information Retrieval]]



-----------
##  Recommended Notes and References

- [[Understanding Machine Learning by Shalev-Shwartz]]


- [[Deep Learning by Goodfellow]]
- [[Probabilistic Machine Learning Advanced Topics by Murphy]]
- [[Deep Learning Foundations and Concepts by Bishop]]

- [[Artificial Intelligence Modern Approach by Russell]]


