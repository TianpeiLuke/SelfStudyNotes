---
tags:
  - entry_point
  - concept
  - machine_learning/theory
  - machine_learning/models
  - machine_learning/metrics
keywords: 
topics:
  - machine_learning_theory
  - machine_learning_models
  - deep_learning
name: 
date of note: 2024-06-01
---

## List of Concepts

### Foundations in Math, Statistics and Optimization

- [[Concepts and Theorems in Finite Dimensional Vector Space]]
- [[Concepts and Theorems for Matrix Theory]]
- [[Concepts and Theorems in Probability Theory]]
- [[Concepts and Theorems for Markov Process]]
- [[Theory and Algorithms for Nonlinear Optimization]]
- [[Concepts and Algorithms for Monte Carlo Methods]]
- [[Concepts and Theorems in Statistical Inference and Decision Theory]]
- [[Concepts and Theorems in Information Theory]]

### Artificial Neural Network

- [[Artificial Neural Network and Deep Learning]]
- [[Activation Functions for Deep Learning]]
	- [[Rectified Linear Unit as Activation for Deep Learning]]
	- [[Sigmoid Function as Activation for Deep Learning]]


- [[Error Function or Loss Function for Deep Learning]]
	- [[Maximum Likelihood Estimation]]
	- [[Minimum Mean Square Estimation]]
	- [[Cross-Entropy Loss Function]]
	- [[Score Matching and Denoising Score Matching]]
	- [[Noise Contrastive Estimation]]
	- [[Information Noise Contrastive Estimation as Contrastive Learning]]


- [[Perceptron Algorithm]]
- [[Multi-Layer Perceptron and Feed-Forward Network]]

### Back Propagation and Automatic Differentiation

- [[Automatic Differentiation]]
- [[Back-Propagation Algorithm]]
- [[Back-Propagation Through Time]]

### Optimization Algorithm 

- [[Stochastic Gradient Descent Algorithm]]
- [[Stochastic Gradient Descent with Nesterov Momentum]]
- [[Stochastic Gradient Descent with Momentum]]
- [[AdaGrad Algorithm]]
- [[RMSProp Algorithm]]
- [[Adam Algorithm]]

### Normalization

- [[Batch Normalization]]
- [[Layer Normalization]]

### Regularization

- [[Inductive Bias in Machine Learning]]
- [[Tikhonov Regularization in Optimization and Learning]]
- [[Regularized Loss Minimization]]
- [[Equivariance of Estimator]]

- [[Weight Decay for Deep Learning]]
- [[Early Stopping for Deep Learning]]
- [[Soft Weight Sharing for Deep Learning]]
- [[Residual Connection for Deep Learning]]
- [[Dropout for Deep Learning]]


## Deep Learning Architectures

### Convolutional Neural Network

- [[Convolutional Filters]]
- [[Pooling for Deep Learning]]
- [[Convolutional Neural Network]]

### Energy-based Model

- [[Gibbs Measure and Energy-based Model]]
- [[Gibbs Distribution]]
- [[Log-Partition Function and Score Function of Graphical Models]]

#### Stochastic Maximum Likelihood Training

- [[Markov Chain Monte Carlo Methods]]
- [[Maximum Likelihood Estimation]]
- [[Gibbs Sampling]]
- [[Langevin Dynamics and Langevin Sampling]]
- [[Contrastive Divergence]]

#### Score Matching

- [[Score Matching and Denoising Score Matching]]

#### Noise Contrastive Estimation

- [[Noise Contrastive Estimation]]
- [[Contrastive Learning]]
- [[Information Noise Contrastive Estimation as Contrastive Learning]]
- [[Contrastive Predictive Coding as Sequence Representation]]


### Representation Learning

- [[Distributed Representation]]
- [[Representation Learning]]

#### Autoencoder

- [[Auto-Encoder and Stochastic Auto-Encoder]]
- [[Sparse Auto-Encoder and Regularized Auto-Encoder]]
- [[Denoising Auto-Encoder]]
- [[Contractive Auto-Encoder]]

#### Contrastive Learning

- [[Contrastive Learning]]
- [[Noise Contrastive Estimation]]
- [[Information Noise Contrastive Estimation as Contrastive Learning]]
- [[Contrastive Predictive Coding as Sequence Representation]]
- [[Siamese Network for Contrastive Learning]]
- [[Triplet Loss Minimization for Contrastive Learning]]


### Recurrent Network

- [[Recurrent Neural Network]]
- [[Challenge of Long-Term Dependencies for Sequential Networks]]
- [[Leaky Units and Other Strategies for Multiple Time Scales Sequential Network]]

- [[Gated Recurrent Units in Neural Network]]
- [[Long-Short Term Memory Network]]
- [[Residual Neural Network]]
- [[Bidirectional Recurrent Neural Network]]
- [[Encoder-Decoder Sequence-to-Sequence Architecture]]
- [[State Space Models and Nonlinear Dynamic System]]


### Generative Model

#### Variational Auto-Encoder

- [[Evidence Lower Bound]]
- [[Variational Inference vs EM Algorithm]]
- [[Variational Auto-Encoder]]

#### Normalizing Flows

- [[Normalizing Flows]]
- [[Neural Ordinary Differential Equations]]


#### Diffusion Network

- [[Denoising Diffusion Probabilistic Models]]
- [[Diffusion Network]]

#### Generative Adversarial Network

- [[Learning by Comparison for GAN]]
- [[Generative Adversarial Network]]


### Graph-based Neural Network

- [[Neural Message Passing Algorithm for Graph Neural Network]]
- [[Graph Neural Network]]
- [[Graph Convolutional Filters]]
- [[Graph Convolutional Neural Network]]

### Memorization Network

- [[Attention Mechanism in Neural Network]]
- [[Transformer Network]]


### Neural Architecture Search

- [[Neural Architecture Search]]





## Explanation





-----------
##  Recommended Notes and References

- [[Understanding Machine Learning by Shalev-Shwartz]]


- [[Deep Learning by Goodfellow]]
- [[Probabilistic Machine Learning Advanced Topics by Murphy]]
- [[Deep Learning Foundations and Concepts by Bishop]]

- [[Artificial Intelligence Modern Approach by Russell]]


