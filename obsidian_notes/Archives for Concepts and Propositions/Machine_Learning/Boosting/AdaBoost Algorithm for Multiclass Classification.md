---
tags:
  - concept
  - machine_learning/models
  - boosting
  - machine_learning/algorithms
  - machine_learning/boosting
keywords:
  - boosting
  - adaboost
  - ensemble_method
topics:
  - machine_learning_models
  - machine_learning_algorithm
name: AdaBoost Algorithm
date of note: 2024-05-12
---

## Concept Definition

>[!important]
>**Name**: AdaBoost Algorithm for Multiclass Classification

![[AdaBoost Algorithm#^1982f3]]






- [[Empirical Risk Minimization]]
- [[Empirical Process and Empirical Measure]]


## Explanation

>[!important]
>The basic principles behind the *AdaBoost* algorithm are two folds:
>1. A **strong classifier** can be obtained by a **linear combination of weak classifiers**
>2. Each weak classifier only **focuses on a sub-population** that are not learned well by the previous learned models. This is obtained by **increase the importance weight** for the samples that are **misclassified** so that we can learn a hierarchical sequence of classifiers.



-----------
##  Recommended Notes and References


- [[AdaBoost Algorithm]]
- [[Iterative Maximum Entropy Learning for AdaBoost]]
- [[Ensemble Learning]]

- [[Log-Partition Function of Exponential Family]]



- [[Empirical Risk Minimization]]
- [[Empirical Process and Empirical Measure]]



- [[Boosting Foundations and Algorithms by Schapire]]  pp 5
- [[Elements of Statistical Learning by Hastie]]
- [[Understanding Machine Learning by Shalev-Shwartz]]
- [[Elements of Information Theory by Cover]]