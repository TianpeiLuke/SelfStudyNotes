---
tags:
  - concept
  - machine_learning/theory
  - machine_learning/boosting
keywords:
  - vc_dimension
  - generalization_error
  - adaboost
topics:
  - machine_learning_theory
name: VC-based Generalization Error Bound for AdaBoost
date of note: 2024-07-29
---

## Concept Definition

>[!important]
>**Name**: VC-based Generalization Error Bound for AdaBoost

![[AdaBoost Algorithm#^1982f3]]

![[Empirical Risk Minimization#^a84128]]

>[!important] Definition
>- Assume $\mathcal{H}$ is the set of *base hypotheses space* from which all of $h_{t}$'s are selected.
>- Let $\mathcal{C}_{T}$ be the space of *combined classifiers* that might potentially be generated by Adaboost.
>- A combined classifier $H\in \mathcal{C}_{T}$ computed the a *weighted majority vote* of $T$ classifiers $$H(x) = \text{sgn}\left( \sum_{t=1}^{T}\alpha_{t}\,h_{t}(x) \right)$$ for $(\alpha_{1} \,{,}\ldots{,}\,\alpha_{T})$, and some base classifiers $h_{1} \,{,}\ldots{,}\,h_{T}$ in $\mathcal{H}$
>- Define $\sigma: \mathbb{R}^{T} \to \left\{ -1, 0, +1 \right\}$ as $$\sigma(x) = \text{sgn}(\left\langle w ,  x\right\rangle)$$ for *some* $w\in \mathbb{R}^{T}$. In this way, we can write $$H(x) = \sigma(h) :=  \sigma(h_{1}(x) \,{,}\ldots{,}\,h_{T}(x)).$$
>- Let $\Sigma_{T}$ be the space of all such *linear threshold functions*.
>- We can represent $\mathcal{C}_{T}$ as $$\mathcal{C}_{T} := \left\{ \sigma(h_{1} \,{,}\ldots{,}\,h_{T}):\; \sigma\in \Sigma_{T},\, h_{1} \,{,}\ldots{,}\,h_{T} \in \mathcal{H} \right\}.$$

^49f151

### VC Dimension of Space of Linear Thresholding Functions

>[!important] Lemma
>The space $\Sigma_{n}$ of **linear thresholding functions** over $\mathbb{R}^{n}$ has **VC-Dimension** $n$. $$\text{VC-Dim}(\Sigma_{n}) = n.$$

^81976d

### Finite Hypothesis Space

![[Restriction of Function Class to Data#^60c89d]]

![[Shattering of Data by Function Class#^0ab3b0]]

![[Growth Function of Function Class#^dac056]]

>[!important] Lemma
>Assume that $\mathcal{H}$ is finite, $|\mathcal{H}|$. Let $m \ge T \ge 1$. 
>
>For any set $S$ of $m$ points, the *cardinality* of the **restriction** of $\mathcal{C}_{T}$ *to* $S$ is *bounded* as follows:
>$$
>\lvert \mathcal{C}_{T, S} \rvert \le \tau_{\mathcal{C}_{T}}(m) \le \left( \frac{e m}{T} \right)^{T}\,\lvert \mathcal{H} \rvert^{T}. 
>$$
>where $\tau_{\mathcal{C}_{T}}$ is the **growth function** of $\mathcal{C}_{T}$ $$\tau_{\mathcal{C}_{T}}(m) = \max\left\{ \mathcal{C}_{T, S}: |S| = m \right\}.$$

^6ffb11

- [[Restriction of Function Class to Data]]
- [[Growth Function of Function Class]]
- [[Shattering of Data by Function Class]]


>[!info]
>Note that by [[Sauer-Shelah Lemma for VC Dimension]], since the $\text{VC-Dim}(\Sigma_{T}) = T,$ we have
>$$
> \begin{align}
> \tau_{\Sigma_{T}}(m) &\le \sum_{i=0}^{T}{{T}\choose{i}}\le \left(\frac{em}{T}\right)^{T} 
> \end{align}
>$$ 


- [[Sauer-Shelah Lemma for VC Dimension]]


### Generalization Error Bound 

![[Generalization Error Bound for Binary Classification Finite Case#^5bf1f1]]

![[Generalization Error Bound for Binary Classification Finite Case#^ac9f27]]

>[!important] Theorem
>Suppose that **AdaBoost** is run for $T$ rounds on $m \ge T$ random samples, using base classifiers from a **finite hypothesis space** $\mathcal{H}$. 
>
>Then, *with probability at least* $1 - \delta$, the **generalization error** of the **combined classifier** $H$ satisifies
>$$
>L_{\mathcal{P}}(H) \le L_{\mathcal{D}}(H) + \sqrt{ \frac{32 \left[ T \log \left( \dfrac{e\, m\, |\mathcal{H}|}{T} \right) + \log\left(  \dfrac{8}{\delta}  \right)  \right] }{m} }
>$$
>
>Furthermore, with probability at least $1 - \delta$, if $H$ is **consistent** with the training set (i.e. under the **realizability assumption**) $$L_{\mathcal{D}}(H^{*}) = 0,\quad  \mathcal{P}\text{-a.s.},$$ then 
>$$
>L_{\mathcal{P}}(H) \le \frac{2T\,\log \left( \dfrac{2\,e\,m\,\lvert \mathcal{H} \rvert }{T} \right) + 2\log \left( \dfrac{2}{\delta} \right)}{m}
>$$

^99fbd3

- [[Generalization Error Bound for Binary Classification Finite Case]]
- [[PAC Learnable and Agnostic PAC Learnable]]
- [[Realizability Assumption for Empirical Risk Minimization]]

>[!important] Corollary
>Suppose that
>- **AdaBoost** is run for $T$ rounds on $m \ge T$ random samples, using base classifiers from a **finite hypothesis space** $\mathcal{H}$;
>- And each base classifier has **weighted error** $$\epsilon_{t} \le \frac{1}{2} - \gamma$$ for some $\gamma >0$.
>- Furthermore, let the number of rounds $$T = \left\lceil  \frac{\log m}{2\gamma^2}  \right\rceil $$
>
>Then, *with probability at least* $1 âˆ’ \delta$, the **generalization error** of the combined classifier $H$ 
>$$
>L_{\mathcal{P}}(H) \le O\left( \frac{1}{m} \left[ \frac{\log(m)}{\gamma^2}\left( \log (m) + \log |\mathcal{H}| \right) + \log \left( \frac{1}{\delta} \right) \right]  \right)
>$$

### Infinite Base Classifier Space

- [[Generalization Error Bound for AdaBoost VC Dimension]]


## Explanation



-----------
##  Recommended Notes and References

- [[VC Dimension]]
- [[Restriction of Function Class to Data]]
- [[Shattering of Data by Function Class]]
- [[PAC Learnable and Agnostic PAC Learnable]]
- [[Growth Function of Function Class]]
- [[Sauer-Shelah Lemma for VC Dimension]]


- [[AdaBoost Algorithm]]
- [[Empirical Risk Minimization]]
- [[Log-Partition Function of Exponential Family]]

- [[Generalization Error Bound for Binary Classification Finite Case]]

- [[Boosting Foundations and Algorithms by Schapire]]  pp 77 - 82