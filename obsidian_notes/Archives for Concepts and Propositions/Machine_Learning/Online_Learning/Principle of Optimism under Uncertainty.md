---
tags:
  - concept
  - reinforcement_learning/bandit
  - online_learning/algorithms
keywords:
  - optimism_under_uncertainty
topics:
  - online_learning
  - bandit_problem
  - reinforcement_learning
name: Principle of Optimism under Uncertainty
date of note: 2024-08-13
---

## Concept Definition

>[!important]
>**Name**: Principle of Optimism under Uncertainty

>[!quote]
>... **optimism in the face of uncertainty**, which states that one should act as if the environment is *as nice as plausibly possible*.As we shall see in later chapters, the principle is applicable beyond the *finite-armed stochastic bandit problem*.
>
>-- [[Bandit Algorithms by Lattimore]] pp 86 

^9b8645

>[!quote]
>Imagine visiting a new country and making a choice between sampling the local cuisine or visiting a well-known multinational chain. Taking an **optimistic view** of *the unknown* local cuisine *leads to* **exploration** because without data, it could be amazing. After trying the new option a few times, you can update your statistics and make a more informed decision. On the other hand, taking a **pessimistic view** of the new option **discourages exploration**, and you may suffer significant *regret* if the local options are delicious. Just how optimistic you should be is a difficult decision, which we explore for the rest of the chapter in the context of finite-armed bandits.
>
>-- [[Bandit Gradient Algorithm]] pp 86




## Explanation





-----------
##  Recommended Notes and References

- [[Upper Confidence Bound Algorithm]]
- [[KL-Upper Confidence Bound Algorithm]]
- [[Upper Confidence Bound Algorithm Asymptotic Optimality]]
- [[Upper Confidence Bound Algorithm Minmax Optimality]]

- [[Returns and Goals of Reinforcement Learning]]
- [[Explore-Then-Commit Bandit Algorithm]]
- [[Multi-Armed Adversarial Bandit]]



- [[Bandit Algorithms by Lattimore]] pp 86 
