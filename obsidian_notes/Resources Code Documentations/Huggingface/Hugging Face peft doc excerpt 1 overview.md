---
tags:
  - excerpt
  - documentation
  - code_package
  - python
  - package/huggingface
  - peft
aliases: 
keywords:
  - huggingface/peft
topics:
  - code/doc
language: python
date of note: 2024-03-25
name: Hugging Face `dataset` package official doc
version: 2.18.0
year: 2024
---
# About Package

>[!quote]
>ðŸ¤— **PEFT (Parameter-Efficient Fine-Tuning)** is a library for efficiently adapting large pretrained models to various downstream applications without fine-tuning all of a modelâ€™s parameters because it is prohibitively costly. *PEFT methods* only fine-tune a small number of (extra) model parameters - significantly decreasing computational and storage costs - while yielding performance comparable to a fully fine-tuned model. This makes it more accessible to train and store large language models (LLMs) on consumer hardware.
>
>PEFT is integrated with the *Transformers*, *Diffusers*, and *Accelerate* libraries to provide a faster and easier way to load, train, and use large models for inference.
> -- [link](https://huggingface.co/docs/peft/index#peft)





-----------
##  Recommended Notes







----------
##  Citations

**Hugging face** `datasets` package documentation
- Home page [link](https://huggingface.co/docs/peft/index)
- overview [doc](https://huggingface.co/docs/datasets/how_to)



