---
tags:
  - excerpt
  - documentation
  - code_package
  - python
  - huggingface
  - package/huggingface/datasets
aliases: 
keywords:
  - huggingface/datasets
topics:
  - code/doc
language: python
date of note: 2024-03-23
name: Hugging Face `dataset` package official doc
version: 2.18.0
year: 2024
---

# Stream

> [!quote]
>**Dataset streaming** lets you work with a dataset **without downloading** it. The data is streamed as you **iterate** over the dataset. This is especially helpful when:
> 
> - You donâ€™t want to **wait** for an **extremely large** dataset to **download**.
> - The dataset size **exceeds** the amount of available **disk space** on your computer.
> - You want to **quickly explore** just a few samples of a dataset.

> [!quote]
> Dataset streaming also lets you work with a dataset made of **local** files **without doing any conversion**. In this case, the data is streamed from the local files as you iterate over the dataset. This is especially helpful when:
> 
> - You donâ€™t want to wait for an **extremely large local dataset** to be converted to Arrow.
> - The converted files size would **exceed** the amount of available **disk space** on your computer.
> - You want to **quickly explore** just a few samples of a dataset.

- [original doc link](https://huggingface.co/docs/datasets/stream)
- We can get streaming data by adding `streaming=True` to `load_dataset`
  
```python
from datasets import load_dataset
dataset = load_dataset('oscar-corpus/OSCAR-2201', 'en', split='train', streaming=True)
```

-  It will return `Iterator` object. We can obtain one sample/batch by

```python
print(next(iter(dataset)))
```

## `IterableDataset`

- Loading a dataset in streaming mode creates a **new dataset type instance** (instead of the classicÂ [Dataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.Dataset)Â object), known as anÂ [IterableDataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.IterableDataset). This special type of dataset has its own set of processing methods shown below.


>[!important]
AnÂ [IterableDataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.IterableDataset)Â is useful for iterative jobs like training a model. You shouldnâ€™t use aÂ [IterableDataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.IterableDataset)Â for jobs that require random access to examples because you have to iterate all over it using a for loop. Getting the **last** example in an iterable dataset would require you to **iterate over all the previous examples**.

## Convert from a Dataset

>If you have an existingÂ [Dataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.Dataset)Â object, you can convert it to anÂ [IterableDataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.IterableDataset)Â with theÂ [to_iterable_dataset()](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.Dataset.to_iterable_dataset)Â function. This is actually **faster** than setting theÂ `streaming=True`Â argument inÂ [load_dataset()](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/loading_methods#datasets.load_dataset)Â because the data is streamed from local files.

>[!example]
>```python
>from datasets import load_dataset
>
># faster
>dataset = load_dataset("food101")
>iterable_dataset = dataset.to_iterable_dataset()
>
># slower
>iterable_dataset = load_dataset("food101", streaming=True)
>```

- TheÂ [to_iterable_dataset()](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.Dataset.to_iterable_dataset)Â function supports sharding when theÂ [IterableDataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.IterableDataset)Â is instantiated. This is useful when working with big datasets, and youâ€™d like to shuffle the dataset or to enable *fast parallel loading* with a **PyTorch DataLoader**.


## Highlights

>[!Important] Highlights
>- Compared to original `Dataset` object. `IterableDataset` object has no row operations (such as **Sort**) since the streaming data can only access to immediate next sample. 
>- We can only download samples in a **buffer** to do *row operations* such as **Shuffle**. 
>- No dataset operations such as **Concatenate**, **Format** etc. 
>- No **Save** or **Export** since we cannot download entire dataset.
>- **Interleave** operation exists to merge two datastreams
>
>Common operations on `IterableDataset` are
>- **Split**, **Shuffle**, **Reshuffle**
>- **Filter**
>- Rename, Remove, Cast
>- **Map**
>- **Interleave**


## Some Functions

- **Shuffle**: Like a regularÂ [Dataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.Dataset)Â object, you can also shuffle aÂ [IterableDataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.IterableDataset)Â withÂ [IterableDataset.shuffle()](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle). 
	- TheÂ `buffer_size`Â argument controls the size of the **buffer** to randomly sample examples from. Letâ€™s say your dataset has one million examples, and you set theÂ `buffer_size`Â to ten thousand.Â [IterableDataset.shuffle()](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.IterableDataset.shuffle)Â will randomly select examples from the first ten thousand examples in the buffer. Selected examples in the buffer are *replaced with new examples*. By default, the buffer size is 1,000.
	  
- **Reshuffle**: Sometimes you may want to reshuffle the dataset after each epoch. This will require you to set a different seed for each epoch. UseÂ `IterableDataset.set_epoch()`Â in between epochs to tell the dataset what epoch youâ€™re on.
  
- **Split**: 
	- [IterableDataset.take()](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.IterableDataset.take)Â returns the firstÂ `n`Â examples in a dataset:
	- [IterableDataset.skip()](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.IterableDataset.skip)Â **omits** the firstÂ `n`Â examples in a dataset and returns the remaining examples

- **Interleave**: [interleave_datasets()](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.interleave_datasets)Â can combine anÂ [IterableDataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.IterableDataset)Â with other datasets. The combined dataset returns alternating examples from each of the original datasets.
  
- **Map**: Similar to theÂ [Dataset.map()](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.Dataset.map)Â function for a regularÂ [Dataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.Dataset), ğŸ¤— Datasets featuresÂ [IterableDataset.map()](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.IterableDataset.map)Â for processing anÂ [IterableDataset](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.IterableDataset).

- **Filter**: You can filter rows in the dataset based on a predicate function usingÂ [Dataset.filter()](https://huggingface.co/docs/datasets/v2.18.0/en/package_reference/main_classes#datasets.Dataset.filter).


-----------
##  Recommended Notes

[^1]: Many functions use the same syntax as the **Pandas Dataframe**.  
- `Dataset` and `IterableDataset` [doc](https://huggingface.co/docs/datasets/about_mapstyle_vs_iterable)
- **Dataset Features** [doc](https://huggingface.co/docs/datasets/about_dataset_features)




----------
##  Citations

**Hugging face** `datasets` package documentation
- Home page [link](https://huggingface.co/docs/datasets/index)
- overview [doc](https://huggingface.co/docs/datasets/how_to)



