---
tags:
  - "#paper"
  - deep_learning/generative_models
aliases:
  - kaplanScalingLawsNeural2020
year: 2020
name: Scaling Laws for Neural Language Models
authors: Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei
publication: ""
type: preprint
DOI: 10.48550/arXiv.2001.08361
date of note: 2024-10-22
---
# Descriptions

## Scaling Laws for Neural Language Models 
> [!info] 
> - **Abstract:** We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence. 
> - **Sources**: [online](http://zotero.org/users/13492210/items/LEG5T5C2) [local](zotero://select/library/items/LEG5T5C2) [pdf](file:////Users/lukexie/Zotero/storage/W6AUEEAV/Kaplan%20et%20al.%20-%202020%20-%20Scaling%20Laws%20for%20Neural%20Language%20Models.pdf) 
> - **Bibliography**: Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). _Scaling Laws for Neural Language Models_ (Machine Learning (Cs.LG); Machine Learning (Stat.ML) arXiv:2001.08361). arXiv. [https://doi.org/10.48550/arXiv.2001.08361](https://doi.org/10.48550/arXiv.2001.08361)
> - **Cite Key:** [[@kaplanScalingLawsNeural2020]] 
> - **Conclusion**:
> - **Tags:** #Computer-Science---Machine-Learning, #Statistics---Machine-Learning, #Scaling-Laws


>[!note] Highlights:
>
>-
>-
>-



# Questions
## Questions Regarding to This Paper


>[!question] 
>What are the *primary assumptions* behind this paper?



>[!question]
>What are the major *problems of concern* behind this paper? If i had to guess, what does the author seems to be concerned. 




>[!question]
>What are *the main contributions* of the paper?



## Highlight of Technical Details


>[!todo]
>Highlight the main techniques proposed in this paper. Also highlight the performance comparison to state-of-the-art.



## Questions Related to Me


> [!question] 
> Is the problem behind this paper related to problem of mine?



> [!question] 
> What impact does this paper have on me?




----

## Reference and Related Notes


- [[Scaling Law of Large Language Model]]

- [[Large Language Model and Pretrained Language Models]]
- [[Bidirectional Encoder Representation from Transformer or BERT]]
- [[Generative Pre-trained Transformer or GPT]]