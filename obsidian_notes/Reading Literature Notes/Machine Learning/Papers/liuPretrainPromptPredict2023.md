---
tags:
- '#paper'
aliases: "liuPretrainPromptPredict2023"
year: 2023
name: "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"
authors: "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig"
publication: "ACM Computing Surveys"
type: "journalArticle"
DOI: "10.1145/3560815"
date of note: 2024-11-28 
---
# Descriptions

## Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing 
> [!info] 
> - **Abstract:** This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x′ that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x̂, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia–Pretrain including constantly updated survey and paperlist. 
> - **Sources**: [online](http://zotero.org/users/13492210/items/R36KBWET) [local](zotero://select/library/items/R36KBWET) [pdf](file:////home/lukexie/Documents/Papers/storage/4BFX4XL2/Liu%20et%20al.%20-%202023%20-%20Pre-train,%20Prompt,%20and%20Predict%20A%20Systematic%20Surve.pdf) 
> - **Bibliography**: Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. _ACM Computing Surveys_, _55_(9), 195:1-195:35. [https://doi.org/10.1145/3560815](https://doi.org/10.1145/3560815)
> - **Cite Key:** [[@liuPretrainPromptPredict2023]] 
> - **Conclusion**:
> - **Tags:** #Pre-trained-language-models, #prompting


>[!note] Highlights:
>
>-
>-
>-



# Questions
## Questions Regarding to This Paper


>[!question] 
>What are the *primary assumptions* behind this paper?



>[!question]
>What are the major *problems of concern* behind this paper? If i had to guess, what does the author seems to be concerned. 




>[!question]
>What are *the main contributions* of the paper?



## Highlight of Technical Details


>[!todo]
>Highlight the main techniques proposed in this paper. Also highlight the performance comparison to state-of-the-art.



## Questions Related to Me


> [!question] 
> Is the problem behind this paper related to problem of mine?



> [!question] 
> What impact does this paper have on me?




----

## Reference and Related Notes