---
tags:
  - "#paper"
  - programmatic_weak_supervision
  - large_language_models
  - zero-shot-learning
aliases:
  - smithLanguageModelsLoop2023
year: 2023
name: "Language Models in the Loop: Incorporating Prompting into Weak Supervision"
authors: Ryan Smith, Jason A. Fries, Braden Hancock, Stephen H. Bach
publication: ACM / IMS Journal of Data Science
type: journalArticle
DOI: 10.1145/3617130
date of note: 2024-03-12
---
# Descriptions

## Language Models in the Loop: Incorporating Prompting into Weak Supervision 
> [!info] 
> - **Abstract:** We propose a new strategy for applying large pre-trained language models to novel tasks when labeled training data is limited. Rather than apply the model in a typical zero-shot or few-shot fashion, we treat the model as the basis for labeling functions in a weak supervision framework. To create a classifier, we first prompt the model to answer multiple distinct queries about an example and define how the possible responses should be mapped to votes for labels and abstentions. We then denoise these noisy label sources using the Snorkel system and train an end classifier with the resulting training data. Our experimental evaluation shows that prompting large language models within a weak supervision framework can provide significant gains in accuracy. On the WRENCH weak supervision benchmark, this approach can significantly improve over zero-shot performance, an average 19.5% reduction in errors. We also find that this approach produces classifiers with comparable or superior accuracy to those trained from hand-engineered rules. 
> - **Sources**: [online](http://zotero.org/users/13492210/items/AP4HTIQN) [local](zotero://select/library/items/AP4HTIQN) [pdf](file:////Users/lukexie/Zotero/storage/3UVW6CJY/Smith%20et%20al.%20-%202023%20-%20Language%20Models%20in%20the%20Loop%20Incorporating%20Prompti.pdf) 
> - **Bibliography**: Smith, R., Fries, J. A., Hancock, B., & Bach, S. H. (2023). Language Models in the Loop: Incorporating Prompting into Weak Supervision. _ACM / IMS Journal of Data Science_. [https://doi.org/10.1145/3617130](https://doi.org/10.1145/3617130)
> - **Cite Key:** [[@smithLanguageModelsLoop2023]] 
> - **Conclusion**: Developing flexible methods to query and adapt large-scale foundation models for downstream tasks is emerging as a critical component of machine learning systems. Our work demonstrates several benefits of using prompted weak supervision to query and repurpose information found in language models. Combining multiple prompted labeling functions provides significant improvements over underspecified prompts commonly used for zero-shot classification. By formulating tasks using multiple prompts, prompted weak supervision provides an inspectable mechanism for contextualizing task insight and querying knowledge found in large language models. 
>   
>   Prompts provide several advantages that compliment traditional code-based labeling functions. Unlike code, which is static and potentially expensive to refine, prompts are interpreted by an underlying language model, meaning the labels generated by prompts may improve as language models themselves continue improving. Moreover, the prompts explored in this work likely underestimate the potential performance of our approach, as we focused on translating existing labeling functions rather than developing and refining new prompts. In our experiments, T0++, which was pretrained with multi-task prompted examples, consistently outperforms the InstructGPT family of language models when used for prompted weak supervision. Future work may consider methods of generating additional prompted pretraining data that aligns more closely with how SMEs approach prompt design in weakly supervised workflows. This is a particularly exciting use of data exhaust, as the process of querying and interacting with a language model can be used to directly improve the quality of the underlying model [37]. 
>   
>   Finally, the success of contextual calibration underscores the benefits and current limitations of recalibration methods for prompt-based zero-shot learning. Performance gains, while consistent at the level of collections of prompts, is inconsistent and brittle at the level of an individual prompt. As new methods continue to improve language model calibration, we expect prompted weak supervision to benefit by increasing the ability of SMEs to refine the operating threshold of individual labeling functions.
> - **Tags:** #weak-supervision, #zero-shot-learning


>[!Personal Summary] 


# Questions
## Questions Regarding to This Paper


>[!question] 
>What are the *primary assumptions* behind this paper?

- [[Weak Supervision Basic Problem and Assumptions]]
- The zero-shot capability of LLM to reason and solve complex unseen tasks
-  


>[!question]
>What are the major *problems of concern* behind this paper? If i had to guess, what does the author seems to be concerned. 

- [[Weak Supervision Basic Problem and Assumptions]]
- the design of labeling functions (LFs) requires hand-written code and other models
- Native prompting LLM to label training data has limitations:
	1. LLM are sensitive to prompting words, phrasing etc.
	2. prompted models cannot follow too complicated instructions
	3. there may exists sophisticated contexts behind decisions, which cannot be summarized in a single prompt. 


>[!question]
>What are *related publications* before this paper? What may be *after* this paper? What excites you and why?

- Works that use LLMs to generate or modify text examples conditioned on a desired label that can be used for training
- Other recent works use pre-trained models to aid in labeling unlabeled examples
- Co-training approach to iteratively generate training data for variations of the same prompt.
- Prompting and Labeled training samples are used to generate new LFs
- Use embeddings to capture which examples are based labeled by labeling functions (Active learning setting)
- Previous works on PWS.


>[!question]
>What are *the main contributions* of the paper?

- This paper propose to design heuristic LF as zero-shot prompts for LLM, and then use label model to resolve conflicts. It is easy to specify heuristic labels using natural languages as compared to generate codes.
- The authors provide a framework that incorporates LLM into PWS. This framework also allows user to refine the decision boundaries with multiple prompts, given small set of labeled samples
- Specifically, prompts are considered as LFs by adding additional meta data that maps possible completions to target labels or abstentions. It can map negative response to abstentions. 
- The authors explore pre-trained language models as knowledge source that can be queried in *multiple complementary ways* to create training data.
- The authors demonstrate that they can achieve performance improvement on task of interest by prompting LLMs on multiple heuristic tasks. 
- The authors show that transforming code-based LFs to prompt-based LFs would achieve performance gain. 


>[!question]
>What *technical terms and vocabulary* it used?

- Programmatic Weak Supervision
- Zero-shot Learning
- Pre-trained Language Models
- Subject Matter Expert (SME)
- Prompting
- labeling functions
- prompted labeling functions
- prompt template
- label map
- contextual calibration



>[!question] Other Questions
> - What is the specific case that the study focus on?
> - What is the argument in this paper?
> - What is the specific questions posed by the study of the paper?


## System Design and Workflow

![[figure 2 language model in the loop.png]]

- User is Subject Matter Expert (SME). SME has access to a small amount of development data with manual labels. SME has a large amount of unlabeled data. 
- SME develops heuristics for labeling examples by inspecting unlabeled examples. Heuristics are represented as prompts that capture some aspect or feature of the data that is likely to indicate the true label. 
- This heuristic prompts are encapsulated as *prompted labeling function*. It contains a *prompt template* and a *label map*. 
	- The prompt template defines how the SME’s prompt is applied to unlabeled examples. Unlabeled examples consist of one or more fields of text. In this work, we focus on Yes/No question answering-style prompt templates. 
	- The label map then defines how responses by the pre-trained language model are mapped to votes on the true label for the example. Label map would also map answers to abstentions. 
	- SME can evaluate labeling function outputs on development set. They can also construct new ones or add new constraints. 
- After prompted labeling function, use standard PWS framework such as Snorkel [[ratnerSnorkelRapidTraining2020]] to produce votes on all unlabeled samples. Then aggregate these votes in label model to produce probabilistic labels 
- Train deep learning model as end model

>[!quote]
 >"In this way, language models in the loop enable SMEs to distill information locked away in large foundation models into smaller, more servable models."

### Prompted LFs

- LLMs can provide useful signals for many task of interest but may not know how to integrate them.
- PWS provide a paradigm to integrate these signals effectively. 
- In this work, multiple prompts are used to collect multiple signals related to the task of interest. 
- rule-based LFs are brittle. In contrast, prompts can handle significant amounts of ambiguity.
- Strategy of multiple prompts that covers wide-range of scenarios, or reference to specific entities, or specific phrases/keywords. 

### Calibrations

- Miscalibration creates challenges in prompting, which requires choosing the most likely answer from a set of candidate text completions.
- This paper used *contextual calibration* method. 
	- "scaling weights are estimated from the predicted token probabilities of a prompt queried using “content-free” or null input instances. Contextual calibration has demonstrated empirical performance gains when used in prompt-based, few-shot classification."


## Primary Sources


>[!question]
>What are the *primary sources* to the problem of this paper? list important the datasets, code repositories, literature citations

Datasets
- **WRENCH benchmark**: Zhang, Jieyu, Yue Yu, Yinghao Li, Yujing Wang, Yaming Yang, Mao Yang, and Alexander Ratner. “WRENCH: A Comprehensive Benchmark for Weak Supervision.” In _Thirty-Fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_. arXiv, 2021. [https://doi.org/10.48550/arXiv.2109.11377](https://doi.org/10.48550/arXiv.2109.11377).
	- text classification tasks: *YouTube*, *SMS*, and *Spouse*

![[wrench dataset.png]]


Two alternative models in comparison:
- distilling pre-trained language models in a zero-shot fashion
- hand-written labeling functions

LLM models:
- GPT-3: InstructGPT (Ada, Babbage, Curie), API submission
- T0++: open source 11B LLM with T5 architecture

Additional settings:
- "Prompts were developed for GPT-3 and T0++ separately by iteratively querying each language model with unlabeled training instances, performing an ad hoc performance assessment, and then selecting a single prompt to use per labeling function. This mirrors the process by which a SME might query a language model to guide prompt development." (Smith et al., 2023, p. 9)
  
- "We compare *three approaches* for programmatically generating training labels, following the typical workflow used for weakly supervised learning." The three approaches we compare are: 
	- *WRENCH Benchmark*: The original WRENCH labeling functions are used with majority vote for label model.
	- *Zero Shot*: a zero-shot baseline. The training set are labeled by one prompt with direct query on the class label. 
	- *Prompted Weak Supervision*: The prompted versions of the WRENCH labeling functions. 
(Smith et al., 2023, p. 9)

>[!question]
>*How* is the source _primary_ to the problem of this paper? List major results from *literary, theoretical and experimental perspectives* that support the conclusion of the paper. Also explain the success criteria that are used in this paper.


>[!quote]
>“We find that 
>1. Creating models via prompted labeling functions can significantly outperform directly prompting the model to solve the task of interest, with an average improvement of *20.2 percentage points*, and 
>2. Translating labeling functions expressed as code into prompts can lead to significantly improved weakly supervised models, with an average improvement of *7.1 percentage points*, when using our best language model, T0++ [46].” 
>   
>   \- (Smith et al., 2023, p. 8)



> [!question]
> What do I notice about the source (data, theory, reference)? list feature-question pair from the sources








>[!question] 
>Which of the feature-question in this paper lights my fire?





>[!question]
>What is the *very next primary source* that the author might want to find?


## Questions Related to Me


> [!question] 
> Is the problem behind this paper related to problem of mine?



> [!question] 
> What impact does this paper have on me?



> [!question] 
> How does the author call my problem of concern?



>[!question]
>What aspects of the paper that *excite* me the most? Why, if i have to guess?



>[!question]
>What aspects of the paper that are *boring* to me? Why, if i have to guess?




>[!question]
  What vocabulary does this author, who clearly seems to be kept awake at night by the same gnawing question as I am, use to describe themself, whether professionally, intellectually, or otherwise?



## Advanced Questions

### Limitations and Restrictions


>[!question]
>What are the *limitations* of this paper? What criticism towards the perspective of this paper?
>> [!question] Follow-Up Questions
>> - What question the author did not ask? but for me they should.
>> - What are the limitations on the assumptions of the paper?
>> - What are the limitations and restrictions for primary sources in the paper? 
>> - Are there untold tricks to achieve the results?
>> - Are we satisfied with the conclusion of the paper? why or why not?
>> - Under what situation the conclusion of this paper do not hold?
>> - Under what condition the major argument do not hold? 
>> - How much resources do we need to replicate the results?
>> - Can we use this paper in real world applications?
>> - What are the costs to use this paper in real life applications?




> [!question] 
> What questions and concerns that i might have after reading the paper?
> 


### Connection, Expansion and Generalization


>[!todo] Connect More Papers
>- What are related genres of questions that might related to the author's problem?
>- Listed all the author's questions, connected to other papers. Identify as many questions as possible.



>[!todo] Problem Summarization and Expansion
>
>- Identity patterns among these questions of the author. What does this author seem to be concerned about or preoccupied with? 
>- try to identify a common theme that connect different author's problems; 
>- see past the author’s case study, and identify the deeper-seated problem



> [!question]
> Change one variable regarding the features of the problem behind this paper. What new problem do you see? Could you find related papers? 





>[!todo]
> Try to group papers of different author together




----

## Reference and Related Notes


- [[Large Language Model and Pretrained Language Models]]