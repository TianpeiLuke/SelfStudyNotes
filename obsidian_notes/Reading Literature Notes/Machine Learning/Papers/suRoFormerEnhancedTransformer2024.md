---
tags:
- '#paper'
aliases: "suRoFormerEnhancedTransformer2024"
year: 2024
name: "RoFormer: Enhanced transformer with Rotary Position Embedding"
authors: "Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, Yunfeng Liu"
publication: "Neurocomputing"
type: "journalArticle"
DOI: "10.1016/j.neucom.2023.127063"
date of note: 2024-11-26 
---
# Descriptions

## RoFormer: Enhanced transformer with Rotary Position Embedding 
> [!info] 
> - **Abstract:** Position encoding has recently been shown to be effective in transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding (RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in the self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model_doc/roformer. 
> - **Sources**: [online](http://zotero.org/users/13492210/items/LMPIVLEA) [local](zotero://select/library/items/LMPIVLEA) [pdf](file:////Users/lukexie/Zotero/storage/TDQUHMVD/Su%20et%20al.%20-%202024%20-%20RoFormer%20Enhanced%20transformer%20with%20Rotary%20Positio.pdf) 
> - **Bibliography**: Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., & Liu, Y. (2024). RoFormer: Enhanced transformer with Rotary Position Embedding. _Neurocomputing_, _568_, 127063. [https://doi.org/10.1016/j.neucom.2023.127063](https://doi.org/10.1016/j.neucom.2023.127063)
> - **Cite Key:** @suRoFormerEnhancedTransformer2024 
> - **Conclusion**:
> - **Tags:** #Natural-language-processing, #Position-information-encoding, #Pre-trained-language-models, #Pre-training


>[!note] Highlights:
>
>-
>-
>-



# Questions
## Questions Regarding to This Paper


>[!question] 
>What are the *primary assumptions* behind this paper?



>[!question]
>What are the major *problems of concern* behind this paper? If i had to guess, what does the author seems to be concerned. 




>[!question]
>What are *the main contributions* of the paper?



## Highlight of Technical Details


>[!todo]
>Highlight the main techniques proposed in this paper. Also highlight the performance comparison to state-of-the-art.



## Questions Related to Me


> [!question] 
> Is the problem behind this paper related to problem of mine?



> [!question] 
> What impact does this paper have on me?




----

## Reference and Related Notes