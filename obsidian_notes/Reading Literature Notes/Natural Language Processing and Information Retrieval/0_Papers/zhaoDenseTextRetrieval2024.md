---
tags:
- '#paper'
aliases: "zhaoDenseTextRetrieval2024"
year: 2024
name: "Dense Text Retrieval Based on Pretrained Language Models: A Survey"
authors: "Wayne Xin Zhao, Jing Liu, Ruiyang Ren, Ji-Rong Wen"
publication: "ACM Trans. Inf. Syst."
type: "journalArticle"
DOI: "10.1145/3637870"
date of note: 2024-11-29 
---
# Descriptions

## Dense Text Retrieval Based on Pretrained Language Models: A Survey 
> [!info] 
> - **Abstract:** Text retrieval is a long-standing research topic on information seeking, where a system is required to return relevant information resources to userâ€™s queries in natural language. From heuristic-based retrieval methods to learning-based ranking functions, the underlying retrieval models have been continually evolved with the ever-lasting technical innovation. To design effective retrieval models, a key point lies in how to learn text representations and model the relevance matching. The recent success of pretrained language models&nbsp;(PLM) sheds light on developing more capable text-retrieval approaches by leveraging the excellent modeling capacity of PLMs. With powerful PLMs, we can effectively learn the semantic representations of queries and texts in the latent representation space, and further construct the semantic matching function between the dense vectors for relevance modeling. Such a retrieval approach is called dense retrieval, since it employs dense vectors to represent the texts. Considering the rapid progress on dense retrieval, this survey systematically reviews the recent progress on PLM-based dense retrieval. Different from previous surveys on dense retrieval, we take a new perspective to organize the related studies by four major aspects, including architecture, training, indexing and integration, and thoroughly summarize the mainstream techniques for each aspect. We extensively collect the recent advances on this topic, and include 300+ reference papers. To support our survey, we create a website for providing useful resources, and release a code repository for dense retrieval. This survey aims to provide a comprehensive, practical reference focused on the major progress for dense text retrieval. 
> - **Sources**: [online](http://zotero.org/users/13492210/items/TA6W2Z5X) [local](zotero://select/library/items/TA6W2Z5X) [pdf](file:////home/lukexie/Documents/Papers/storage/E7XZLKSM/Zhao%20et%20al.%20-%202024%20-%20Dense%20Text%20Retrieval%20Based%20on%20Pretrained%20Language%20Models%20A%20Survey.pdf) 
> - **Bibliography**: Zhao, W. X., Liu, J., Ren, R., & Wen, J.-R. (2024). Dense Text Retrieval Based on Pretrained Language Models: A Survey. _ACM Trans. Inf. Syst._, _42_(4), 89:1-89:60. [https://doi.org/10.1145/3637870](https://doi.org/10.1145/3637870)
> - **Cite Key:** [[@zhaoDenseTextRetrieval2024]] 
> - **Conclusion**:


>[!note] Highlights:
>
>-
>-
>-



# Questions
## Questions Regarding to This Paper


>[!question] 
>What are the *primary assumptions* behind this paper?



>[!question]
>What are the major *problems of concern* behind this paper? If i had to guess, what does the author seems to be concerned. 




>[!question]
>What are *the main contributions* of the paper?



## Highlight of Technical Details


>[!todo]
>Highlight the main techniques proposed in this paper. Also highlight the performance comparison to state-of-the-art.



## Questions Related to Me


> [!question] 
> Is the problem behind this paper related to problem of mine?



> [!question] 
> What impact does this paper have on me?




----

## Reference and Related Notes