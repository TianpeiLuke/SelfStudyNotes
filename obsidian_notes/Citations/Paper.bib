@online{bestaGraphThoughtsSolving2024,
  title = {Graph of {{Thoughts}}: {{Solving Elaborate Problems}} with {{Large Language Models}}},
  shorttitle = {Graph of {{Thoughts}}},
  author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
  date = {2024-02-06},
  eprint = {2308.09687},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.09687},
  url = {http://arxiv.org/abs/2308.09687},
  urldate = {2024-02-19},
  abstract = {We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over ToT, while simultaneously reducing costs by {$>$}31\%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {\section{Graph of Thoughts: reading digest}},
  file = {/home/lukexie/Documents/Papers/storage/GZQ6MX4M/Besta et al. - 2024 - Graph of Thoughts Solving Elaborate Problems with.pdf;/home/lukexie/Documents/Papers/storage/FHDDQVPK/2308.html}
}

@inproceedings{clarkTransformersSoftReasoners2021,
  title = {Transformers as {{Soft Reasoners}} over {{Language}}},
  shorttitle = {{{RuleTakers}}},
  booktitle = {Proceedings of the {{Twenty-Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Clark, Peter and Tafjord, Oyvind and Richardson, Kyle},
  date = {2021-01},
  series = {{{IJCAI}}'20},
  pages = {3882--3890},
  location = {Yokohama, Yokohama, Japan},
  abstract = {Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99\%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95\%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited "soft theorem provers" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering.},
  eventtitle = {{{IJCAI}}'20},
  isbn = {978-0-9992411-6-5},
  file = {/home/lukexie/Documents/Papers/storage/LJ3TPMK7/Clark et al. - 2021 - Transformers as soft reasoners over language.pdf}
}

@inproceedings{labanSummEditsMeasuringLLM2023,
  title = {{{SummEdits}}: {{Measuring LLM Ability}} at {{Factual Reasoning Through The Lens}} of {{Summarization}}},
  shorttitle = {{{SummEdits}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Laban, Philippe and Kryscinski, Wojciech and Agarwal, Divyansh and Fabbri, Alexander and Xiong, Caiming and Joty, Shafiq and Wu, Chien-Sheng},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  date = {2023-12},
  pages = {9662--9676},
  publisher = {Association for Computational Linguistics},
  location = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.600},
  url = {https://aclanthology.org/2023.emnlp-main.600},
  urldate = {2024-02-26},
  abstract = {With the recent appearance of LLMs in practical settings, having methods that can effectively detect factual inconsistencies is crucial to reduce the propagation of misinformation and improve trust in model outputs. When testing on existing factual consistency benchmarks, we find that a few large language models (LLMs) perform competitively on classification benchmarks for factual inconsistency detection compared to traditional non-LLM methods. However, a closer analysis reveals issues with existing evaluation benchmarks, affecting evaluation precision. To address this, we propose a new protocol for inconsistency detection benchmark creation and implement it in a 10-domain benchmark called SummEdits. This new benchmark is 20 times more cost-effective per sample than previous benchmarks and highly reproducible, as we estimate inter-annotator agreement at about 0.9. Most LLMs struggle on SummEdits, with performance close to random chance. The best-performing model, GPT-4, is still 8\% below estimated human performance, highlighting the gaps in LLMs' ability to reason about facts and detect inconsistencies when they occur.},
  eventtitle = {{{EMNLP}} 2023},
  file = {/home/lukexie/Documents/Papers/storage/8TF4TSAA/Laban et al. - 2023 - SummEdits Measuring LLM Ability at Factual Reason.pdf}
}

@online{sahaMURMURModularMultiStep2022,
  title = {{{MURMUR}}: {{Modular Multi-Step Reasoning}} for {{Semi-Structured Data-to-Text Generation}}},
  shorttitle = {{{MURMUR}}},
  author = {Saha, Swarnadeep and Yu, Xinyan Velocity and Bansal, Mohit and Pasunuru, Ramakanth and Celikyilmaz, Asli},
  date = {2022-12-16},
  eprint = {2212.08607},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.08607},
  url = {http://arxiv.org/abs/2212.08607},
  urldate = {2024-02-26},
  abstract = {Prompting large language models has enabled significant recent progress in multi-step reasoning over text. However, when applied to text generation from semi-structured data (e.g., graphs or tables), these methods typically suffer from low semantic coverage, hallucination, and logical inconsistency. We propose MURMUR, a neuro-symbolic modular approach to text generation from semi-structured data with multi-step reasoning. MURMUR is a best-first search method that generates reasoning paths using: (1) neural and symbolic modules with specific linguistic and logical skills, (2) a grammar whose production rules define valid compositions of modules, and (3) value functions that assess the quality of each reasoning step. We conduct experiments on two diverse data-to-text generation tasks like WebNLG and LogicNLG. These tasks differ in their data representations (graphs and tables) and span multiple linguistic and logical skills. MURMUR obtains significant improvements over recent few-shot baselines like direct prompting and chain-of-thought prompting, while also achieving comparable performance to fine-tuned GPT-2 on out-of-domain data. Moreover, human evaluation shows that MURMUR generates highly faithful and correct reasoning paths that lead to 26\% more logically consistent summaries on LogicNLG, compared to direct prompting.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: 22 pages (9 figures, 18 tables)},
  file = {/home/lukexie/Documents/Papers/storage/6CUW2ZMR/Saha et al. - 2022 - MURMUR Modular Multi-Step Reasoning for Semi-Stru.pdf;/home/lukexie/Documents/Papers/storage/LXKBJP8Z/2212.html}
}

@inproceedings{weiChainofThoughtPromptingElicits2022,
  title = {Chain-of-{{Thought Prompting Elicits Reasoning}} in {{Large Language Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc V. and Zhou, Denny},
  date = {2022-12-06},
  series = {{{NeurIPS}}' 22},
  volume = {35},
  pages = {24824--24837},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html},
  urldate = {2024-02-19},
  abstract = {We explore how generating a chain of thought---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  eventtitle = {Advances in {{Neural Information Processing Systems}} 35 ({{NeurIPS}} 2022)},
  langid = {english},
  file = {/home/lukexie/Documents/Papers/storage/6NZCLP27/NeurIPS-2022-chain-of-thought-prompting-elicits-reasoning-in-large-language-models-Supplemental-Conference.pdf;/home/lukexie/Documents/Papers/storage/IV8ZRUZP/Wei et al. - 2022 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf}
}

@online{wenMindMapKnowledgeGraph2023,
  title = {{{MindMap}}: {{Knowledge Graph Prompting Sparks Graph}} of {{Thoughts}} in {{Large Language Models}}},
  shorttitle = {{{MindMap}}},
  author = {Wen, Yilin and Wang, Zifeng and Sun, Jimeng},
  date = {2023-09-15},
  eprint = {2308.09729},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.09729},
  url = {http://arxiv.org/abs/2308.09729},
  urldate = {2024-02-19},
  abstract = {LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question \& answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, prompting a GPT-3.5 with MindMap yields an overwhelming performance over GPT-4 consistently. We also demonstrate that with structured facts retrieved from KG, MindMap can outperform a series of prompting-with-document-retrieval methods, benefiting from more accurate, concise, and comprehensive knowledge from KGs. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl.willing/MindMap.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {\section{MindMap Summary Note}

\begin{itemize}

\item Topic: LLM prompt inference with Knowledge Graph as input
\item LLM is fixed; KG prebuilt but can be expanded using input query and LLM inference reasoning
\item 
\par
important input to prompt:
\par
\begin{itemize}

\item path evidence subgraph
\item neighborhood evidence subgraph

\end{itemize}

\item 
\par
key steps:
\par
\begin{itemize}

\item 
\par
Evidence graph mining
\par
\begin{itemize}

\item entity extraction
\item subgraph exploration

\end{itemize}

\item Evidence graph aggregation
\item LLM reasoning on mind graph

\end{itemize}

\end{itemize}},
  file = {/home/lukexie/Documents/Papers/storage/TUU6BHN5/Wen et al. - 2023 - MindMap Knowledge Graph Prompting Sparks Graph of.pdf;/home/lukexie/Documents/Papers/storage/6AAZJI96/2308.html}
}

@article{yangGiveUsFacts2024,
  title = {Give {{Us}} the {{Facts}}: {{Enhancing Large Language Models}} with {{Knowledge Graphs}} for {{Fact-aware Language Modeling}}},
  shorttitle = {Give {{Us}} the {{Facts}}},
  author = {Yang, Linyao and Chen, Hongyang and Li, Zhao and Ding, Xiao and Wu, Xindong},
  date = {2024},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  pages = {1--20},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2024.3360454},
  url = {https://ieeexplore.ieee.org/abstract/document/10417790},
  urldate = {2024-02-19},
  abstract = {Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention. Due to their powerful emergent abilities, recent LLMs are considered as a possible alternative to structured knowledge bases like knowledge graphs (KGs). However, while LLMs are proficient at learning probabilistic language patterns and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance in generating texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications. Inspired by existing studies on KGPLM, this paper proposes enhancing LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs). KGLLM provides a solution to enhance LLMs’ factual reasoning ability, opening up new avenues for LLM research.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {Chatbots,ChatGPT,Knowledge based systems,Knowledge graph,Knowledge graphs,Knowledge management,Knowledge reasoning,Large language model,Long short term memory,Task analysis,Training,Transformers},
  file = {/home/lukexie/Documents/Papers/storage/6NYQ2SIG/Yang et al. - 2024 - Give Us the Facts Enhancing Large Language Models.pdf;/home/lukexie/Documents/Papers/storage/D8MGSTTN/10417790.html}
}

@inproceedings{yaoTreeThoughtsDeliberate2024,
  title = {Tree of {{Thoughts}}: {{Deliberate Problem Solving}} with {{Large Language Models}}},
  shorttitle = {Tree of {{Thoughts}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  date = {2024-02-13},
  series = {{{NeurIPS}}' 23},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html},
  urldate = {2024-02-19},
  abstract = {Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.Our experiments show that ToT significantly enhances language models’ problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4\textbackslash\% of tasks, our method achieved a success rate of 74\textbackslash\%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.},
  eventtitle = {Advances in {{Neural Information Processing Systems}} 36 Pre-Proceedings ({{NeurIPS}} 2023) {{Main Conference Track}}},
  langid = {english},
  file = {/home/lukexie/Documents/Papers/storage/6X4IYSBQ/Yao et al. - 2024 - Tree of Thoughts Deliberate Problem Solving with .pdf}
}
