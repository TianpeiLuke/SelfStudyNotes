\documentclass[11pt]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent %\usepackage{graphicx}
\usepackage{amsmath,amssymb, amscd}
\usepackage{mathrsfs, dsfont}
\usepackage[all,cmtip]{xy}
\usepackage{tikz-cd}
%\diagramstyle[labelstyle=\scriptstyle]
\usepackage{tabularx}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{xcolor}
%\usepackage[linkbordercolor ={1 1 1} ]{hyperref}
%\usepackage[sf]{titlesec}
\usepackage{natbib}
\usepackage{../../Tianpei_Report}

%\usepackage{appendix}
%\usepackage{algorithm}
%\usepackage{algorithmic}

%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}



\begin{document}
\title{Lecture 6: Martingale}
\author{ Tianpei Xie}
\date{ Feb.2nd., 2023 }
\maketitle
\tableofcontents
\newpage
\section{Conditional Expectation}
\begin{itemize}
\item \begin{definition} (\emph{\textbf{Conditional Expectation}}) \citep{resnick2013probability}\\
Let $(\Omega, \srF, \cP)$ be a probability space and $\srG \subset \srF$ be a sub-$\sigma$-algebra. Suppose $X\in  L^{1}(\Omega, \srF, \cP)$. There exists a function $\E{}{X| \srG}$, called the \underline{\emph{\textbf{conditional expectation}}} of $X$ \emph{\textbf{with respect to}} $\srG$ such that
\begin{enumerate}
\item $\E{}{X| \srG}$ is \emph{\textbf{$\srG$-measureable}} and \emph{\textbf{integrable}} \emph{with respect to} $\cP$. 

\item $\E{}{X| \srG}$ satisfies \emph{\textbf{the functional equation}}: 
\begin{align*}
\int_{G} X d\cP &= \int_{G} \E{}{X| \srG} d\cP, \quad \forall\, G \in \srG.
\end{align*}
\end{enumerate}
\end{definition}

\item \begin{remark}
To \emph{prove the \textbf{existence}} of such a random variable, 
\begin{enumerate}
\item consider first the case of
\emph{\textbf{nonnegative} X}. Define a measure $\nu$ on $\srG$ by 
\begin{align*}
\nu(G) = \int_{G} X d\cP = \int_{\Omega} X \mathds{1}_{G}\; d\cP.
\end{align*}
This measure is \emph{finite} because $X$ is \emph{integrable}, and it is \emph{\textbf{absolutely continuous}} with respect to $\cP$. By the \emph{Lebesgue-Radon-Nikodym Theorem},  there is a $\srG$-measurable function $f$
such that 
\begin{align*}
\nu(G) = \int_{G} f d\cP.
\end{align*}
This $f$ has properties (1) and (2). 
\item If $X$ is \emph{not necessarily nonnegative}, $\E{}{X_{+}| \srG} - \E{}{X_{-}| \srG}$ clearly has the required properties.
\end{enumerate}
\end{remark}

\item \begin{remark}
As $\srG$ increases, condition (1) becomes \emph{\textbf{weaker}} and condition (2) becomes \emph{\textbf{stronger}}.
\end{remark}

\item \begin{remark}
Let $(\Omega, \srF, \cP)$ be a probability space, with  $\srG \subset \srF$ a sub-$\sigma$-algebra, define
\begin{align*}
\cP[A | \srG] &= \E{}{\mathds{1}_{A}| \srG}
\end{align*} for all $A\in \srF$.
\end{remark}

\item \begin{remark}
By definition, the conditional expectation is a \emph{\textbf{Radon-Nikodym derivative}} of $d\nu|_{\srG} = X d\cP|_{\srG}$ w.r.t. $d\cP|_{\srG}$ within $\srG$.
\begin{align*}
\E{}{X| \srG}&:=  \frac{Xd\cP|_{\srG}}{d\cP|_{\srG}} = X|_{\srG}.
\end{align*} Thus \underline{\emph{$\E{}{X| \srG}$ is the \textbf{projection} of $X$ on \textbf{sub $\sigma$-algebra} $\srG$}}.
\end{remark}

\item \begin{remark} (\emph{\textbf{Conditioning on Random Variables}})\\
By definition, conditioning on random variables $(X_{t}, t\in T)$ on $(\Omega, \srB)$ can be expressed as 
\begin{align*}
\E{}{X| X_{t}, t\in T} &\equiv \E{}{X| \sigma(X_{t}, t\in T)}, 
\end{align*}
where $\sigma(X_{t}, t\in T)$ is the $\sigma$-algebra generated by the cylinder set
\begin{align*}
C_{n}[A]&\equiv \set{\omega: (X_{t}(\omega), 1\le t\le n) \in A  } \in \srB, \quad A\in \cB(\bR^{n}), \forall \, n
\end{align*}
\end{remark}

\item \begin{remark}(\emph{\textbf{$\sigma$-Algebra Generated by Partition of Sample Space}})\\
As above, assume that the sub $\sigma$-algebra $\srG$ is generated by a \emph{\textbf{partition}} $B_1, B_2, \ldots$ of $\Omega$, then for $X \in L^1(\Omega, \srF, \cP)$, 
\begin{align*}
\E{}{X | B_i} &= \int X d\cP(X | B_i) = \int_{B_i} X d\cP / \cP(B_i)
\end{align*} where $\cP(X | B_i)$ is \emph{the conditional probability} defined in previous section. If $\cP(B_i) = 0$, then $\E{}{X | B_i} = 0$.
We claim that 
\begin{enumerate}
\item 
\begin{align*}
\E{}{X | \srG} &= \sum_{i=1}^{\infty}\E{}{X | B_i} \mathds{1}_{B_i}, \quad a.s.
\end{align*}
\item For any $A \in \srF$,
\begin{align*}
\cP(A | \srG) &= \sum_{i=1}^{\infty}\cP(A | B_i) \mathds{1}_{B_i}, \quad a.s.
\end{align*}
\end{enumerate}
\end{remark}



\item \begin{remark}
Both $P[A | \srF]$ and $\E{}{X| \srF}$ are \emph{random variables} from $\Omega \rightarrow \bR$. Formally speaking, 
\begin{align*}
P\brac{(X,Y)\in A| \sigma(X)}_{\omega} &\equiv P\brac{(X(\omega), Y)\in A}\\
&= P\set{\omega': (X(\omega), Y(\omega'))\in A}\\
&\equiv f(X(\omega))\\
&= \rlat{\nu}{\sigma(X)}(A)\\
\E{}{(X,Y)| \sigma(X)}_{\omega} &= \lim\limits_{m(A)\rightarrow 0\atop \omega\in A\in \sigma(X)} \frac{P\set{\omega': (X(\omega), Y(\omega'))\in A}}{m(A)}
\end{align*}
It is the expected value of $X$ for someone who knows for each $E\in \srF$, whether or not $\omega\in E$, which $E$ itself remains unknown.
\end{remark}

\item \begin{proposition} (\textbf{Properties of Conditional Expectation}) \citep{resnick2013probability}\\
Let $(\Omega, \srF, \cP)$ be a probability space and $\srG \subset \srF$ be a sub-$\sigma$-algebra. Suppose $X, Y \in  L^{1}(\Omega, \srF, \cP)$ and $\alpha, \beta \in \bR$.
\begin{enumerate}
\item (\textbf{Linearity}): $\E{}{\alpha X+ \beta Y | \srG} = \alpha \E{}{X| \srG} +\beta  \E{}{Y | \srG}$;
\item (\textbf{Projection}): If $X$ is \textbf{$\srG$-measurable}, then $\E{}{X | \srG} = X$ almost surely.
\item (\textbf{Conditioning on Indiscrete $\sigma$-Algebra}): 
\begin{align*}
\E{}{X | \set{\emptyset, \Omega}} = \E{}{X}.
\end{align*}
\item (\textbf{Monotonicity}):  If $X \ge 0$, then $\E{}{X | \srG} \ge 0$ almost surely. Similarly, if $X \ge Y$, then $\E{}{X | \srG} \ge \E{}{Y | \srG}$  almost surely.
\item (\textbf{Modulus Inequality}): 
\begin{align*}
\abs{\,\E{}{X | \srG}\,} &\le \E{}{\,\abs{X} \, | \srG}.
\end{align*}
\item (\textbf{Monotone Convergence Theorem}): If  $\set{X_n}_{n=1}^{\infty} \subset  L^{1}(\Omega, \srF, \cP)$,  $0 \le X_1 \le X_{2}\le \ldots$ is a \textbf{monotone sequence} of \textbf{non-negative} random variables and $X_n \rightarrow X$ then
\begin{align*}
\lim\limits_{n\rightarrow \infty}\E{}{X_n | \srG} &= \E{}{\lim\limits_{n\rightarrow \infty} X_n \big| \srG} = \E{}{X | \srG}.
\end{align*}
\item (\textbf{Fatou Lemma}): If  $\set{X_n}_{n=1}^{\infty} \subset  L^{1}(\Omega, \srF, \cP)$, and $X_n \ge 0$ for all $n$, then
\begin{align*}
\E{}{\liminf\limits_{n\rightarrow \infty} X_n \big| \srG} &\le \liminf\limits_{n\rightarrow \infty} \E{}{X_n | \srG} 
\end{align*} 
\item  (\textbf{Dominated Convergence Theorem}):  If  $\set{X_n}_{n=1}^{\infty} \subset  L^{1}(\Omega, \srF, \cP)$ and $\abs{X_n} \le Z$, where $Z \in L^{1}(\Omega, \srF, \cP)$ is a random variable, $X_n \rightarrow X$ almost surely,  then
\begin{align*}
\lim\limits_{n\rightarrow \infty}\E{}{X_n | \srG} &= \E{}{\lim\limits_{n\rightarrow \infty} X_n \big| \srG} = \E{}{X | \srG}, \quad a.s.
\end{align*}
\item (\textbf{Product Rule}): If $Y$ is $\srG$-measurable, 
\begin{align*}
\E{}{X\,Y | \srG} &= Y\,\E{}{X | \srG},\;\; \text{ a.s.}
\end{align*}


\item (\textbf{Smoothing}): For $\srF_{1}\subset \srF_{0} \subset \srF$, 
\begin{align*}
\E{}{\E{}{X| \srF_{0}}\,|\srF_{1} } &= \E{}{X| \srF_{1}}\\
\E{}{\E{}{X| \srF_{1}}\,|\srF_{0} } &= \E{}{X| \srF_{1}}.
\end{align*} Note that $ \E{}{X| \srF_{1}}$ is \textbf{smoother} than $ \E{}{X| \srF_{0}}$.
Moreover
\begin{align*}
\E{}{X} = \E{}{X| \set{\emptyset, \Omega}} &= \E{}{\E{}{X| \srF_{0}}\,|\set{\emptyset, \Omega} } = \E{}{\E{}{X| \srF_{0}}}.
\end{align*}


\item (\textbf{The Conditional Jensen's Inequality}). Let $\phi$ be a \textbf{convex} function, $\phi(X) \in L^1(\Omega, \srF, \cP)$. Then almost surely
\begin{align*}
\phi\paren{\E{}{X | \srG}} &\le \E{}{\phi(X) | \srG}
\end{align*}
\end{enumerate}
\end{proposition}
\end{itemize}

\section{Martingale}
\subsection{Martingale, Sub-Martingale, Super-Martingale}
\begin{itemize}
\item \begin{definition} (\emph{\textbf{Martingale}}) \citep{resnick2013probability}\\
Let $\set{X_n, n \ge 0}$ be a stochastic process on $(\Omega, \srF)$ and $\set{\srF_n, n \ge 0}$ be a \underline{\textbf{\emph{filtration}}}; that is, $\set{\srF_n, n \ge 0}$ is an \emph{increasing sub $\sigma$-fields} of $\srF$
\begin{align*}
\srF_0 \subseteq \srF_1 \subseteq \srF_2 \xdotx{\subseteq} \srF.
\end{align*} Then $\set{ (X_n, \srF_n),  n \ge 0}$ is a \underline{\emph{\textbf{martingale (mg)}}} if
\begin{enumerate}
\item  $X_n$ is \emph{\textbf{adapted}} in the sense that for each $n$, $X_n \in \srF_n$; that is, $X_n$ is $\srF_n$-measurable.
\item  $X_n \in L_1$; that is $\E{}{\abs{X_n}} < \infty$ for $n \ge 0$.
\item For $0 \le m < n$
\begin{align}
\E{}{X_n \;|\; \srF_m} &= X_m, \quad \text{a.s.} \label{def: martingale}
\end{align}
\end{enumerate}
If the equality of \eqref{def: martingale} is replaced by $\ge$; that is, things are getting better on the average:
\begin{align}
\E{}{X_n \;|\; \srF_m} &\ge X_m, \quad \text{a.s.} \label{def: sub_martingale}
\end{align} then $\set{X_n}$ is called a \underline{\emph{\textbf{sub-martingale (submg)}}} while if things are getting worse on
the average
\begin{align}
\E{}{X_n \;|\; \srF_m} &\le X_m, \quad \text{a.s.} \label{def: sup_martingale}
\end{align}  $\set{X_n}$ is called a \underline{\emph{\textbf{super-martingale (supermg)}}}.
\end{definition}

\item \begin{remark}
$\set{X_n}$ is \emph{\textbf{martingale}} if it is \emph{both} a \emph{\textbf{sub}} and \emph{\textbf{supermartingale}}. $\set{X_n}$ is a \emph{\textbf{supermartingale}} if and only if $\set{-X_n}$ is a \emph{\textbf{submartingale}}.
\end{remark}

\item \begin{remark}
If $\set{X_n}$ is a \emph{\textbf{martingale}}, then $\E{}{X_n}$ is \emph{constant}. In the case of a \emph{\textbf{submartingale}}, \emph{the mean increases} and for a \emph{\textbf{supermartingale}}, \emph{the mean decreases}.
\end{remark}

\item \begin{proposition} \citep{resnick2013probability}\\
If  $\set{ (X_n, \srF_n),  n \ge 0}$ is a \textbf{(sub, super) martingale}, then 
\begin{align*}
\set{ (X_n, \sigma\paren{X_0, X_1 \xdotx{,} X_n}),  n \ge 0}
\end{align*} is also a \textbf{(sub, super) martingale}.
\end{proposition}
\end{itemize}

\subsection{Martingale Difference Sequence}
\begin{itemize}
\item \begin{definition} (\textbf{\emph{Martingale Differences}}).  \citep{resnick2013probability}\\
$\set{(d_j, \srB_j), j \ge 0}$ is a \underline{\emph{\textbf{(sub, super) martingale difference sequence}}} or a \textit{\textbf{(sub, super) fair sequence}} if
\begin{enumerate}
\item For $j \ge 0$,  $\srB_j \subset \srB_{j+1}$.
\item For $j \ge 0$,  $d_j \in L_1$,  $d_j \in \srB_j$; that is, $d_j$ is \emph{absolutely integrable} and \emph{$\srB_j$-measurable}.
\item For $j \ge 0$,
\begin{align*}
\E{}{d_{j+1} | \srB_j} &= 0, && \text{(\emph{martingale difference / fair sequence})};\\
& \ge 0, && \text{(\emph{submartingale difference / subfair sequence})};\\
& \le 0, && \text{(\emph{supmartingale difference / supfair sequence})}
\end{align*}
\end{enumerate}
\end{definition}

\item \begin{theorem} (\textbf{Construction of Martingale From Martingale Difference})\citep{resnick2013probability}\\
If $\set{(d_j, \srB_j), j \ge 0}$ is \textbf{(sub, super) martingale difference sequence}, and
\begin{align*}
X_n = \sum_{j=0}^{n} d_j, 
\end{align*} then $\set{(X_n, \srB_n), n \ge 0}$ is a \textbf{(sub, super) martingale}.
\end{theorem}

\item \begin{theorem} (\textbf{Construction of Martingale Difference From Martingale}) \citep{resnick2013probability}\\
Suppose $\set{(X_n, \srB_n), n \ge 0}$ is a \textbf{(sub, super) martingale}. Define
\begin{align*}
d_0&:= X_0 - \E{}{X_0}\\
d_j &:= X_j - X_{j-1}, \quad j\ge 1.
\end{align*}
Then $\set{(d_j, \srB_j), j \ge 0}$ is a \textbf{(sub, super) martingale difference sequence}.
\end{theorem}

\item \begin{theorem} (\textbf{Orthogonality of Martingale Differences}). \citep{resnick2013probability}\\
If $\set{(X_n, \srB_n), n \ge 0}$ is a \textbf{martingale} where $X_n$ can be decomposed as
\begin{align*}
X_n = \sum_{j=0}^{n} d_j, 
\end{align*}  $d_j$ is $\srB_j$-measurable and  $\mathds{E}[d_j^2] < \infty$ for $j \ge 0$, then $\set{d_j}$ are \textbf{orthogonal}:
\begin{align*}
\E{}{d_i\,d_j} = 0 \quad i \neq j.
\end{align*}
\end{theorem}
\begin{proof}
This is an easy verification: If $j > i$, then
\begin{align*}
\E{}{d_i\, d_j} &= \E{}{\E{}{d_i\, d_j \,|\, \srB_i}}\\
&= \E{}{d_i \E{}{d_j \,|\, \srB_i}} = 0. \qed
\end{align*}
A consequence is that
\begin{align*}
\E{}{X_n^2} &= \E{}{\sum_{i=1}^{n}d_i^2} + 2\sum_{0 \le i < j \le n}\E{}{d_i\,d_j} = \E{}{\sum_{i=1}^{n}d_i^2},
\end{align*}
which is \textbf{\emph{non-decreasing}}. From this, it seems likely (and turns out to be true) that $\set{X_n^2}$ is a \emph{\textbf{sub-martingale}}. 
\end{proof}
\end{itemize}

\subsection{Examples of Martingales}
\begin{itemize}
\item \begin{example} (\textbf{\emph{Smoothing as Martingale}})\\
Suppose $X \in L_1$ and $\set{\srB_n, n \ge 0}$ is an increasing family of sub $\sigma$-algebra of $\srB$. Define for $n \ge 0$
\begin{align*}
X_n &:= \E{}{X | \srB_n}.
\end{align*}
Then $(X_n, \srB_n)$ is a \emph{\textbf{martingale}}. From this result, we see that $\set{(d_n, \srB_n), n \ge 0}$ is a \emph{\textbf{martingale difference sequence}} when 
\begin{align}
d_n &:= \E{}{X | \srB_n} - \E{}{X | \srB_{n-1}}, \quad n\ge 1. \label{eqn: smoothing_martingale_difference}
\end{align}
\end{example}
\begin{proof}
See that 
\begin{align*}
\E{}{X_{n+1} | \srB_{n}} &= \E{}{ \E{}{X | \srB_{n+1}} | \srB_n} \\
&= \E{}{X | \srB_{n}}  \qquad \text{(Smoothing property of conditional expectation)}\\
&= X_n \qed
\end{align*} 
\end{proof}

\item \begin{example}(\emph{\textbf{Sums of Independent Random Variables}}) \\
Suppose that $\set{Z_n, n \ge 0}$ is an \emph{\textbf{independent} sequence of integrable random variables} satisfying for $n \ge 0$, 
$\E{}{Z_n} = 0$.  Set
\begin{align*}
X_0 &:= 0,\\
X_n &:= \sum_{i=1}^{n}Z_i, \quad n \ge 1 \\
\srB_n &:= \sigma\paren{Z_0 \xdotx{,} Z_n}.
\end{align*} Then $\set{(X_n, \srB_n), n \ge 0}$ is a \emph{\textbf{martingale}} since $\set{(Z_n, \srB_n), n \ge 0}$ is a \emph{\textbf{martingale difference sequence}}.
\end{example}


\item \begin{example} (\emph{\textbf{Likelihood Ratios}}).\\ 
Suppose $\set{Y_n, n \ge 0}$ are \emph{\textbf{independent identically distributed}} random variables and suppose \emph{the true density} of $Y_n$ is $f_0$Â· (The word ``\emph{density}" can be understood with respect to some fixed reference measure $\mu$.)  Let $f_1$ be \emph{some other probability density}. For simplicity suppose $f_0(y) > 0$, for all $y$.  For $n \ge 0$, define the likelihood ratio
\begin{align*}
X_n &:= \frac{\prod_{i=0}^{n}f_1(Y_i)}{\prod_{i=0}^{n}f_0(Y_i)}\\
\srB_n &:= \sigma\paren{Y_0 \xdotx{,} Y_n}
\end{align*} Then $(X_n, \srB_n)$ is a \emph{\textbf{martingale}}.
\end{example}
\begin{proof}
See that
\begin{align*}
\E{}{X_{n+1} | \srB_n} &= \E{}{ \paren{\frac{\prod_{i=0}^{n}f_1(Y_i)}{\prod_{i=0}^{n}f_0(Y_i)}}\frac{f_1(Y_{n+1})}{f_0(Y_{n+1})} \;  \Big| \; Y_0 \xdotx{,} Y_{n}}\\
&= X_n \E{}{\frac{f_1(Y_{n+1})}{f_0(Y_{n+1})} \;  \big| \; Y_0 \xdotx{,} Y_{n}} \\
&= X_n \E{}{\frac{f_1(Y_{n+1})}{f_0(Y_{n+1})}} \quad (\text{by independence})\\
&:= X_n \int \frac{f_1(y_{n+1})}{f_0(y_{n+1})} f_0(y_{n+1}) d\mu(y_{n+1}) = X_n. \qed
\end{align*}
\end{proof}

\item \begin{example}(\textbf{\emph{Moment Generating Functions}})\\
\end{example}


\item \begin{example}(\textbf{\emph{Method of Centering by Conditional Means}}).\\
\end{example}

\item \begin{example}(\textbf{\emph{Markov Chains}}).\\
\end{example}
\end{itemize}

\subsection{Doob's Decomposition}
\begin{itemize}
\item \begin{definition}(\textbf{\emph{Predictable and Increasing Process}}) \\
Given a process $\set{X_n, n \ge 0}$ and an increasing family of $\sigma$-algebras $\set{\srB_n, n \ge 0}$. We call $\set{X_n, n \ge 0}$ \underline{\emph{\textbf{predictable}}} if $X_0 \in \srB_0 =  \set{\emptyset, \Omega}$ and  $X_n$ is \emph{\textbf{$\srB_{n-1}$-measurable}} for each $n =1, 2, \ldots$.

Call a process $\set{A_n, n \ge 0}$ an \underline{\emph{\textbf{increasing process}}} if $\set{A_n}$ is \emph{\textbf{predictable}} and \emph{\textbf{almost
surely}} 
\begin{align*}
 0 = A_0 \le A_1 \xdotx{\le} A_n \le \ldots.
\end{align*}
\end{definition}

\item \begin{theorem} (\textbf{Doob Decomposition}) \citep{billingsley2008probability, resnick2013probability}\\
Any \textbf{submartingale} $\set{(X_n, \srB_n), n \ge 0}$ can be written in a \textbf{unique} way as the \textbf{sum} of a \textbf{martingale} $\set{(M_n, \srB_n), n \ge 0}$ and an \textbf{increasing process} $\set{A_n, n \ge 0}$; that is
\begin{align*}
X_n &= M_n + A_n, \quad n\ge 0, \quad \text{a.s.}
\end{align*}
\end{theorem}
\end{itemize}

\subsection{Stopping Time and Optional Sampling Theorem}
\begin{itemize}
\item \begin{definition}
Let $(X_t)_t$ be a stochastic process and let $\set{\srB_t, n\ge 0}$ be an increasing family of $\sigma$-algebras. Denote $\srB = \bigcup_{t\ge 0} \srB_t$.

A random variable $T: (\Omega, \srB) \rightarrow (\bN_{+}\cup\set{+\infty}, 2^{\bN_{+}\cup\set{+\infty}})$ is called a \underline{\textbf{\emph{stopping time}}} \emph{with respect to} $\set{\srB_t, t\ge 0}$, if $\mathds{1}_{\set{T = k}}$ is \textbf{\emph{$\srB_k$-measurable}} for $\forall k \ge 0$; i.e.
\begin{align*}
\set{T = k} \in \srB_{k}, \quad \forall k \ge 0.
\end{align*}
\end{definition}


\item \begin{theorem}(\textbf{Weak Version of Optional Sampling Theorem}) \citep{billingsley2008probability}\\
Let $\set{(X_n, \srB_n), n \ge 0}$ be a \textbf{martingale} where $\srB_n = \sigma\paren{X_1 \xdotx{,} X_n}$ and let $T$ be \textbf{stopping time} with respect to $\set{\srB_n, n \ge 0}$. Suppose that \textbf{at least one} of the following conditions hold:
\begin{enumerate}
\item $T \le n_0,\; \text{a.s.}$ for some constant $n_0 >0$;
\item $T < \infty,\; \text{a.s.}$ and $\abs{X_i} \le K$ where $i \le T$.
\end{enumerate} Then 
\begin{align*}
\E{}{X_T} &= \E{}{X_0}.
\end{align*} 
\end{theorem}

\item \begin{theorem}(\textbf{Weak Version of Optional Sampling Theorem, again}) \citep{billingsley2008probability}\\
Let $\set{(X_n, \srB_n), n \ge 0}$ be a \textbf{martingale} where $\srB_n = \sigma\paren{X_1 \xdotx{,} X_n}$ and let $\tau$ be \textbf{stopping time} with respect to $\set{\srB_n, n \ge 0}$. Suppose $\E{}{\tau} < \infty$, and there exists $n \ge 0$ and some constant $c$ such that 
\begin{align*}
\E{}{\abs{X_{n+1} - X_n} | \srB_n} \le c, \quad \text{a.s.} \; \text{ on }\set{\tau \ge n}.
\end{align*}
Then $\E{}{\abs{X_{\tau}}} &< \infty$ and
\begin{align*}
\E{}{X_\tau} &= \E{}{X_0}.
\end{align*} 
\end{theorem}


\item \begin{theorem}(\textbf{Doob's Optional Sampling Theorem}) \citep{billingsley2008probability, resnick2013probability}\\
Let $\set{(X_n, \srB_n), n \ge 0}$ be a \textbf{martingale} and let $S, T$ be $\set{\srB_n, n \ge 0}$-\textbf{stopping time} bounded by constant $c$, with 
$S \le T \le c, \;\text{a.s.}$, then 
\begin{align}
\E{}{X_T | \srB_{S}} = X_S, \quad \text{a.s.} \label{eqn: optional_sampling_theorem}
\end{align} where 
\begin{align*}
\srB_{S} := \set{A \in \srB: A\cap \set{S \le n} \in \srB_{n}, \quad n=0,1,\ldots }
\end{align*}
\end{theorem}



\item \begin{theorem}(\textbf{Wald's Identity}) \citep{billingsley2008probability, resnick2013probability}\\
Consider random walk $S_0 = X_0$ and $S_n = X_0 + \xi_1 + \xi_2 \xdotx{+} \xi_n$, $n = 1,2,\ldots$, where $\xi_1 \xdotx{,} \xi_n$ are i.i.d. random variables such that $\E{}{\abs{\xi_i}} < \infty$ and $\E{}{\xi_i} = \mu$. Let $\tau$ be a \textbf{stopping time} with respect to $\set{\sigma(S_0, S_1 \xdotx{,} S_n), n\ge 0}$ and $\E{}{\tau} < \infty$. Then 
\begin{align*}
\E{}{S_{\tau} - S_0} = \mu \E{}{\tau}
\end{align*}
\end{theorem}

\item \begin{proposition}(\textbf{Wald's Equation}) \citep{billingsley2008probability}\\
Let $X_1 \xdotx{,} X_n$ be i.i.d. random variables with $\E{}{\abs{X_i}} < \infty$ and $\E{}{X_i} = \mu <\infty$. Let $\tau$ be a \textbf{stopping time} with respect to $\set{\sigma\paren{X_1 \xdotx{,} X_n}, n \ge 1}$ satisfying $\E{}{\tau} < \infty$. Then
\begin{align}
\E{}{\sum_{i=1}^{\tau}X_i} &=\E{}{\tau} \E{}{X_i} \label{eqn: wald_equation}
\end{align}
\end{proposition}

\item \begin{proposition}) \citep{resnick2013probability}\\
Let $\set{(X_n, \srB_n), n \ge 0}$ be a \textbf{martingale} and let $\tau$ be a \textbf{stopping time} with respect to $\set{\srB_n, n \ge 0}$. Then 
\begin{align*}
\set{X_{\tau \land t}, \;\; t=0, 1, \ldots}
\end{align*} is a \textbf{martingale} with respect to $\set{\srB_n, n \ge 0}$, where $t\land \tau = \min\set{t, \tau}$.
\end{proposition}
\end{itemize}

\subsection{Martingale Inequalities}
\begin{itemize}
\item \begin{theorem}(\textbf{Doob's Maximal Inequality}) \citep{billingsley2008probability}\\
Suppose $\set{(X_n, \srB_n), n \ge 0}$ is a \textbf{sub-martingale}. Then for any $t >0$, 
\begin{align}
\bP\set{\max_{k \le n}X_k \ge t } \le \frac{1}{t}\E{}{X_n \ind{\max_{k \le n}X_k \ge t}} \label{ineqn: submartingale_maximal_inequality}
\end{align}
\end{theorem}

\item \begin{definition}(\textbf{\emph{Gambling Strategy}}) \\
A \underline{\emph{\textbf{gambling strategy}}} with respect to an increasing family of $\sigma$-algebras $\set{\srB_n, n \ge 0}$ is a sequence of random variables $\gamma_1, \gamma_2, \ldots$ such that $\gamma_n$ is \emph{\textbf{$\srB_{n-1}$-measurable}} for each $n =1, 2, \ldots$, where $\srB_0 = \set{\emptyset, \Omega}$.

The random variable sequence $(\gamma_1, \gamma_2, \ldots)$ is called \underline{\emph{\textbf{predictable}}}.
\end{definition}

\item \begin{definition}(\textbf{\emph{Upcrossing}}) \\
Consider a sequence of random variables $X_1 \xdotx{,} X_n, \ldots$ such that $X_n$ is $\srB_n$-measurable for all $n$ and two real numbers $a < b$. Define a gambling strategy $(\gamma_1, \gamma_2, \ldots)$ by setting:
\begin{align*}
\gamma_0 &= 0;\\
\gamma_{n+1} &= \left\{\begin{array}{cc}
1 & \text{ if }\gamma_n = 0 \text{ and }X_n <0\\
1 & \text{ if }\gamma_n = 1 \text{ and }X_n  \ge b\\
0 &\text{ otherwise}
\end{array}
\right.  \qquad \forall n=1, 2, \ldots.
\end{align*} Such strategy is called a \underline{\emph{\textbf{upcrossing strategy}}}.
\end{definition}

\item \begin{lemma}(\textbf{The Upcrossing Inequality Lemma}) \citep{billingsley2008probability} \\
Suppose $\set{(X_n, \srB_n), n \ge 0}$ is a \textbf{super-martingale} and $a < b$. Then 
\begin{align}
 \E{}{U_n[a, b]} &\le \frac{\E{}{(X_n - a)_{+}}}{b- a} = \frac{-\E{}{(X_n - a)_{-}}}{b- a} \label{ineqn: supermartingale_upcrossing_inequality}
\end{align} where $U_n[a, b]$ denote \textbf{the total number of upcrossings} of $[a, b]$ up to time $n$, 
\begin{align*}
(X_n - a)_{-} &= \min\set{X_n-a, 0} = - \max\set{a - X_n, 0}; \\
(X_n - a)_{+} &= \max\set{X_n-a, 0}.
\end{align*}
\end{lemma}

\item \begin{theorem} (\textbf{Bernstein Inequality, Martingale Difference Sequence Version}) \citep{wainwright2019high}\\
Let $\set{(D_k, \srB_k), k \ge 1}$ be a \textbf{martingale difference sequence}, and suppose that 
\begin{align*}
\E{}{\exp\paren{\lambda D_k} | \srB_{k-1}} \le \exp\paren{\frac{\lambda^2 \nu_k^2}{2} }
\end{align*} almost surely for any $\abs{\lambda} < 1/\alpha_k$. Then the following hold:
\begin{enumerate}
\item The sum $\sum_{k=1}^{n}D_k$ is \textbf{sub-exponential} with \textbf{parameters} $\paren{\sqrt{\sum_{k=1}^{n}\nu_k^2}\;  , \;\alpha_{*}}$ where $\alpha_{*} := \max_{k=1 \xdotx{,} n} \alpha_k$. That is, for any $\abs{\lambda} < 1/\alpha_{*}$, 
\begin{align*}
\E{}{\exp\set{\lambda \paren{\sum_{k=1}^{n}D_k}}} \le \exp\paren{\frac{\lambda^2\sum_{k=1}^{n}\nu_k^2}{2} }
\end{align*}
\item The sum satisfies \textbf{the concentration inequality}
\begin{align}
\bP\set{\abs{\sum_{k=1}^{n}D_k} \ge t } &\le \left\{ \begin{array}{cc}
2 \exp\paren{- \frac{t^2}{2 \sum_{k=1}^{n}\nu_k^2}} & \text{ if } 0 \le t \le \frac{\sum_{k=1}^{n}\nu_k^2}{\alpha_{*}} \\[15pt]
2 \exp\paren{- \frac{t}{\alpha_{*}}} &\text{ if } t > \frac{\sum_{k=1}^{n}\nu_k^2}{\alpha_{*}}.
\end{array}\right. \label{ineqn: bernstein_inequality_martingale}
\end{align}
\end{enumerate}
\end{theorem}

\item \begin{corollary} (\textbf{Azuma-Hoeffding Inequality, Martingale Difference})\citep{wainwright2019high}\\
Let $\set{(D_k, \srB_k), k \ge 1}$ be a \textbf{martingale difference sequence} for which there are constants $\set{(a_k, b_k)}^{n}_{k=1}$ such that $D_k \in [a_k, b_k]$ almost surely for all $k = 1 \xdotx{,} n$. Then, for all $t \ge 0$,
\begin{align}
\bP\set{\abs{\sum_{k=1}^{n}D_k} \ge t } &\le  2 \exp\paren{- \frac{2 t^2}{ \sum_{k=1}^{n}(b_k - a_k)^2}} \label{ineqn: hoeffding_inequality_martingale}
\end{align}
\end{corollary}

\item An important application of \emph{Azuma-Hoeffding Inequality} concerns functions that satisfy a \emph{bounded difference property}. 
\begin{definition} (\textbf{\emph{Functions with Bounded Difference Property}})\\
Given vectors $x, x' \in \cX^n$ and an index $k \in \set{1, 2 \xdotx{,} n}$, we define a new vector $x^{(-k)} \in \cX^n$ via
\begin{align*}
x_j^{(-k)} &= \left\{\begin{array}{cc}
x_j & j \neq k\\
x_k'& j = k
\end{array}
\right.
\end{align*}
With this notation, we say that $f: \cX^n \to \bR$ satisfies \underline{\textbf{\emph{the bounded difference inequality}}} with parameters $(L_1 \xdotx{,} L_n)$ if, for each index $k = 1, 2 \xdotx{,} n$,
\begin{align}
\abs{f(x) - f(x^{(-k)})} \le L_k, \quad\text{ for all }x, x' \in \cX^n. \label{eqn: bounded_difference_property}
\end{align}
\end{definition}

\item \begin{corollary} (\textbf{McDiarmid's Inequality / Bounded Differences Inequality})\citep{wainwright2019high}\\
Suppose that $f$ satisfies \textbf{the bounded difference property} \eqref{eqn: bounded_difference_property} with parameters $(L_1 \xdotx{,} L_n)$ and that the random vector $X = (X_1, X_2 \xdotx{,} X_n)$ has \textbf{independent} components. Then
\begin{align}
\bP\set{\abs{f(X) - \E{}{f(X)}} \ge t } &\le  2 \exp\paren{- \frac{2 t^2}{ \sum_{k=1}^{n}L_k^2}}. \label{ineqn: macdiarmid_bounded_difference_inequality}
\end{align}
\end{corollary}
\end{itemize}

\subsection{Convergence Theorem}
\begin{itemize}
\item \begin{theorem}(\textbf{Doob's Martingale Convergence Theorem}) \citep{billingsley2008probability}\\
Suppose $\set{(X_n, \srB_n), n \ge 0}$ is a \textbf{super-martingale} and suppose further that
\begin{align*}
\sup_{n}\E{}{\abs{X_n}} < \infty.
\end{align*} Then there exists an integrable random variable $X$ such that 
\begin{align*}
\lim\limits_{n \to \infty}X_n = X \quad \text{a.s.}
\end{align*}
\end{theorem}

\item \begin{remark}
Note that a martingale is a super-martingale, so the above theorem is \emph{\textbf{valid}} for \emph{\textbf{martingale}}. Also the above theorem is valid for \emph{\textbf{sub-martingale}} when $\set{(-X_n, \srB_n), n \ge 0}$ is considered.
\end{remark}
\end{itemize}


\newpage
\bibliographystyle{plainnat}
\bibliography{reference.bib}
\end{document}