\documentclass[11pt]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent %\usepackage{graphicx}
\usepackage{amsmath,amssymb, mathrsfs,  mathtools, dsfont}
\usepackage{tabularx}
\usepackage{tikz-cd}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{xcolor}
%\usepackage[linkbordercolor ={1 1 1} ]{hyperref}
%\usepackage[sf]{titlesec}
\usepackage{natbib}
%\usepackage{tikz-cd}

\usepackage{../../Tianpei_Report}

%\usepackage{appendix}
%\usepackage{algorithm}
%\usepackage{algorithmic}

%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}



\begin{document}
\title{Summary Part 2: Concentration of Measure and Functional Methods}
\author{ Tianpei Xie}
\date{Jan. 26th., 2023 }
\maketitle
\tableofcontents
\newpage
\section{Logarithmic Sobolev Inequality}
\subsection{Functional Form of Logarithmic Sobolev Inequality}
\begin{itemize}
\item From functional analysis, we have \emph{the Sobolev inequality}, 
\begin{remark} (\emph{\textbf{The Sobolev Inequality}}) \citep{evans2010partial}\\
\emph{\textbf{The Sobolev inequality}} states for smooth function $f: \bR^n \to \bR$ in \emph{Sobolev space} where $n \ge 3$ and $p = \frac{2n}{n-2} > 2$
\begin{align*}
\norm{f}{p}^2 &\le C_n \,\int_{\bR^{n}}\abs{\nabla f}^2 dx.
\end{align*} The inequality is sharp when the constant
\begin{align*}
C_n &:= \frac{1}{\pi n(n-2)}\paren{\frac{\Gamma(n)}{\Gamma(n/2)}}^{2/n}
\end{align*}
\end{remark}

\item \begin{proposition} (\textbf{Euclidean Logarithmic Sobolev Inequality}). \\
Let $f: \bR^n \to \bR$ be a smooth function and $m$ be Lebesgue measure on $\bR^n$, then
\begin{align}
\text{Ent}_{m}(f^2) &\le \frac{n}{2}\log\paren{\frac{2}{n \pi e} \E{m}{\norm{\nabla f}{2}^2} } \label{ineqn: log_sobolev_inequality_euclidean} \\
\Leftrightarrow \int f^2 \log\paren{\frac{f^2}{\int f^2 dx}} dx &\le \frac{n}{2}\log\paren{\frac{2}{n \pi e} \int \abs{\nabla f}^2 dx }
\nonumber
\end{align}
\end{proposition}

\item \begin{definition} (\textbf{\emph{Logarithmic Sobolev Inequality for General Probability Measure}}). \\
\emph{A probability measure} $\mu$ on $\bR^n$ is said to satisfy the \underline{\emph{\textbf{logarithmic Sobolev inequality}}}  for some constant $C > 0$ if for any smooth function $f$
\begin{align}
\text{Ent}_{\mu}(f^2) &\le C\, \E{\mu}{\norm{\nabla f}{2}^2} \label{ineqn: log_sobolev_inequality_general}
\end{align} holds for any \textbf{\emph{continuous differentiable}} function $f: \bR^n \to \bR$.  The left-hand side is called \textbf{\emph{the entropy functional}}, which is defined as
\begin{align*}
\text{Ent}(f^2) &:= \E{\mu}{f^2 \log f^2} - \E{\mu}{f^2}\log\E{\mu}{f^2} \\
&= \int f^2 \log\paren{\frac{f^2}{\int f^2 d\mu}} d\mu.
\end{align*} The right-hand side is defined as
\begin{align*}
\E{\mu}{\norm{\nabla f}{2}^2} &= \int \norm{\nabla f}{2}^2 d\mu.
\end{align*} Thus we can rewrite \emph{the logarithmic Sobolev inequality} in \emph{functional form}
\begin{align}
\int f^2 \log\paren{\frac{f^2}{\int f^2 d\mu}} d\mu &\le C \int \norm{\nabla f}{2}^2 d\mu  \label{ineqn: log_sobolev_inequality_general_functional_form} 
\end{align}
\end{definition}

\item \begin{remark}(\textbf{\emph{Logarithmic Sobolev Inequality}})\\
For non-negative function $f$, we can replace $f \to \sqrt{f}$, so that \emph{the logarithmic Sobolev inequality} becomes
\begin{align}
\text{Ent}_{\mu}(f) &\le C \int \frac{\norm{\nabla f}{2}^2}{f} d\mu \label{ineqn: log_sobolev_inequality_general_v2} 
\end{align} 
\end{remark}

\item \begin{remark}(\textbf{\emph{Modified Logarithmic Sobolev Inequality via Convex Cost and Duality}})\\
For some \emph{\textbf{convex non-negative cost}} $c: \bR^n \to \bR_{+}$, \emph{\textbf{the convex conjugate}} of $c$ (Legendre transform of $c$) is defined as
\begin{align*}
c^{*}(x) := \sup_{y}\set{\inn{x}{y} - c(y) }
\end{align*}
Then we can obtain \emph{\textbf{the modified logarithmic Sobolev inequality}}
\begin{align}
\text{Ent}_{\mu}(f) &\le \int f^2\, c^{*}\paren{\frac{\nabla f}{f}} d\mu \label{ineqn: log_sobolev_inequality_general_modified} 
\end{align} 
\end{remark}
\end{itemize}
\subsection{Bernoulli Logarithmic Sobolev Inequality}
\begin{itemize}
\item \begin{remark} (\emph{Setting})\\
Consider a \emph{\textbf{uniformly distributed binary vector}} $Z = (Z_1 \xdotx{,} Z_n)$ on the hypercube $\set{-1, +1}^n$. In other words, the components of $X$ are \emph{independent}, \emph{identically distributed} \emph{\textbf{random sign (Rademacher) variables}} with $\bP\set{Z_i = -1} = \bP\set{Z_i = +1} = 1/2$ (i.e. \emph{symmetric Bernoulli random variables}). 

Let $f: \set{-1, +1}^n \to \bR$ be a real-valued function on \emph{\textbf{binary hypercube}}. $X := f(Z)$ is an induced real-valued random variable. Define $\widetilde{Z}^{(i)} = (Z_1 \xdotx{,} Z_{i-1}, Z_{i}', Z_{i+1} \xdotx{,} Z_n)$ be the sample $Z$ with $i$-th component replaced by an \emph{independent copy} $Z_{i}'$. Since $Z, \widetilde{Z}^{(i)} \in \set{-1, +1}^n$, $\widetilde{Z}^{(i)} = (Z_1 \xdotx{,} Z_{i-1}, -Z_{i}, Z_{i+1} \xdotx{,} Z_n)$, i.e. \emph{the $i$-th sign is \textbf{flipped}}. Also denote the $i$-th \emph{Jackknife sample} as $Z_{(i)} = (Z_1 \xdotx{,} Z_{i-1},  Z_{i+1} \xdotx{,} Z_n)$ by \emph{leaving out} the $i$-th component. $\E{(-i)}{X} := \E{}{X | Z_{(i)}}$.

Denote the $i$-th component of \emph{\textbf{discrete gradient}} of $f$ as
\begin{align*}
\nabla_{i}f(z) &:= \frac{1}{2}\paren{f(z) - f(\widetilde{z}^{(i)})}
\end{align*} and $\nabla f(z) = (\nabla_{1}f(z) \xdotx{,} \nabla_{n}f(z) )$
\end{remark}

\item \begin{proposition} (\textbf{Logarithmic Sobolev Inequality for Rademacher Random Variables}). \citep{boucheron2013concentration}\\
If $f: \set{-1, +1}^n \to \bR$ be an arbitrary real-valued function  defined on the $n$-dimensional \textbf{binary hypercube} and assume that $Z$ is \textbf{uniformly} \textbf{distributed} over $\set{-1, +1}^n$. Then
\begin{align}
\text{Ent}(f^2) &\le \cE(f) \label{ineqn: log_sobolev_inequality_binary_cube} \\
\Leftrightarrow \text{Ent}(f^2(Z)) &\le 2\E{}{\norm{\nabla f(Z)}{2}^2}  \label{ineqn: log_sobolev_inequality_binary_cube_2}
\end{align}
\end{proposition}

\item \begin{remark} (\emph{\textbf{Logarithmic Sobolev Inequality $\Rightarrow$  Efron-Stein Inequality}}). \citep{boucheron2013concentration}\\
Note that for $f$ non-negative, 
\begin{align*}
\text{Var}(f(Z)) &\le \text{Ent}(f^2(Z)).
\end{align*} Thus \emph{logarithmic Sobolev inequality} \eqref{ineqn: log_sobolev_inequality_binary_cube} implies
\begin{align*}
\text{Var}(f(Z)) &\le  \cE(f) 
\end{align*} which is \emph{the Efron-Stein inequality}.
\end{remark}

\item \begin{corollary} (\textbf{Logarithmic Sobolev Inequality for Asymmetric Bernoulli Random Variables}). \citep{boucheron2013concentration}\\
If $f: \set{-1, +1}^n \to \bR$ be an arbitrary real-valued function and $Z = (Z_1 \xdotx{,} Z_n) \in \set{-1, +1}^n$ with $p= \bP\set{Z_i = +1}$. Then
\begin{align}
\text{Ent}(f^2) &\le c(p)\E{}{\norm{\nabla f(Z)}{2}^2}   \label{ineqn: log_sobolev_inequality_binary_cube_asym}
\end{align} where 
\begin{align*}
c(p) &= \frac{1}{1 - 2p}\log\frac{1-p}{p}
\end{align*} Note that $\lim\limits_{p \to 1/2}c(p) = 2$.
\end{corollary}
\end{itemize}
\subsection{Gaussian Logarithmic Sobolev Inequality}
\begin{itemize}
\item \begin{proposition} (\textbf{Gaussian Logarithmic Sobolev Inequality}). \citep{boucheron2013concentration}\\
Let $f: \bR^n \to \bR$ be a \textbf{continuous differentiable} function and let $Z = (Z_1 \xdotx{,} Z_n)$ be a vector of $n$ \textbf{independent} \textbf{standard Gaussian} random variables. Then
\begin{align}
\text{Ent}(f^2(Z)) &\le 2\E{}{\norm{\nabla f(Z)}{2}^2}.  \label{ineqn: log_sobolev_inequality_gaussian}
\end{align}
\end{proposition}
\end{itemize}
\subsection{Modified Logarithmic Sobolev Inequalities}
\begin{itemize}
\item \begin{proposition} (\textbf{A Modified Logarithmic Sobolev Inequalities for Moment Generating Function}) \citep{boucheron2013concentration}\\
Consider independent random variables $Z_1 \xdotx{,} Z_n$ taking values in $\cX$, a real-valued function $f: \cX^n \to \bR$ and the random variable $X = f(Z_1 \xdotx{,} Z_n)$. Also denote $Z_{(-i)}= (Z_1 \xdotx{,} Z_{i-1}, Z_{i+1} \xdotx{,} Z_n)$ and $X_{(-i)} = f_i(Z_{(-i)})$ where $f_i: \cX^{n-1} \to \bR$ is an arbitrary function. Let $\phi(x) = e^x -x -1$.
Then for all $\lambda \in \bR$,
\begin{align}
\text{Ent}(e^{\lambda X}) := \E{}{\lambda Xe^{\lambda X}} - \E{}{e^{\lambda X}}\log\E{}{e^{\lambda X}} &\le \sum_{i=1}^{n}\E{}{e^{\lambda X}\phi(-\lambda(X - X_{(-i)}))}\label{ineqn: log_sobolev_inequality_mgf}
\end{align}
\end{proposition}


\item \begin{proposition} (\textbf{Symmetrized Modified Logarithmic Sobolev Inequalities}) \citep{boucheron2013concentration}\\
Consider independent random variables $Z_1 \xdotx{,} Z_n$ taking values in $\cX$, a real-valued function $f: \cX^n \to \bR$ and the random variable $X = f(Z_1 \xdotx{,} Z_n)$. Also denote $\widetilde{X}^{(i)} = f(Z_1 \xdotx{,} Z_{i-1}, Z_i', Z_{i+1} \xdotx{,} Z_n)$. Let $\phi(x) = e^x -x -1$.
Then for all $\lambda \in \bR$,
\begin{align}
\lambda \E{}{Xe^{\lambda X}} - \E{}{e^{\lambda X}}\log\E{}{e^{\lambda X}} &\le \sum_{i=1}^{n}\E{}{e^{\lambda X}\phi(-\lambda(X - \widetilde{X}^{(i)}))}\label{ineqn: log_sobolev_inequality_sym_mgf}
\end{align} Moreover, denoting $\tau(x) = x(e^x - 1)$, for all $\lambda \in \bR$,
\begin{align*}
\lambda \E{}{Xe^{\lambda X}} - \E{}{e^{\lambda X}}\log\E{}{e^{\lambda X}} &\le \sum_{i=1}^{n}\E{}{e^{\lambda X}\tau(-\lambda(X - \widetilde{X}^{(i)})_{+})},\\
\lambda \E{}{Xe^{\lambda X}} - \E{}{e^{\lambda X}}\log\E{}{e^{\lambda X}} &\le \sum_{i=1}^{n}\E{}{e^{\lambda X}\tau(\lambda(\widetilde{X}^{(i)} - X)_{-})}.
\end{align*}
\end{proposition}
\end{itemize}

\section{Isoperimetric Inequalities and Concentration of Measure}
\subsection{Brunn-Minkowski Inequality}
\begin{itemize}
\item \begin{definition} (\textbf{\emph{Minkowski Sum of Sets}})\\
Consider sets $A, B \subseteq \bR^n$ and define \underline{\emph{\textbf{the Minkowski sum}}} of $A$ and $B$ as the set of all vectors in $\bR^n$ formed by sums of elements of $A$ and $B$:
\begin{align*}
A + B &:= \set{x+y: x \in A, y \in B}
\end{align*} 
Similarly, for $c \in \bR$, let $c A = \set{cx : x \in A}$. Denote by $\text{Vol}(A)$ the \emph{\textbf{Lebesgue measure}} of a \emph{(measurable) set $A \subset \bR^n$}.
\end{definition}

\item \begin{theorem} (\textbf{The Pr{\'e}kopa-Leindler Inequality}). \citep{boucheron2013concentration, wainwright2019high} \\
Let $\lambda \in (0, 1)$, and let $f, g, h : \bR^n \to [0, \infty)$ be \textbf{non-negative measurable functions} such that for all $x, y \in \bR^n$,
\begin{align*}
h\paren{\lambda x + (1- \lambda) y} &\ge f(x)^{\lambda}g(y)^{1-\lambda}.
\end{align*} Then
\begin{align}
\int_{\bR^n} h(x) dx &\ge \paren{\int_{\bR^n} f(x) dx }^{\lambda}\paren{\int_{\bR^n} g(x) dx}^{1-\lambda}.   \label{ineqn: prekopa_leindler_inequality}
\end{align}
\end{theorem}

\item \begin{corollary} (\textbf{Weaker Brunn-Minkowski Inequality}) \citep{boucheron2013concentration, wainwright2019high}\\
Let $A, B \subset \bR^n$ be \textbf{non-empty compact sets}. Then for all $\lambda \in [0, 1]$,
\begin{align}
\text{Vol}\paren{ \lambda A + (1- \lambda) B } &\ge \text{Vol}(A)^{\lambda}\text{Vol}(B)^{1- \lambda}.   \label{ineqn: brunn_minkowski_inequality_weaker}
\end{align}
\end{corollary}

\item \begin{theorem} (\textbf{Brunn-Minkowski Inequality}) \citep{boucheron2013concentration, vershynin2018high, wainwright2019high}\\
Let $A, B \subset \bR^n$ be \textbf{non-empty compact sets}. Then for all $\lambda \in [0, 1]$,
\begin{align}
\text{Vol}\paren{ \lambda A + (1- \lambda) B }^{\frac{1}{n}} &\ge \lambda\text{Vol}(A)^{\frac{1}{n}} + (1- \lambda)\text{Vol}(B)^{\frac{1}{n}}.   \label{ineqn: brunn_minkowski_inequality}
\end{align}
\end{theorem}
\end{itemize}
\subsection{Classical Isoperimetric Problem on Euclidean Space $\bR^n$}
\begin{itemize}
\item \begin{definition} (\textbf{\emph{Blowup of Sets}}) \\
For any $t > 0$, and any (measurable) sets $A \subset \bR^n$,  \emph{\underline{\textbf{the $t$-blowup (or, $t$-enlargement)}} of $A$} is defined by
\begin{align*}
A_t &:= \set{x \in \bR^n: d(x, A) < t} = A + t\,B
\end{align*} where $B = \set{x \in \bR^n: d(0, x) < 1}$ is an \emph{open unit ball} and $d(x, A) = \inf_{y \in A}d(x, y)$.
\end{definition}

\item \begin{definition}(\textbf{\emph{Surface Area of Sets}}) \\
let $A \subset \bR^n$ be a measurable set and denote by $\text{Vol}(A)$ its \emph{Lebesgue measure}. \emph{The  \underline{\textbf{surface area}} of $A$} is
defined by
\begin{align*}
\text{Vol}(\partial A) &= \lim\limits_{t \to 0}\frac{\text{Vol}(A_t) - \text{Vol}(A)}{t}.
\end{align*} provided that the limit exists. Here $A_t$ denotes \emph{the $t$-blowup} of $A$.
\end{definition}

\item \begin{remark}(\textbf{\emph{Isoperimetry Theorem}})\\
The classical isoperimetric theorem in $\bR^n$ states that, among all sets with \emph{\textbf{a given volume}}, \underline{\emph{\textbf{the Euclidean unit ball minimizes the surface area}}}.  This theoerm can be formally stated as below:
\end{remark}

\item \begin{theorem} (\textbf{Isoperimetry Theorem}) \citep{boucheron2013concentration, vershynin2018high, wainwright2019high}\\
Let $A \subset \bR^n$ be such that $\text{Vol}(A) = \text{Vol}(B)$ where $B := \set{x \in \bR^n: d(0, x) < 1}$ is an unit ball. Then for any $t > 0$, 
\begin{align}
\text{Vol}(A_t) &\ge  \text{Vol}(B_t) \label{ineqn: isoperimetry_inequality_blowup}
\end{align} Moreover, if $\text{Vol}(\partial A) $ exists, then
\begin{align}
\text{Vol}(\partial A)  &\ge \text{Vol}(\partial B).  \label{ineqn: isoperimetry_inequality_surface}
\end{align}
\end{theorem}

\item \begin{example} (\textbf{\emph{Concentration of Lebesgue Measure in $\bR^n$ and Isoperimetric Inequality}})\\
Note that the volume of a \emph{$t$-ball in $\bR^n$} is 
\begin{align*}
\text{Vol}(tB) =  \frac{\pi^{\frac{n}{2}}}{\Gamma\paren{\frac{n}{2} + 1}}t^n  \equiv c_n t^n 
\end{align*}  Thus the radius of ball $B$ with the same volume of $A$ is
\begin{align*}
r := \paren{\frac{\text{Vol}(A) }{c_n}}^{\frac{1}{n}}.
\end{align*} \emph{\textbf{The classical isoperimetric inequality}} states that 
\begin{align}
 \text{Vol}(A_t)) &\ge \paren{ (r+  t)\text{Vol}(B)^{1/n}}^{n} \nonumber\\
 \Leftrightarrow  \text{Vol}(A_t) &\ge c_n\paren{\paren{\frac{\text{Vol}(A) }{c_n}}^{\frac{1}{n}} + t}^n \nonumber \\
  \Leftrightarrow  \paren{\frac{\text{Vol}(A_t) }{c_n}}^{\frac{1}{n}}  &\ge \paren{\frac{\text{Vol}(A) }{c_n}}^{\frac{1}{n}} + t  \label{ineqn: lebesgure_measure_concentration}
\end{align}
\end{example}

\item \begin{definition} (\emph{\textbf{Isoperimetric Function of Probability Measure}}) \\
Define  \emph{\textbf{the isoperimetric function}} of \emph{the Lebesgue measure space} $(\bR^n, \mu)$ as
\begin{align*}
\lambda(u) := \paren{\frac{u}{c_n}}^{\frac{1}{n}}
\end{align*} so \emph{the classical isoperimetric inequality}  is equivalent to \emph{the concentration of Lebesgue measure}
\begin{align*}
\lambda\paren{\mu(A_t)} &\ge \lambda\paren{\mu(A)} + t. 
\end{align*}
\end{definition}
\end{itemize}

\subsection{Isoperimetric Problem on Unit Sphere}
\begin{itemize}
\item \begin{definition} (\textbf{\emph{Spherical Cap and its $t$-Blowup}}) \\
Let $\bS^{n - 1} :=  \set{x \in \bR^n: \norm{x}{} = 1}$ be the \emph{$(n-1)$-dimensional \textbf{unit sphere}}. The \emph{\textbf{intersection}} of a \emph{\textbf{half-space}} and $\bS^{n-1}$ is called a \emph{\textbf{spherical cap}}. In particular, for some $y \in \bR^n$, denote the associated spherical cap as
\begin{align*}
H_y := \set{x \in \bS^{n-1}: \inn{x}{y} \le 0}
\end{align*} With some simple geometry, it can be shown that its  \emph{$t$-blowup}  corresponds to the set
\begin{align*}
H_y^{t} := \set{x \in \bS^{n-1}: \inn{x}{y} < \sin(t)}
\end{align*}
\end{definition}

\item \begin{theorem} (\textbf{Isoperimetry Theorem on Unit Sphere}) \citep{boucheron2013concentration, vershynin2018high, wainwright2019high}\\
Let $A$ be a subset of the sphere $\bS^{n-1}$, and let $\sigma$ denote the \textbf{normalized area} on that sphere. Let $t > 0$. Then,
among all sets $A \subset \bS^{n-1}$ with given area $\sigma(A)$, the \underline{\textbf{spherical caps}} \textbf{minimize}
\textbf{the area of the neighborhood} $\sigma(A_t)$, where
\begin{align*}
A_t := \set{x \in \bS^{n-1}: \exists y \in A \text{ such that } \norm{x - y}{} < t}
\end{align*}
\end{theorem}

\item \begin{remark}
Define a \emph{metric} $\rho$ on sphere $\bS^{n-1}$ as 
\begin{align*}
\rho(x, y) := \arccos(\inn{x}{y})
\end{align*}
Thus $(\bS^{n-1}, \rho)$ is a \emph{\textbf{metric space}}.  Let $\bP$ be uniform distribution on $\bS^{n-1}$ so that $((\bS^{n-1}, \rho), \bP)$ is a probability space. 
\end{remark}

\item \begin{proposition} (\textbf{Isoperimetric Inequalities for Uniform Distribution over Sphere})  \citep{boucheron2013concentration, vershynin2018high, wainwright2019high}\\
Let $\bS^{n - 1} :=  \set{x \in \bR^n: \norm{x}{} = 1}$ be the $(n-1)$-dimensional \textbf{unit sphere}.  For any $t\in [0, 1]$, 
\begin{align}
\alpha_{\bS^{n-1}}(t) &\le c \exp\paren{-\frac{n t^2}{2}} \label{ineqn: isoperimetric_inequality_uniform_unit_sphere}
\end{align} for some constant $c$.
\end{proposition}

\item By Levy's inequality, we have the following proposition
\begin{proposition} (\textbf{Lipschitz Function on $\bS^{n-1}$}) \citep{wainwright2019high}\\
For any $1$-Lipschitz function $f$ defined on the sphere $\bS^{n-1}$, we have the two-sided bound
\begin{align}
\bP\set{\abs{f(Z) - \text{Med}(f(Z))} \ge t} &\le \sqrt{2\pi}\exp\paren{-\frac{n t^2}{2}} \label{ineqn: lipschitz_unit_sphere_median}
\end{align} Moreover, replacing median by the mean, we have 
\begin{align}
\bP\set{\abs{f(Z) - \E{}{f(Z)} } \ge t} &\le 2\sqrt{2\pi}\exp\paren{-\frac{n t^2}{8}} \label{ineqn: lipschitz_unit_sphere_mean}
\end{align}
\end{proposition}

\item \begin{exercise} \textbf{(The Blow-Up Phenomenon)}\\
Let $A$ be a subset of the sphere $\sqrt{n} \bS^{n-1}$ such that
\begin{align*}
\bP\paren{A} > 2 \exp\paren{-c s^2} \text{ for some }s > 0;
\end{align*}
\begin{enumerate}
\item Prove that $\bP(A_s) > 1/2$.
\item Deduce from this that for any $t \ge s$,
\begin{align*}
\bP\paren{A_{2t}} > 1 - 2 \exp\paren{-c t^2}.
\end{align*} Here $c > 0$ is the absolute constant in upper bound of concentration function.
\end{enumerate}
\end{exercise}
\end{itemize}
\subsection{Concentration via Isoperimetric Inequalities}
\begin{itemize}
\item \begin{definition} (\emph{\textbf{Isoperimetry Problem}}) \citep{boucheron2013concentration}\\
Given a \emph{\textbf{metric space}} $\cX$ with corresponding \emph{distance} $d$, consider \emph{\textbf{the measure space}} formed by $\cX$ , \emph{the $\sigma$-algebra} of all \emph{\textbf{Borel sets} of $\cX$}, and a probability measure $\bP$. Let $X$ be a \emph{random variable} taking values in $\cX$, distributed according to $\bP$. 

\underline{\emph{\textbf{The isoperimetric problem}}} in this case is the following: given $p \in (0, 1)$ and $t > 0$, \emph{\textbf{determine the sets}} $A$ with $\bP\brac{X \in A} \ge p$ for which \emph{the measure}
\begin{align*}
\bP\brac{d(X, A) \ge t}
\end{align*}  is \emph{\textbf{maximal}}. 
\end{definition}

\item \begin{remark} (\emph{\textbf{Isoperimetric Inequalities}})\\
Even though the exact solution is only known in a few special cases, useful \emph{bounds} for $\bP\brac{d(X, A) \ge t}$ can be derived under remarkably general circumstances. \emph{Such bounds are usually referred to as} \underline{\emph{\textbf{isoperimetric inequalities}}}.
\end{remark}

\item \begin{definition} (\textbf{\emph{Concentration Function}}) \citep{boucheron2013concentration, wainwright2019high}\\
\underline{\emph{\textbf{The concentration function}}} $\alpha: [0, \infty) \to \bR_{+}$  associated with \emph{\textbf{metric measure space}} $((\cX, d), \bP)$ is given by
\begin{align*}
\alpha_{\bP, (\cX, d)}(t) &:= \sup_{ A \subset \cX:\, \bP\paren{A} \ge \frac{1}{2}}\bP\brac{d(X, A) \ge t } = \sup_{A \subset \cX: \,\bP\paren{A} \ge \frac{1}{2}}\bP\paren{A_{t}^{c} }
\end{align*} where $A_t:= A + tB = \set{x \in \cX: d(x, A) < t}$ is \emph{the $t$-blowup} of $A \subset \cX$. We simply denote it as $\alpha(t)$.
\end{definition}
Thus the optimal $A^{*}$ for isoperimetry problem is the one that attains the $\alpha(t) = \bP\paren{A_{t}^{c} }$.


\item \begin{theorem} (\textbf{Levy's Inequalities})\citep{boucheron2013concentration, wainwright2019high}\\
For any Lipschitz function $f: \cX \to \bR$, 
\begin{align}
\bP\set{f(X) \ge  \text{Med}(f(X)) + t } &\le \alpha_{\bP}(t) \label{ineqn: levy_inequality}\\
\bP\set{f(X)  \le  \text{Med}(f(X))  -  t } &\le \alpha_{\bP}(t).  \nonumber
\end{align} where $\text{Med}(f(X))$ is \underline{\textbf{the median} of $f(X)$}, i.e.
\begin{align*}
\bP\set{f(X) \le \text{Med}(f(X)} \ge  \frac{1}{2}, \;\text{ and }\; \bP\set{f(X) \ge \text{Med}(f(X)} \ge  \frac{1}{2}.
\end{align*} \textbf{Conversely}, if $\beta : \bR_{+} \to [0, 1]$ is a function such that for \textbf{every Lipschitz function} $f : \cX \to \bR$
\begin{align}
\bP\set{f(X) - \text{Med}(f(X)) \ge t } &\le \beta(t). \label{ineqn: converse_levy_inequality}
\end{align} \textbf{then} $\beta(t) \ge \alpha_{\bP}(t)$.
\end{theorem}



\item \begin{corollary} (\textbf{Concentration of  Measure on Hamming Metric Space}) \citep{boucheron2013concentration}\\
Consider \emph{independent random variables} $Z_1 \xdotx{,} Z_n$ taking their values in a \emph{(measurable) set} $\cX$ and denote the vector of these variables by $Z = (Z_1 \xdotx{,} Z_n)$ taking its value in $\cX^n$.  For an arbitrary (measurable) set $A \subset \cX^n$, we write $\bP\paren{A} = \bP\paren{Z \in A}$.  The \textbf{Hamming distance} $d_{H}(x, y)$ between the vectors $x, y \in \cX^n$ is defined as \textbf{the number of coordinates} in which $x$ and $y$ \textbf{differ}. Then for any $t >0$, 
\begin{align}
\bP\set{d_{H}(x, A) \ge \sqrt{\frac{n}{2}\log \frac{1}{\bP(A)}} + t} \le \exp\paren{-\frac{2t^2}{n}}  \label{ineqn: isoperimetry_inequality_hamming_distance}
\end{align}
\end{corollary}

\item \begin{remark} (\textbf{\emph{Equivalent Form}})\\
From above isoperimetric inequality, 
\begin{align*}
\bP\set{d_{H}(x, A) \ge \sqrt{\frac{n}{2}\log \frac{1}{\bP(A)}} + t} \le \exp\paren{-\frac{2t^2}{n}}
\end{align*} Denote $u := \sqrt{\frac{n}{2}\log \frac{1}{\bP(A)}}$. By change of variable, for any $t \ge u$, 
\begin{align*}
\bP\set{d_{H}(x, A) \ge t} \le \exp\paren{-\frac{2\paren{t - u}^2}{n}}.
\end{align*} On the one hand, if $t \le 2u = \sqrt{-2 n \log \bP\paren{A}}$, then $\bP\paren{A} \le \exp(-t^2/(2n))$. On the other hand,
since $(t - u)^2 \ge t^2/4$ for $t \ge 2u =  \sqrt{-2 n \log \bP\paren{A}}$. the inequality above implies
$\bP\set{d_{H}(x, A) \ge t} \le  \exp(-t^2/(2n))$.
Thus, for all $t > 0$, we have \textbf{\emph{the concentration of measure in Hamming metric space}}:
\begin{align}
\bP(A)\bP\set{d_{H}(x, A) \ge t} \le \min\set{\bP(A), \bP\set{d_{H}(x, A) \ge t}} \le  \exp\paren{-\frac{t^2}{2n}} \label{ineqn: concentration_measure_hamming_distance}
\end{align}
\end{remark}

\item \begin{proposition} (\textbf{Levy's Inequalities for Mean})\citep{boucheron2013concentration, wainwright2019high}\\
If $\beta: \bR_{+} \to [0,1]$ is a  function such that for \textbf{every Lipschitz function} $f : \cX \to \bR$
\begin{align}
\bP\set{f(X) - \E{}{f(X)} \ge t } &\le \beta(t). \label{ineqn: converse_levy_inequality_mean}
\end{align} then $\beta(t) \ge \alpha_{\bP}(t/2)$.
\end{proposition}
\end{itemize}
\subsection{Convex Distance Inequality}
\begin{itemize}
\item \begin{definition} (\textbf{\emph{Weighted Hamming Distance}}) \\
Given $\alpha = (\alpha_1 \xdotx{,} \alpha_n)$, where $\alpha_i  \ge 0$, \emph{\textbf{the weighed Hamming distance}} between $x , y \in \cX^n$ is defined as
\begin{align*}
d_{\alpha}(x, y) &= \sum_{i=1}^{n}\alpha_i\ind{x_i \neq y_i}.
\end{align*}
\end{definition}


\item \begin{definition}(\textbf{\emph{Convex Distance}})\\
For any $x = (x_1 \xdotx{,} x_n)  \in \cX^n$, \underline{\emph{\textbf{the convex distance}}} of $x$ from the set $A$ by
\begin{align*}
d_{T}(x, A) &:=\sup_{\alpha \in \bR_{+}^{n}: \norm{\alpha}{2} = 1}d_{\alpha}(x, A)
\end{align*}

\end{definition}

\item \begin{theorem} (\textbf{Convex Distance Inequality}) \citep{boucheron2013concentration}\\
For any subset $A \subset \cX^n$ and $t > 0$,
\begin{align}
\bP(A)\bP\set{d_{T}(X, A)  \ge t}  \le  \exp\paren{-\frac{t^2}{4}}. \label{ineqn: convex_dist_inequality} \\
\Leftrightarrow  \bP(A)\bP\set{\sup_{\alpha \in \bR_{+}^{n}: \norm{\alpha}{2} = 1}\inf_{y\in A} \sum_{i=1}^{n}\alpha_i\ind{x_i \neq y_i} \ge t} \le  \exp\paren{-\frac{t^2}{4}}. \nonumber
\end{align}
\end{theorem}
\end{itemize}
\subsection{Concentration of Convex Lipschitz Functions}
\begin{itemize}
\item \begin{theorem} (\textbf{Concentration of Separately Convex Lipschitz Functions}) \citep{boucheron2013concentration}\\
Let  $Z:= (Z_1 \xdotx{,} Z_{n})$ be independent random variables taking values in the interval $[0, 1]$ and let $f : [0, 1]^n \to \bR$ be a \textbf{separately convex function} (i.e. $f$ is convex in each coordinate while the others are fixed) such that
\begin{align*}
\abs{f(x) - f(y)} &\le \norm{x - y}{} \quad \text{for all }x, y \in [0, 1]^n.
\end{align*}
Then $X = f(Z_1 \xdotx{,} Z_{n})$ satisfies, for all $t > 0$,
\begin{align}
\bP\set{f(Z) - \E{}{f(Z)} \ge t } &\le \exp\paren{- \frac{t^2}{2}}. \label{ineqn: convex_lipschitz_concentration}
\end{align}
\end{theorem}

\item \begin{remark}
For $Z_i \in [a_i, b_i]$, and $f$ being $L$-Lipschitz function, we have
\begin{align*}
\bP\set{f(Z) - \E{}{f(Z)} \ge t } &\le \exp\paren{- \frac{t^2}{2L^2\sum_{i=1}^n(b_i - a_i)^2}}
\end{align*}
\end{remark}

\item With convex distance inequality, we can improve \emph{the  concentration bound} for \emph{convex Lipschitz functions}. First, we relate convex distance with the minimal distance to convex set
\begin{lemma} \label{lem: convex_dist_bound_dist_convex_set} (\textbf{Convex Distance vs. Distance to Convex Set}) \citep{boucheron2013concentration}\\
Let $A \subset [0, 1]^n$ be a \textbf{convex set} and let $x = (x_1 \xdotx{,} x_n)  \in [0,1]^n$. Then
\begin{align}
 d(x, A):= \inf_{y \in A}\norm{x - y}{2} \le d_{T}(x, A).  \label{ineqn: convex_distance_bound_for_dist_convex_set}
\end{align}
\end{lemma}

\item \begin{theorem} (\textbf{Concentration of Convex Lipschitz Functions}) \citep{boucheron2013concentration}\\
Let  $Z:= (Z_1 \xdotx{,} Z_{n})$ be independent random variables taking values in the interval $[0, 1]$ and let $f : [0, 1]^n \to \bR$ be a \textbf{quasi-convex function}; that is
\begin{align*}
\set{z: f(z) \le s} \text{ is convex set for all }s \in \bR. 
\end{align*} Moreover, $f$ is Lipschitz function satisfying
\begin{align*}
\abs{f(x) - f(y)} &\le \norm{x - y}{} \quad \text{for all }x, y \in [0, 1]^n.
\end{align*}
Then $X = f(Z_1 \xdotx{,} Z_{n})$ satisfies, for all $t > 0$,
\begin{align}
\bP\set{f(Z)  \ge  \text{Med}(f(Z)) + t } &\le 2\exp\paren{- \frac{t^2}{4}}, \label{ineqn: convex_lipschitz_concentration_med} \\
\bP\set{f(Z)  \le \text{Med}(f(Z)) -t } &\le 2\exp\paren{- \frac{t^2}{4}}. \nonumber
\end{align}
\end{theorem}



\item \begin{remark}
This result can be generalized to \emph{\textbf{convex Lipschitz function} of \textbf{sub-Gaussian random variables}}. Note that in above theorems, $Z_i \in [0, 1]$ is \emph{\textbf{sub-Gaussian}} as well. 
\end{remark}
\end{itemize}

\section{Concentration of Gaussian Measure}
\subsection{Gaussian Isoperimetric Theorem and Gaussian Concentration Theorem}
\begin{itemize}
\item \begin{remark} (\textbf{\emph{Gaussian Isoperimetric Problem}})\\
\underline{\emph{\textbf{The Gaussian isoperimetric problem}}} is to determine which (Borel) sets $A$ have \emph{\textbf{minimal Gaussian boundary measure}} among all sets in $\bR^n$ with a \emph{given probability} $p$. 

\underline{\emph{\textbf{The Gaussian isoperimetric theorem}}} states the beautiful fact that \underline{\emph{\textbf{the extremal sets}}} are \underline{\emph{\textbf{linear half-spaces}}} \emph{in all dimensions} and \emph{for all $p$}.
\end{remark}

\item \begin{definition} (\textbf{\emph{Gaussian Isoperimetric Function}}) \\
Denote \emph{the cumulative distribution function of standard Normal distribution}:
\begin{align*}
\Phi(x) &= \int_{-\infty}^{x}\frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}}dt := \int_{-\infty}^{x}\varphi(t) dt
\end{align*} where $\varphi(x) := \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}} = (\Phi(x))'$ is \emph{the probability density function} of standard normal distribution. $\Phi^{-1}(x)$ is \emph{the quantile function of normal distribution}.

Define \underline{\textbf{\emph{the Gaussian isoperimetric function}}} as 
\begin{align*}
\gamma(x) &:= \varphi\paren{\Phi^{-1}(x)}, \quad x \in (0,1). 
\end{align*} Also we define $\gamma(0) = \gamma(1) = 0$. 
\end{definition}

\item \begin{remark}
Note that
\begin{align*}
x &= \Phi(\Phi^{-1}(x)) \\
\Rightarrow 1 &= \varphi(\Phi^{-1}(x))(\Phi^{-1}(x))'  = \gamma(x) (\Phi^{-1}(x))' \\
\Leftrightarrow 1/\gamma(x) &= (\Phi^{-1}(x))'.
\end{align*} The quantity $1/\gamma(x) = (\Phi^{-1}(x))'$ is known as \emph{\textbf{quantile-density function} of normal distribution}.
\end{remark}

\item \begin{proposition}(\textbf{Basic Property of the Gaussian Isoperimetric Function}) \citep{boucheron2013concentration} \\
The Gaussian isoperimetric function $\gamma$ satisfies:
\begin{enumerate}
\item 
\begin{align*}
\gamma'(x) &= - \Phi^{-1}(x), \quad \text{ for all }x \in (0,1),
\end{align*}
\item 
\begin{align*}
\gamma(x)\gamma''(x) &= - 1, \quad \text{ for all }x \in (0,1),
\end{align*}
\item $(\gamma')^2$ is convex over $(0, 1).$
\end{enumerate}
\end{proposition}


\item \begin{lemma} (\textbf{Asymptotic Behavior of Gaussian Isoperimetric Function}) \citep{boucheron2013concentration}\\
For all $x \in [0, 1/2]$, 
\begin{align*}
x\sqrt{\frac{1}{2} \log\frac{1}{x}} \le \gamma(x) \le x\sqrt{2 \log \frac{1}{x}}.
\end{align*} Moreover, 
\begin{align*}
\lim\limits_{x \to 0}\frac{\gamma(x)}{x\sqrt{2\log \frac{1}{x}}} = 1
\end{align*}
\end{lemma}

\item \begin{proposition} (\textbf{Bobkov's Gaussian Inequality}) \citep{boucheron2013concentration}\\
Let $Z := (Z_1 \xdotx{,} Z_n)$ be a vector of \textbf{independent standard Gaussian} random variables. Let $f: \bR^n \to [0, 1]$ be a differentiable function with gradient $\nabla f$. Then
\begin{align}
\gamma\paren{\E{}{f(X)}} &\le \E{}{\sqrt{\gamma(f(X))^2 + \norm{\nabla f(X)}{2}^2 }} \label{ineqn: bobkov_gaussian_inequality}
\end{align} where $\gamma = \varphi \circ \Phi^{-1}$ is \textbf{the Gaussian isoperimetric function}.
\end{proposition}

\item \begin{theorem} (\textbf{Gaussian Isoperimetric Theorem}) \citep{boucheron2013concentration} \citep{boucheron2013concentration, vershynin2018high, wainwright2019high}\\
Let $\bP$ be the \textbf{standard Gaussian measure} on $\bR^n$ and let $A \subset \bR^n$ be a Borel set. Then 
\begin{align}
\liminf\limits_{t \to 0}\frac{\bP(A_t) - \bP(A)}{t} &\ge \gamma\paren{\bP(A)}, \label{ineqn: gaussian_isoperimetric_inequality}
\end{align} where $A_t := \set{x: d(x, A) < t}$ be the $t$-blowup of $A$. Moreover, if $A$ is a \underline{\textbf{half-space}} defined by $A := \set{x \in \bR^n: x_1 \le z}$, then 
\begin{align}
\liminf\limits_{t \to 0}\frac{\bP(A_t) - \bP(A)}{t} &= \gamma\paren{\bP(A)} = \varphi(z), \label{ineqn: gaussian_isoperimetric_inequality_half_space}
\end{align}  where $\gamma := \varphi \circ \Phi^{-1}$ is \textbf{the Gaussian isoperimetric function}.
\end{theorem}

\item \begin{proposition} \label{prop: diff_t_blowup} (\textbf{Differentiablity of Measure of $t$-Blowup}) \citep{boucheron2013concentration}\\
If $A$ is a \textbf{finite union} of \textbf{open} balls in $\bR^n$, then $\bP(A_t)$ is a \textbf{differentiable} function of $t > 0$.
\end{proposition}

\item Next we describe \emph{\textbf{an equivalent version}} of \emph{\textbf{the Gaussian isoperimetric theorem}} in the manner of \emph{measure concentration}:
\begin{theorem} (\textbf{Gaussian Concentration Theorem}) \citep{boucheron2013concentration} \citep{boucheron2013concentration, vershynin2018high, wainwright2019high}\\
Let $\bP$ be the \textbf{standard Gaussian measure} on $\bR^n$ and let $A \subset \bR^n$ be a Borel set. Then for all $t \ge 0$, 
\begin{align}
\bP(A_t) &\ge \Phi\paren{\Phi^{-1}\paren{\bP(A)} + t}. \label{ineqn: gaussian_concentration_inequality}\\
\Leftrightarrow \Phi^{-1}(\bP(A_t)) & \ge \Phi^{-1}\paren{\bP(A)} + t \nonumber
\end{align} Equality holds if $A$ is a \textbf{half-space}.
\end{theorem}

\item \begin{remark} (\emph{\textbf{Gaussian Concentration Theorem} $\equiv$ \textbf{Gaussian Isoperimetric Theorem}}) \\
\emph{The Gaussian concentration theorem} is equivalent to \emph{the Gaussian isoperimetric theorem} since
\begin{align*}
\liminf\limits_{t \to 0}\frac{\bP(A_t) - \bP(A)}{t} &\ge \liminf\limits_{t \to 0}\frac{\Phi\paren{\Phi^{-1}\paren{\bP(A)} + t} - \Phi\paren{\Phi^{-1}\paren{\bP(A)}}}{t} \\
&= \Phi'(\Phi^{-1}(\bP(A))) \\
&= \varphi(\Phi^{-1}(\bP(A))) \\
&= \gamma(\bP(A)).
\end{align*}
\end{remark}

\item \begin{exercise} (\textbf{From Isoperimetry to Concentration}) \citep{boucheron2013concentration}\\
Assume that a probability distribution $\bP$ on $\bR^n$ satisfies, for all Borel sets $A \subset \bR^n$, 
\begin{align*}
\liminf\limits_{t \to 0}\frac{\bP(A_t) - \bP(A)}{t} &\ge c\,f\paren{F^{-1}(\bP(A))},
\end{align*} where $c \in (0, 1]$ is a constant, $F$ is a continously differentiable distribution function and $f = F'$ is its derivative. Prove that for all Borel set $A$ and all $t \ge 0$, 
\begin{align*}
\bP(A_t) \ge F\paren{F^{-1}(\bP(A)) + c\,t}.
\end{align*}
\end{exercise}
\end{itemize}
\subsection{Lipschitz Functions of Gaussian Variables}
\begin{itemize}
\item \begin{theorem} (\textbf{Rademacher Theorem}).\\
If $f: U \to \bR$ is a $L$-Lipschitz function where $U \subseteq \bR^n$, then $f$ is \textbf{differentiable almost everywhere} in $U$ and \textbf{the essential supremum} of the \textbf{norm} of its \textbf{derivative}  is \textbf{bounded} by its \textbf{Lipschitz constant}. 
\end{theorem}

\item \begin{theorem} (\textbf{Lipschitz Functions of Gaussian Variables}) \citep{boucheron2013concentration} \\
Let $Z = (Z_1 \xdotx{,} Z_n)$ be a vector of $n$ \textbf{independent} \textbf{standard normal} random variables. Let $f : \bR^n \to \bR$ denote an \textbf{$L$-Lipschitz function}, that is, there exists a constant $L > 0$ such that for all $x, y \in \bR^n$,
\begin{align*}
\abs{f(x) - f(y)} &\le  L\norm{x - y}{}.
\end{align*} Then, for all $\lambda \in \bR$,
\begin{align}
\psi_{f(Z) - \E{}{f(Z)}}(\lambda) := \log \E{}{e^{\lambda\paren{f(Z) - \E{}{f(Z)}}}} &\le \frac{L^2 \lambda^2}{2} \label{ineqn: lip_fun_gaussian}
\end{align}
\end{theorem}

\item \begin{theorem}  (\textbf{Gaussian Concentration Inequality}) (\textbf{The Tsirelson-Ibragimov-Sudakov Inequality}) \citep{boucheron2013concentration, wainwright2019high} \\
Let $Z = (Z_1 \xdotx{,} Z_n)$ be a vector of $n$ \textbf{independent} \textbf{standard normal} random variables. Let $f : \bR^n \to \bR$ denote an \textbf{$L$-Lipschitz function}. Then, for all $t > 0$, 
\begin{align}
\bP\set{f(Z) - \E{}{f(Z)} \ge t } &\le \exp\paren{- \frac{t^2}{2 L^2}}. \label{ineqn: gaussian_concentration_inequality}
\end{align}
\end{theorem}

\item As a direct consequence of \emph{the Gaussian isoperimetric inequality}, we have the improved result for \emph{Gaussian concentration inequality}:
 \begin{theorem}  (\textbf{Gaussian Concentration Inequality, Sharp Bound}) \citep{boucheron2013concentration, wainwright2019high} \\
Let $Z = (Z_1 \xdotx{,} Z_n)$ be a vector of $n$ \textbf{independent} \textbf{standard normal} random variables. Let $f : \bR^n \to \bR$ denote an \textbf{$L$-Lipschitz function}. Then, for all $t > 0$, 
\begin{align}
\bP\set{f(Z) - \text{Med}(f(Z)) \ge t } &\le 1 - \Phi\paren{\frac{t}{L}}. \label{ineqn: gaussian_concentration_inequality_improved}
\end{align} where $\Phi(t)$ is the cumulative distribution function of standard normal random variable.
\end{theorem}

\item \begin{remark}
Note that by \emph{\textbf{Gordon's inequality}}
\begin{align*}
1 - \Phi(t) &\le \paren{\frac{1}{\sqrt{2\pi}}}\frac{1}{t} e^{-\frac{t^2}{2}} = \frac{1}{t}\varphi(t)
\end{align*} The Gaussian concentration inequality fails to capture the corrective factor $t^{-1}$. The inequality above cannot be improved in general as for $f(x) = n^{-1/2}\sum_{i=1}^n x_i$, equality is achieved for all $t > 0$.
\end{remark}
\end{itemize}
\subsection{Gaussian Logarithmic Sobolev Inequality}
\begin{itemize}
\item \begin{proposition} (\textbf{Gaussian Logarithmic Sobolev Inequality}). \citep{boucheron2013concentration}\\
Let $f: \bR^n \to \bR$ be a \textbf{continuous differentiable} function and let $Z = (Z_1 \xdotx{,} Z_n)$ be a vector of $n$ \textbf{independent} \textbf{standard Gaussian} random variables. Then
\begin{align}
\text{Ent}(f^2(Z)) &\le 2\E{}{\norm{\nabla f(Z)}{2}^2}.  \label{ineqn: log_sobolev_inequality_gaussian}
\end{align}
\end{proposition}
\end{itemize}
\subsection{Gaussian Transportation Inequality}
\begin{itemize}
\item \begin{theorem} (\textbf{Talagrand's Gaussian Transportation Inequality}) \citep{boucheron2013concentration}\\
Let $\bP$ be be the standard Gaussian probability measure on $\bR^n$, and let $\bQ$ be a probability measure absolutely continuous with respect to $\bP$. Define two random vectors $X = (X_1 \xdotx{,} X_n), Y = (Y_1 \xdotx{,} Y_n)$ in $\cX^n$ with distribution $\bP$ and $\bQ$ respectively.  Then
\begin{align}
\cW_{2, d}(\bQ, \bP) := \sqrt{\min_{\gamma \in \Pi(\bQ, \bP)}\sum_{i=1}^n\E{\gamma}{(X_i - Y_i)^2}   } &\le \sqrt{2\kl{\bQ}{\bP}} \label{ineqn: talagrand_gaussian_transport_cost_inequality} \\
\Leftrightarrow  \min_{\gamma \in \Pi(\bQ, \bP)}\sum_{i=1}^n\E{\gamma}{(X_i - Y_i)^2}   &\le 2\kl{\bQ}{\bP} \nonumber
\end{align}
\end{theorem}
\end{itemize}
\subsection{Gaussian Hypercontractivity}
\subsection{Suprema of Gaussian Process}
\begin{itemize}
\item \begin{definition} (\textbf{\emph{Gaussian Process}})\\
Let $T$ be a \emph{metric space}. \emph{A stochastic process} $(X_t)_{t \in T}$ is a \underline{\emph{\textbf{Gaussian process} indexed by $T$}} if for  \emph{any finite collection} $\set{t_1 \xdotx{,}, t_n} \subset T$, the vector $(X_{t_1} \xdotx{,} X_{t_n})$ has a \emph{jointly Gaussian distribution}.

In addition, we assume that $T$ is \emph{\textbf{totally bounded}} (i.e. for every $t > 0$ it can be covered by \emph{finitely many balls} of radius $t$) and that the \emph{Gaussian process} is \emph{\textbf{almost surely continuous}}, that is, with probability $1$, $X_t$ is a \emph{continuous function} of $t$.
\end{definition} 

\item \begin{theorem} (\textbf{Concentration of Suprema of Gaussian Process}) \citep{boucheron2013concentration, vershynin2018high,  wainwright2019high, gine2021mathematical}\\
Let  $(X_t)_{t \in T}$ be an \textbf{almost surely continuous} \textbf{centered Gaussian process} indexed by a \textbf{totally bounded} set $T$. If
\begin{align*}
\sigma^2 &:= \sup_{t \in T}\E{}{X_t^2},
\end{align*} then $Z = \sup_{t \in T}X_t$ satisfies $\text{Var}(Z) \le \sigma^2$, and for all $u > 0$,
\begin{align}
\bP\set{Z - \E{}{Z} \ge u} &\le  \exp\paren{- \frac{u^2}{2 \sigma^2}}  \label{ineqn: gaussian_process_sup_inquality_upper}
\end{align} and
\begin{align}
\bP\set{\E{}{Z} - Z \ge u} &\le  \exp\paren{- \frac{u^2}{2 \sigma^2}}  \label{ineqn: gaussian_process_sup_inquality_upper}
\end{align}
\end{theorem}
\end{itemize}
\section{Concentration of Bernoulli Measure on the Binary Hypercube}
\subsection{Edge Isoperimetric Inequality on the Binary Hypercube}
\subsection{Bobkov's Inequality}
\begin{itemize}
\item \begin{proposition} (\textbf{Bobkov's Inequality}) \citep{boucheron2013concentration}\\
Suppose $Z$ is uniformly distributed over $\set{-1,1}^n$. Then for all $n \ge 1$ and for all functions $f: \set{-1, 1}^n \to [0,1]$,
\begin{align}
\gamma\paren{\E{}{f(Z)}} &\le \E{}{\sqrt{\gamma(f(Z))^2 + \norm{\nabla f(Z)}{2}^2 }} \label{ineqn: bobkov_inequality}
\end{align}
\end{proposition}
\end{itemize}
\subsection{Vertex Isoperimetric Inequality on the Binary Hypercube}
\subsection{Hypercontractivity: The Bonami-Beckner Inequality}
\subsection{Influence Function}
\subsection{Monotone Sets}
\subsection{Threshold Phenomena}





\newpage
\bibliographystyle{plainnat}
\bibliography{reference.bib}
\end{document}