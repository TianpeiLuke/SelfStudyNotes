\documentclass[11pt]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent %\usepackage{graphicx}
\usepackage{amsmath,amssymb, mathrsfs,  mathtools, dsfont}
\usepackage{tabularx}
\usepackage{tikz-cd}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{xcolor}
%\usepackage[linkbordercolor ={1 1 1} ]{hyperref}
%\usepackage[sf]{titlesec}
\usepackage{natbib}
%\usepackage{tikz-cd}

\usepackage{../../Tianpei_Report}

%\usepackage{appendix}
%\usepackage{algorithm}
%\usepackage{algorithmic}

%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}



\begin{document}
\title{Lecture 3: Information Inequalities}
\author{ Tianpei Xie}
\date{Jan. 6th., 2023 }
\maketitle
\tableofcontents
\newpage
\section{Information Theory Basics}
\subsection{Entropy, Relative Entropy, and Mutual Information}
\begin{itemize}
\item \begin{definition} (\textbf{\emph{Shannon Entropy}}) \citep{thomas2006elements}\\
Let $(\Omega, \srF, \bP)$ be a probability space and $X: \bR \to \cX$ be a random variable. Define $p(x)$ as \emph{the probability density function} of $X$ with respect to a base measure $\mu$ on $\cX$. \underline{\emph{\textbf{The Shannon Entropy}}} is defined as 
\begin{align*}
H(X) &:= \E{p}{-\log p(X)} \\
&= \int_{\Omega} -\log p(X(\omega)) d\bP(\omega) \\
&= - \int_{\cX} p(x)  \log p(x) d\mu(x)
\end{align*}
\end{definition}

\item \begin{definition} (\textbf{\emph{Conditional Entropy}}) \citep{thomas2006elements}\\
If a pair of random variables $(X, Y)$ follows the joint probability density function $p(x, y)$ with respect to a base product measure $\mu$ on $\cX \times \cY$. Then \emph{\textbf{the joint entropy}} of $(X, Y)$, denoted as $H(X, Y)$, is defined as
\begin{align*}
H(X, Y) &:=  \E{X, Y}{-\log p(X, Y)} = - \int_{\cX \times \cY} p(x, y)  \log p(x, y) d\mu(x, y)
\end{align*} Then \emph{\textbf{the conditional entropy}} $H(Y | X)$ is defined as
\begin{align*}
H(Y | X) &:= \E{X, Y}{-\log p(Y|X)}  = -\int_{\cX \times \cY} p(x, y)  \log p(y | x) d\mu(x, y) \\
&=  \E{X}{\E{Y}{-\log p(Y|X)}} = \int_{\cX}p(x) \paren{-\int_{\cY}p(y|x) \log p(y|x) d\mu(y)}d\mu(x)
\end{align*}
\end{definition}

\item \begin{proposition}(\textbf{Properties of Shannon Entropy})  \citep{thomas2006elements}\\
Let $X, Y, Z$ be random variables. 
\begin{enumerate}
\item (\textbf{Non-negativity}) $H(X) \ge 0$;
\item (\textbf{Chain Rule}) 
\begin{align*}
H(X, Y) &= H(X) + H(Y | X)
\end{align*} Furthermore, 
\begin{align*}
H(X, Y | Z) &= H(X | Z) + H(Y | X, Z)
\end{align*}
\item (\textbf{Sub-Additivity}) 
\begin{align*}
H(X, Y) &\le H(X) + H(Y)
\end{align*}
\item (\textbf{Concavity}) $H(p) := \E{p}{-\log p(X)}$ is a concave function in terms of p.d.f. $p$, i.e.
\begin{align*}
H(\lambda p_1 + (1- \lambda) p_2) \ge \lambda H(p_1) + (1- \lambda) H(p_2)
\end{align*} for any two p.d.fs $p_1, p_2$ on $\cX$ and any $\lambda \in [0,1]$.
\end{enumerate}
\end{proposition}

\item \begin{definition} (\textbf{\emph{Relative Entropy / Kullback-Leibler Divergence}}) \citep{thomas2006elements}\\
Suppose that $P$ and $Q$ are \emph{probability measures} on a measurable space $\cX$, and $P$ is \emph{absolutely continuous} with respect to $Q$, then \underline{\emph{\textbf{the relative entropy}}} or \underline{\emph{\textbf{the Kullback-Leibler divergence}}} is defined as
\begin{align*}
\kl{P}{Q} &:=\E{P}{\log\paren{\frac{dP}{dQ}}} = \int_{\cX} \log\paren{\frac{dP(x)}{dQ(x)}} dP(x)
\end{align*} where $\frac{dP}{dQ}$ is \emph{the Radon-Nikodym derivative} of $P$ with respect to $Q$. Equivalently, the KL-divergence can be written as
\begin{align*}
\kl{P}{Q} &= \int_{\cX} \paren{\frac{dP(x)}{dQ(x)}} \log\paren{\frac{dP(x)}{dQ(x)}} dQ(x) 
\end{align*} which is \emph{the entropy of $P$ relative to $Q$}. Furthermore, if $\mu$ is a base measure on $\cX$ for which densities $p$ and $q$ with $dP = p(x)d\mu$ and $dQ = q(x) d\mu$ exist, then 
\begin{align*}
\kl{P}{Q} &= \int_{\cX} p(x)\log\paren{\frac{p(x)}{q(x)}} d\mu(x)
\end{align*}
\end{definition}

\item \begin{definition}(\textbf{\emph{Mutual Information}}) \citep{thomas2006elements}\\
Consider two random variables $X, Y$ on $\cX \times \cY$ with joint probability distribution $P_{(X, Y)}$ and marginal distribution $P_{X}$ and $P_{Y}$. \underline{\emph{\textbf{The mutual information $I(X; Y)$}}} is \emph{the relative entropy} between \emph{the joint distribution} $P_{(X, Y)}$ and \emph{the product distribution} $P_{X}\otimes P_{Y}$:
\begin{align*}
I(X; Y) &= \kl{P_{(X, Y)}}{P_{X}\otimes P_{Y}} = \E{P_{(X, Y)}}{\log \frac{dP_{(X,Y)}}{dP_{X} \otimes dP_{Y}}}
\end{align*} If $P_{(X, Y)}$ has a probability density function $p(x,y)$ with respect to a base measure $\mu$ on $\cX \times \cY$, then 
\begin{align*}
I(X; Y) &=\int_{\cX \times \cY} p(x, y)\log\paren{\frac{p(x, y)}{p_{X}(x)p_{Y}(y)}} d\mu(x, y)
\end{align*}
\end{definition}

\item \begin{proposition} (\textbf{Properties of Relative Entropy and Mutual Information})  \citep{thomas2006elements}\\
Let $X, Y$ be random variables.
\begin{enumerate}
\item (\textbf{Non-negativity})  Let $p(x), q(x)$ be probability density function of $P ,Q$.
\begin{align*}
\kl{P}{Q} \ge 0
\end{align*} with equality if and only if $p(x) = q(x)$ almost surely. Therefore, the mutual information is non-negative as well:
\begin{align*}
I(X; Y) \ge 0
\end{align*} with equality if and only if $X$ and $Y$ are independent.
\item (\textbf{Finite Cardinality Domain}) Let $\abs{\cX}$ be the number of elements in domain $\cX$ and $X$ is a discrete random variables in $\cX$. Then the relative entropy of probability distribution $p$ with respect to uniform distribution $u$ on $\cX$ is 
\begin{align*}
\kl{p}{u}  &= \log \abs{\cX} - H(X) \ge 0\\
\Rightarrow H(X) &\le \log \abs{\cX}
\end{align*}
\item (\textbf{Symmetry})  $I(X; Y) = I(Y; X)$
\item (\textbf{Information Gain via Conditioning}) The mutual information $I(X; Y)$ is the reduction in the uncertainty of $X$ due to the knowledge of $Y$ (and vice versa)
\begin{align}
I(X; Y) &= H(X) - H(X | Y) \label{eqn: mutual_information_gain}\\
&= H(Y) - H(Y | X) \nonumber\\
&= H(X) + H(Y) - H(X, Y) \nonumber
\end{align}
\item (\textbf{Shannon Entropy as Self-Information})  $I(X; X) = H(X)$
\end{enumerate}
\end{proposition}
\end{itemize}
\subsection{Chain Rules for Entropy, Relative Entropy, and Mutual Information}
\begin{itemize}
\item \begin{proposition} (\textbf{Conditioning Reduces Entropy}) \citep{thomas2006elements}\\
From non-negativity of mutual information, we see that the entropy of $X$ is non-increasing when conditioning on $Y$
\begin{align}
H(X | Y) \le  H(X)  \label{ineqn: conditional_entropy}
\end{align} where equality holds if and only if $X$ and $Y$ are independent.
\end{proposition}


\item \begin{proposition} (\textbf{Chain Rule for Entropy}) \citep{thomas2006elements}\\
Let $X_1, X_2 \xdotx{,} X_n$ be drawn according to $p(x_1, x_2 \xdotx{,} x_n)$. Then
\begin{align}
H(X_1, X_2 \xdotx{,} X_n) &= \sum_{i=1}^{n}H(X_i | X_{i-1} \xdotx{,} X_1) \label{eqn: chain_rule_entropy}
\end{align}
\end{proposition}

\item \begin{proposition} (\textbf{Sub-Additivity of Entropy}) \citep{thomas2006elements}\\
Let $X_1, X_2 \xdotx{,} X_n$ be drawn according to $p(x_1, x_2 \xdotx{,} x_n)$. Then
\begin{align}
H(X_1, X_2 \xdotx{,} X_n) &\le \sum_{i=1}^{n}H(X_i)  \label{ineqn: sub_additivity_entropy}
\end{align} with equality if and only if the $X_i$ are independent.
\end{proposition}

\item \begin{proposition} (\textbf{Chain Rule for Mutual Information}) \citep{thomas2006elements}\\
Let $X_1, X_2 \xdotx{,} X_n, Y$ be drawn according to $p(x_1, x_2 \xdotx{,} x_n, y)$. Then
\begin{align}
I(X_1, X_2 \xdotx{,} X_n; Y) &= \sum_{i=1}^{n}H(X_i ; Y | X_{i-1} \xdotx{,} X_1) \label{eqn: chain_rule_mutual_info}
\end{align} where \textbf{the conditional mutual information} is defined as 
\begin{align*}
I(X; Y | Z) := H(X | Z) - H(X | Y, Z) = \kl{P_{(X, Y| Z)}}{P_{X|Z}\otimes P_{Y|Z}} 
\end{align*}
\end{proposition}

\item \begin{proposition} (\textbf{Chain Rule for Relative Entropy}) \citep{thomas2006elements}\\
Let $P_{(X, Y)}$ and $Q_{(X, Y)}$ be two probability measures on product space $\cX \times \cY$ and $P \ll Q$. Denote the marginal distributions $P_X, Q_X$ and $P_Y$, $Q_Y$ on $\cX$ and $\cY$, respectively. $P_{Y|X}$ and $Q_{Y|X}$ are conditional distributions (Note that $P_{Y|X} \ll Q_{Y|X}$).  Define \textbf{the conditional relative entropy} as
\begin{align*}
\E{X}{\kl{P_{Y | X}}{Q_{Y | X}}} := \E{X}{\E{P_{Y|X}}{\log \paren{\frac{dP_{Y| X}}{dQ_{Y | X}}}}}. 
\end{align*} Then the relative entropy of joint distribution $P_{(X, Y)}$ with respect to $Q_{(X, Y)}$ is 
\begin{align}
\kl{P_{(X, Y)}}{Q_{(X, Y)}} &= \kl{P_{X}}{Q_{X}} + \E{X}{\kl{P_{Y | X}}{Q_{Y | X}}} \label{eqn: chain_rule_kl}
\end{align} In addition, let $P$ and $Q$ denote two joint distributions for $X_1, X_2 \xdotx{,} X_n$, let $P_{1:i}$ and $Q_{1:i}$ denote the marginal distributions of $X_1, X_2 \xdotx{,} X_i$ under $P$ and $Q$, respectively. Let $P_{X_i | 1 \ldots i-1}$ and $Q_{X_i | 1 \ldots i-1}$ denote the conditional distribution of $X_i$ with respect to $X_1, X_2 \xdotx{,} X_{i-1}$ under $P$ and under $Q$.
\begin{align}
\kl{P}{Q} &= \sum_{i=1}^{n}\E{P_{1:i-1}}{\kl{P_{X_i | 1 \ldots i-1}}{Q_{X_i | 1 \ldots i-1}}} \label{eqn: chain_rule_kl_k}
\end{align} 
\end{proposition}
\end{itemize}
\subsection{Log-Sum Inequalities and Convexity}
\begin{itemize}
\item \begin{proposition} (\textbf{Log-Sum Inequalities}) \citep{thomas2006elements}\\
For non-negative numbers $a_1 \xdotx{,} a_n$ and $b_1 \xdotx{,} b_n$,
\begin{align}
\sum_{i=1}^{n}a_i \log\frac{a_i}{b_i} \ge \paren{\sum_{i=1}^{n}a_i} \log\frac{\sum_{i=1}^{n}a_i}{\sum_{i=1}^{n}b_i} \label{ineqn: log_sum_inequality}
\end{align} with equality if and only if $\frac{a_i}{b_i}$ is constant.
\end{proposition}

\item \begin{proposition} (\textbf{Joint Convexity of Relative Entropy}) \citep{thomas2006elements}\\
$\kl{p}{q}$ is \textbf{convex} in the pair $(p, q)$; that is, if $(p_1, q_1)$ and $(p_2, q_2)$ are two pairs of probability density functions, then for $\lambda \in [0, 1]$,  
\begin{align}
\kl{\lambda p_1 + (1- \lambda) p_2}{\lambda q_1 + (1- \lambda) q_2} &\le \lambda \kl{p_1}{q_1} + (1- \lambda) \kl{p_2}{q_2} \label{ineqn: kl_divergence_joint_convex}
\end{align}
\end{proposition}

\item \begin{proposition} \citep{thomas2006elements}\\
Let $(X,Y) \sim p(x,y) = p(x)p(y|x)$. The mutual information $I(X; Y)$ is a \textbf{concave} function of $p(x)$ for fixed $p(y|x)$ and a \textbf{convex} function of $p(y|x)$ for fixed $p(x)$.
\end{proposition}
\end{itemize}

\subsection{Data Processing Inequality}
\begin{itemize}
\item \begin{definition} (\emph{Data Processing Markov Chain})\\
Random variables $X, Y, Z$ are said to \emph{\textbf{form a Markov chain}} in that order (denoted by $X \to Y \to Z$) if the conditional distribution of $Z$ depends only on $Y$ and is \emph{\textbf{conditionally independent}} of $X$. Specifically, $X, Y$, and $Z$ form a Markov chain $X \to Y \to Z$ if the joint probability mass function can be written as
\begin{align*}
p(x, y, z) &= p(x) p(y | x) p(z | y)
\end{align*}
\end{definition}

\item \begin{proposition} (\textbf{Data Processing Inequality})   \citep{thomas2006elements}\\
If $X \to Y \to Z$, then
\begin{align*}
I(X; Z) \le I(X; Y)
\end{align*}
\end{proposition}

\item \begin{corollary}\citep{thomas2006elements}\\
In particular, if $Z = g(Y)$, we have 
\begin{align*}
I(X; g(Y)) &\le I(X; Y)
\end{align*}
\end{corollary}

\item \begin{corollary}\citep{thomas2006elements}\\
If $X \to Y \to Z$, then
\begin{align*}
I(X; Y| Z) &\le I(X; Y)
\end{align*} Thus, the dependence of $X$ and $Y$ is \textbf{decreased} (or remains unchanged) by the observation of a ``\textbf{downstream}" random variable $Z$. 
\end{corollary}
\end{itemize}


\section{Information Inequalities}
\subsection{Han's Inequality}
\begin{itemize}
\item \begin{proposition} (\textbf{Han's Inequality}) \citep{thomas2006elements, boucheron2013concentration}\\
Let $X_1, X_2 \xdotx{,} X_n$ be random variables. Then
\begin{align}
H(X_1, X_2 \xdotx{,} X_n) &\le \frac{1}{n-1}\sum_{i=1}^{n}H(X_1 \xdotx{,} X_{i-1}, X_{i+1} \xdotx{,} X_n) \label{ineqn: han_inequality} \\
\Leftrightarrow H(X) &\le \frac{1}{n-1}\sum_{i=1}^{n}H(X_{(-i)}) \nonumber
\end{align}
\end{proposition}
\begin{proof}
For any $i= 1 \xdotx{,} n$, by the definition of the conditional entropy and the fact that conditioning reduces entropy,
\begin{align*}
H(X_1, X_2 \xdotx{,} X_n) &= H(X_1 \xdotx{,} X_{i-1}, X_{i+1} \xdotx{,} X_n)  + H(X_i | X_1 \xdotx{,} X_{i-1}, X_{i+1} \xdotx{,} X_n) \\
&\le H(X_1 \xdotx{,} X_{i-1}, X_{i+1} \xdotx{,} X_n)  + H(X_i | X_1 \xdotx{,} X_{i-1}).
\end{align*} Summing these n inequalities and using the chain rule for entropy, we get
\begin{align*}
n H(X_1, X_2 \xdotx{,} X_n) &\le \sum_{i=1}^{n}H(X_1 \xdotx{,} X_{i-1}, X_{i+1} \xdotx{,} X_n) + \sum_{i=1}^{n}H(X_i | X_1 \xdotx{,} X_{i-1})\\
&= \sum_{i=1}^{n}H(X_1 \xdotx{,} X_{i-1}, X_{i+1} \xdotx{,} X_n) + H(X_1, X_2 \xdotx{,} X_n)
\end{align*} which is what we wanted to prove. \qed
\end{proof}

\item \begin{proposition} (\textbf{Han's Inequality for Relative Entropy}) \citep{boucheron2013concentration}\\
Let $(\cX, \srB)$ be a measurable space, and $P$ and $Q$ be probability measures on $\cX^{n}$ such that $P = P_1 \xdotx{\otimes} P_n$ is a \textbf{product measure}. We denote the element of $\cX^{n}$ by $x = (x_1 \xdotx{,} x_n)$ and write $x_{(-i)} := (x_1 \xdotx{,} x_{i-1}, x_{i+1} \xdotx{,} x_n)$ for the $(n-1)$-vector obtained by \textbf{leaving out the $i$-th component of $x$} (i.e. the $i$-th Jackknife sample of $x$). Denote $Q_{(-i)}$ and $P_{(-i)}$ the marginal distributions of $Q$ and $P$. Let $p_{(-i)}$ and $q_{(-i)}$ denote the corresponding probability density function with respect to base measure $\mu$ on $\cX$.
\begin{align*}
q_{(-i)}(x_{(-i)}) &= \int_{y \in \cX}q(x_1 \xdotx{,} x_{i-1}, y, x_{i+1} \xdotx{,} x_n) d\mu(y)\\
p_{(-i)}(x_{(-i)}) &= \int_{y \in \cX}p(x_1 \xdotx{,} x_{i-1}, y, x_{i+1} \xdotx{,} x_n) d\mu(y)\\
&= \prod_{j \neq i}p_{j}(x_j).
\end{align*} Then 
\begin{align}
\kl{Q}{P} &\ge \frac{1}{n-1}\sum_{i=1}^{n}\kl{Q_{(-i)}}{P_{(-i)}}  \label{ineqn: han_inequality_relative_entropy}
\end{align} or equivalently, 
\begin{align}
\kl{Q}{P} &\le \sum_{i=1}^{n}\paren{\kl{Q}{P} - \kl{Q_{(-i)}}{P_{(-i)}}}  \label{ineqn: han_inequality_relative_entropy_2}
\end{align}
\end{proposition}
\begin{proof}
From Han's inequality, we have
\begin{align*}
-H(Q) &\ge -\frac{1}{n-1}\sum_{i=1}^{n}H(Q_{(-i)}).
\end{align*} Since
\begin{align*}
\kl{Q}{P} &= -H(Q) + \E{Q}{-\log P(X)}
\end{align*} and
\begin{align*}
\kl{Q_{(-i)}}{P_{(-i)}} &= -H(Q_{(-i)}) + \E{Q_{(-i)}}{-\log P_{(-i)}(X_{(-i)})}, 
\end{align*} it suffices to show that 
\begin{align*}
\E{Q}{-\log P(X)} &= \frac{1}{n-1}\sum_{i=1}^{n}\E{Q_{(-i)}}{-\log P_{(-i)}(X_{(-i)})}.
\end{align*} This may be seen easily by noting that by the product property of $P$, we have $p(x) = p_{(-i)}(x_{(-i)})p_i(x_i)$ for all $i$, and also $p(x) = \prod_i p_i(x_i)$, and therefore
\begin{align*}
\E{Q}{-\log P(X)} &= \frac{1}{n}\sum_{i=1}^{n}\E{Q}{-\log P_{(-i)}(X_{(-i)}) -\log P_i(X_i)} \\
&= \frac{1}{n}\sum_{i=1}^{n}\E{Q}{-\log P_{(-i)}(X_{(-i)})} + \frac{1}{n}\sum_{i=1}^{n}\E{Q}{-\log P_i(X_i)}\\
&= \frac{1}{n}\sum_{i=1}^{n}\E{Q}{-\log P_{(-i)}(X_{(-i)})} + \frac{1}{n}\E{Q}{-\log P(X)}.
\end{align*} Rearranging, we obtain
\begin{align*}
\E{Q}{-\log P(X)} &= \frac{1}{n-1}\sum_{i=1}^{n}\E{Q}{-\log P_{(-i)}(X_{(-i)})}\\
&=\frac{1}{n-1}\sum_{i=1}^{n}\E{Q_{(-i)}}{-\log P_{(-i)}(X_{(-i)})}. \qed
\end{align*}
\end{proof}
\end{itemize}

\subsection{Applications of Han's Inequality}
\subsubsection{Combinatorial Entropies}
\subsubsection{Edge Isoperimetric Inequality on the Binary Hypercube}


\subsection{$\Phi$-Entropy}
\begin{itemize}
\item \begin{definition}(\textbf{\emph{$\Phi$-Entropy}})\citep{boucheron2013concentration}\\
Let $\Phi: [0, \infty) \rightarrow \bR$ be a \textbf{\emph{convex}} function, and assign, to every \emph{\textbf{non-negative} integrable random variable} $X$, \underline{\textbf{\emph{the $\Phi$-entropy}}} of $X$ is defined as 
\begin{align}
H_{\Phi}(X) &= \E{}{\Phi(X)} - \Phi(\E{}{X}). \label{def: phi_entropy}
\end{align}
\end{definition}

\item \begin{remark}
The $\Phi$-entropy is a \emph{\textbf{functional}} of \emph{distribution} $P_{X}$ instead of a function of $X$.
\end{remark}

\item \begin{remark}
By Jenson's inequality, the $\Phi$-entropy is \emph{non-negative}
\begin{align*}
\Phi(\E{}{X}) &\le \E{}{\Phi(X)}\\
\Rightarrow H_{\Phi}(X) &= \E{}{\Phi(X)} - \Phi(\E{}{X}) \ge 0.
\end{align*} 
\end{remark}

\item \begin{example} (\emph{\textbf{Special Examples for $\Phi$-Entropy}})
\begin{enumerate}
\item For $\Phi(x) = x^2$, \emph{the $\Phi$-entropy} of $X$ is \emph{the \textbf{variance}} of $X$:
\begin{align*}
H_{\Phi}(X) &= \E{}{X^2} - (\E{}{X})^2 = \text{Var}(X).
\end{align*}
\item For $\Phi(x) = -\log(x)$, \emph{the $\Phi$-entropy} of $Y=e^{\lambda X}$ is \emph{the \textbf{logarithm of moment generating function}} of $X - \E{}{X}$:
\begin{align}
H_{\Phi}(e^{\lambda X}) &= -\lambda \E{}{X} + \log\paren{\E{}{e^{\lambda X}}} = \log\E{}{e^{\lambda(X - \E{}{X})}} := \psi_{X- \E{}{X}}(\lambda). \label{eqn: phi_entropy_log_mgf}
\end{align}
\item For $\Phi(x) = x\log x$, \emph{the $\Phi$-entropy} of $X$ is defined as the \underline{\emph{\textbf{entropy}} of $X$}
\begin{align}
H_{\Phi}(X) = \text{Ent}(X)&:=  \E{}{X\log X} - \E{}{X}\log\paren{\E{}{X}}. \label{def: phi_entropy_kl}
\end{align} Let $(\Omega, \srB)$ be measurable space, and $P$ and $Q$ are probability measures on $\Omega$ with $P \ll Q$. Define a random variable $X$ by the \emph{Radon-Nikodym derivative} of $P$ with respect to $Q$; that is,
\begin{align*}
X(\omega) &:= \left\{ \begin{array}{cc}
\frac{dP}{dQ}(\omega) & Q(\omega) > 0\\
0 &\text{o.w.}
\end{array}
\right. .
\end{align*} We see that $X$ is $Q$-measurable and $dP = X\,dQ$ with $\E{Q}{X} = 1$. Then the entropy of $X$ is the relative entropy of $P$ with respect to $Q$.
\begin{align}
\text{Ent}(X)&= \kl{P}{Q} \label{eqn: phi_entropy_kl_divg}
\end{align}
\end{enumerate}
\end{example}

\end{itemize}




\subsection{Sub-Additivity of $\Phi$-Entropy}
\begin{itemize}
\item \begin{remark}(\emph{\textbf{Sub-Additivity of Shannon Entropy}})\\
Let $X_1, X_2 \xdotx{,} X_n$ be drawn according to $p(x_1, x_2 \xdotx{,} x_n)$. Then
\begin{align*}
H(X_1, X_2 \xdotx{,} X_n) &\le \sum_{i=1}^{n}H(X_i)  
\end{align*} with equality if and only if the $X_i$ are independent.
\end{remark}

\item \begin{proposition} (\textbf{Sub-Additivity of The Entropy}) \citep{boucheron2013concentration}\\
Let $\Phi(x) = x\log x$,  for $x >0$ and $\Phi(0) = 0$. Let $Z_1, Z_2 \xdotx{,} Z_n$ be independent random variables taking values in $\cX$, and let $f: \cX^n \to [0, \infty)$. Letting $X = f(Z_1, Z_2 \xdotx{,} Z_n)$, we have 
\begin{align}
\E{}{\Phi(X)} - \Phi(\E{}{X}) &\le \sum_{i=1}^{n}\E{}{\E{(-i)}{\Phi(X)} - \Phi(\E{(-i)}{X})}, \label{ineqn: sub_additivity_phi_entropy}
\end{align} where $\E{(-i)}{\cdot}$ is the conditional expectation operator conditioning on $Z_{(-i)}$. Introducing the notation $\text{Ent}_{(-i)}(X) = \E{(-i)}{\Phi(X)} - \Phi(\E{(-i)}{X})$, this can be re-written as
\begin{align}
\E{}{\Phi(X)} - \Phi(\E{}{X}) &\le \E{}{\sum_{i=1}^{n} \text{Ent}_{(-i)}(X)}. \label{ineqn: sub_additivity_phi_entropy_2}
\end{align}
\end{proposition}
\begin{proof}
The proposition is a direct consequence of Han's inequality for relative entropies. First note that if the inequality is true for a random variable $X$, then it is also true for $cX$ where $c$ is a positive constant. Hence, we may assume that $\E{}{X} = 1$. Now define the probability measure $P$ on $\cX^n$ by its probability density function $p$ given by
\begin{align*}
p(z) &= f(z) q(z), \quad \forall z \in \cX^n
\end{align*} where $q$ denote the probability density of $Z:= (Z_1, Z_2 \xdotx{,} Z_n)$ and $Q$ the corresponding probability measure. Then 
\begin{align*}
\text{Ent}(X)&:= \E{}{X\log X} - \E{}{X}\log\paren{\E{}{X}} = \kl{P}{Q}
\end{align*} which, by Han's inequality for relative entropy
\begin{align*}
\text{Ent}(X) = \kl{P}{Q}& \le \sum_{i=1}^{n}\paren{\kl{P}{Q} - \kl{P_{(-i)}}{Q_{(-i)}}} 
\end{align*} However, straightforward calculation shows that
\begin{align*}
\sum_{i=1}^{n}\paren{\kl{P}{Q} - \kl{P_{(-i)}}{Q_{(-i)}}} &=  \sum_{i=1}^{n}\E{}{\E{(-i)}{\Phi(X)} - \Phi(\E{(-i)}{X})}
\end{align*} and the statement follows. \qed
\end{proof}

\item \begin{remark}
The Efron-Stein inequality is the special case of the inequality when $\Phi(x) = x^2$,
\begin{align*}
\E{}{\Phi(X)} - \Phi(\E{}{X}) &\le \sum_{i=1}^{n}\E{}{\E{(-i)}{\Phi(X)} - \Phi(\E{(-i)}{X})}.\\
\Rightarrow \text{Var}(X) &\le \sum_{i=1}^{n}\E{}{\text{Var}_{(-i)}(X)}
\end{align*} 
\end{remark}

\item \begin{remark} (\textbf{\emph{Tensorization of Entropy}})\\
The inequality in \eqref{ineqn: sub_additivity_phi_entropy} or \ref{ineqn: sub_additivity_phi_entropy_2} is also called the tensoriztion of entropy in \citep{wainwright2019high}
\end{remark}

\item \begin{proposition} (\textbf{Sub-Additivity of $\Phi$-Entropy}) \citep{boucheron2013concentration}\\
Let $\cC$ denote the class of functions $\Phi: [0, \infty) \to \bR$ that are \textbf{continuous} and \textbf{convex} on $[0, \infty)$, \textbf{twice differentiable} on $(0, \infty)$, and such that either $\Phi$ is \textbf{affine} or $\Phi''$ is \textbf{strictly positive} and $1/\Phi''$ is \textbf{concave}. For all $\Phi \in \cC$, the \textbf{entropy functional} $H_{\Phi}$ is \textbf{sub-additive}. That is,
\begin{align}
\E{}{\Phi(X)} - \Phi(\E{}{X}) &\le \sum_{i=1}^{n}\E{}{\E{(-i)}{\Phi(X)} - \Phi(\E{(-i)}{X})}, \label{ineqn: sub_additivity_phi_entropy_general}\\
\Leftrightarrow H_{\Phi}(X) &\le \E{}{\sum_{i=1}^{n} H_{\Phi}^{(-i)}(X)} \nonumber
\end{align} where $H_{\Phi}^{(-i)}(X) := \E{(-i)}{\Phi(X)} - \Phi(\E{(-i)}{X})$ is the conditional entropy and, $\E{(-i)}{\cdot}$ denotes conditional expectation conditioned on the $(n-1)$-vector $Z_{(-i)}:= (Z_1 \xdotx{,} Z_{i-1}, Z_{i+1} \xdotx{,} Z_n)$.
\end{proposition}
%\begin{proof}
%It suffice to show that for $n=2$ and setting $X = f(Z_1, Z_2)$, the sub-additivity property reduces to 
%\begin{align}
%H_{\Phi}\paren{\int f(z, Z_2)d\mu_1(z)} &\le \int H_{\Phi}(f(z, Z_2)) d\mu_1(z) \nonumber\\
%\Leftrightarrow H_{\Phi}\paren{\E{Z_1}{f(Z_1, Z_2)}} &\le \E{Z_1}{H_{\Phi}\paren{f(Z_1, Z_2)}} \label{eqn: sub_additivity_phi_entropy_general_proof_1}
%\end{align} where $\mu_1$ denotes the distribution of $Z_1$. It is a direct consequence of \emph{the Jenson's inequality} since $H_{\Phi}$ is a convex function. 
%\begin{align*}
%H_{\Phi}(\lambda X_1 + (1-\lambda) X_2)&= \E{}{\Phi\paren{\lambda X_1 + (1-\lambda) X_2}} - \Phi\paren{\E{}{\lambda X_1 + (1-\lambda) X_2}} \\
%&= \E{}{\Phi\paren{\lambda X_1 + (1-\lambda) X_2}} -  \Phi\paren{\lambda \E{}{X_1} + (1-\lambda) \E{}{X_2}} \\
%&\le \E{}{\lambda \Phi(X_1) + (1- \lambda)\Phi(X_2)} - \Phi\paren{\lambda \E{}{X_1} + (1-\lambda) \E{}{X_2}} \\
%\end{align*}
%
%To show the \eqref{ineqn: sub_additivity_phi_entropy_general}, let $Y_1= Z_1$ and $Y_2 := (Z_2 \xdotx{,} Z_n)$ with distributions $\mu_1$ and $\mu_2$, respectively. Then $X = f(Z_1, Z_2)$ is a measurable function of two independent random variables $Y_1$ and $Y_2$. By the \emph{Tonelli-Fubini theorem}, 
%\begin{align*}
%H_{\Phi}(X) %&= \iint\paren{\Phi\paren{f(y_1, y_2)} - \Phi\paren{\iint f(y_1', y_2') d\mu_1(y_1') d\mu_2(y_2')}} d\mu_1(y_1) d\mu_2(y_2)\\
%&= \E{(Y_1, Y_2)}{\Phi(f(Y_1, Y_2)) - \Phi\paren{\E{(Y_1, Y_2)}{f(Y_1, Y_2)}}}\\
%&= \E{(Y_1, Y_2)}{\Phi(f(Y_1, Y_2)) - \Phi\paren{\E{Y_1}{f(Y_1, Y_2)}} + \Phi\paren{\E{Y_1}{f(Y_1, Y_2)}} - \Phi\paren{\E{(Y_1, Y_2)}{f(Y_1, Y_2)}}} \\
%&= \E{Y_2}{\E{Y_1}{\Phi(f(Y_1, Y_2)) - \Phi\paren{\E{Y_1}{f(Y_1, Y_2)}} }} \\
%&\quad + \E{Y_2}{\Phi\paren{\E{Y_1}{f(Y_1, Y_2)}} - \Phi\paren{\E{(Y_1, Y_2)}{f(Y_1, Y_2)}}} \\
%&= \E{Y_2}{H_{\Phi}^{(-1)}(f(Y_1, Y_2))} + H_{\Phi}\paren{\E{Y_1}{f(Y_1, Y_2)}} \\
%&\le  \E{Y_2}{H_{\Phi}^{(-1)}(f(Y_1, Y_2))} + \E{Y_1}{H_{\Phi}\paren{f(Y_1, Y_2)}}
%\end{align*} where the last step from \eqref{eqn: sub_additivity_phi_entropy_general_proof_1}. In other words, we get,
%\begin{align*}
%H_{\Phi}(X) &\le \E{}{H_{\Phi}^{(-1)}(X)} + \int H_{\Phi}(f(z, Z_2 \xdotx{,} Z_n)) d\mu_1(z)
%\end{align*} Proceeding by induction, \eqref{eqn: sub_additivity_phi_entropy_general_proof_1} leads to the sub-additivity property for every $n$. \qed
%\end{proof}

\item \begin{remark}
\emph{\textbf{The sub-additivity property}} of $H_{\Phi}$ is equivalent to what we could call \emph{\textbf{the Jensen property}}
\begin{align}
H_{\Phi}\paren{\int f(z, Z_2)d\mu_1(z)} &\le \int H_{\Phi}(f(z, Z_2)) d\mu_1(z) \nonumber\\
\Leftrightarrow H_{\Phi}\paren{\E{Z_1}{f(Z_1, Z_2)}} &\le \E{Z_1}{H_{\Phi}\paren{f(Z_1, Z_2)}} \label{eqn: sub_additivity_phi_entropy_general_proof_1}
\end{align}
This implies that in order to prove sub-additivity of a $\Phi$-entropy, it
suffices to show that it has the Jensen property.
\end{remark}

\end{itemize}



\subsection{Duality and Variational Formulas}
\begin{itemize}
\item \begin{lemma}
The \textbf{Legendre transform} (or \textbf{convex conjugate}) of $\Phi(x) = x\log(x)$ is $e^{u-1}$. That is,
\begin{align*}
\sup_{x > 0}\set{u\,x - x\log(x)} &= e^{u-1}
\end{align*}
\end{lemma}
\begin{proof}
Solve the supremum on the left-hand side by taking derivative of the objective function and setting it as zero:
\begin{align*}
\nabla g(x) &= u - \log(x) - 1 = 0\\
\Rightarrow x^{*} &= e^{u-1} \\
\Rightarrow \sup_{x}\set{u\,x - x\log(x)} &= g(x^{*}) = u\,e^{u-1} - e^{u-1}(u-1) = e^{u-1} \qed
\end{align*}
\end{proof}

\item \begin{remark}
If $\Phi(X) = X\log(X)$ is integrable, and $\E{}{e^{U}} = 1$, we have
\begin{align*}
UX \le X\log(X) + \frac{1}{e}e^{U}.
\end{align*} Therefore, $U_{+}X$ is integrable, and one can always define $\E{}{UX} = \E{}{U_{+}X} - \E{}{U_{-}X}$ for positive and negative part of $U$. Thus the $\E{}{UX}$ is well-defined.
\end{remark}

\item \begin{theorem}(\textbf{Duality Formula of Entropy}) \citep{boucheron2013concentration}\\
Let $X$ be a non-negative random variable defined on a probability space $(\Omega, \srA, P)$ such that $\E{}{\Phi(X)} < \infty$. Then we have \textbf{the duality formula}
\begin{align}
\text{Ent}(X) &= \sup_{U \in \cU}\E{}{U\,X}  \label{eqn: duality_entropy}
\end{align} where the supremum is taken over the set $\cU$ of all random variables $U: \Omega \to \bR \cup \set{\infty}$ with $\E{}{e^{U}} = 1$. Moreover, if $U$ is such that $\E{}{U X} \le \text{Ent}(X)$ for all non-negative random variable $X$ such that $\Phi(X)$ is integrable and $\E{}{X} = 1$, then $\E{}{e^{U}} \le 1$. 
\end{theorem}
\begin{proof}
Note that for any random variable $U$ such that $\E{}{e^{U}} = 1$, we have
\begin{align*}
\text{Ent}(X) - \E{P}{UX} &= \E{P}{X\log(X)} - \E{P}{X}\log(\E{P}{X}) - \E{P}{UX}\\
&= \E{P}{X(\log(X) - U)}  - \E{P}{X}\log(\E{P}{X}) \\
&= \E{P}{X\log(Xe^{-U})} - \E{P}{X}\log(\E{P}{X})\\
&= \E{e^{U}P}{Xe^{-U}\log(Xe^{-U})} - \E{e^{U}P}{Xe^{-U}}\log(\E{e^{U}P}{Xe^{-U}}) \\
&= \text{Ent}_{e^{U}P}(Xe^{-U})
\end{align*} Note that due to $\E{}{e^{U}} = 1$, $\int e^{U}dP = 1$, thus $e^{U}P$ is a proper probability measure. This shows that
\begin{align*}
\text{Ent}_{e^{U}P}(Xe^{-U}) &\ge 0\\
\Rightarrow \text{Ent}(X) &\ge \E{P}{UX}
\end{align*}  with equality whenever $e^U = X/\E{P}{X}$. This proves the duality formula. 

Conversely, let $U$ be such that $\E{P}{UX} \le \text{Ent}(X)$ for all non-negative random variables such that $\Phi(X)$ is integrable. If $\E{}{e^{U}} = 0$, then there is nothing to prove Otherwise, given a positive integer $n$ large enough to ensure that $x_n = \E{}{e^{\min\set{U, n}}} >0$, one may define $X_n = e^{\min\set{U, n}}/x_n$, so that $\E{}{X_n} = 1$, which leads to 
\begin{align*}
\E{}{UX_n} &\le  \text{Ent}(X_n),
\end{align*} and therefore
\begin{align*}
\frac{1}{x_n}\E{}{U e^{\min\set{U, n}}} &\le \text{Ent}(e^{\min\set{U, n}}/x_n)\\
&= \frac{1}{x_n}\brac{\E{}{\min\set{U, n} e^{\min\set{U, n}} }  - \log(x_n)} 
\end{align*} Hence 
\begin{align*}
\log(x_n) \le 0
\end{align*} and taking the limit when $n\to \infty$, we show by monotonicity that $\E{}{e^{U}} \le 1$. \qed
\end{proof}

\item  \begin{theorem}(\textbf{Alternative Duality Formula of Entropy}) \citep{boucheron2013concentration}
\begin{align}
\text{Ent}(X) &= \sup_{T}\E{}{X\paren{\log(T) - \log\paren{\E{}{T}}}}  \label{eqn: duality_entropy_2}
\end{align} where the supremum is taken over all non-negative and integrable random variables.
\end{theorem}
\begin{proof}
From \eqref{eqn: duality_entropy}, taking $U = \log\frac{T}{\E{}{T}}$, so that $\E{}{e^U} = \E{}{\frac{T}{\E{}{T}}} = 1$. This gives us \eqref{eqn: duality_entropy_2}.\qed
\end{proof}

\item \begin{corollary} \label{coro: dual_log_mgf}  (\textbf{Duality Formula of Log Moment Generating Function}) \citep{thomas2006elements, boucheron2013concentration}\\
Let $X$ be a real-valued integrable random variable. Then for every $\lambda \in \bR$, 
\begin{align}
\log \E{Q}{e^{\lambda\paren{X - \E{}{X}}}} &= \sup_{P \ll Q}\set{\lambda\paren{\E{P}{X} - \E{Q}{X}} - \kl{P}{Q} }, \label{eqn: duality_log_mgf}
\end{align} where the supremum is taken over all probability measures $P$ absolutely continuous with respect to $Q$, and $\E{P}{\cdot}$ denotes integration with respect to the measure $P$ (recall that $\E{Q}{\cdot}$ is integration with respect to $Q$).
\end{corollary}
\begin{proof}
Let $P \ll Q$. Taking $Y := \frac{dP}{dQ}$ and $U:= \lambda(X - \E{Q}{X}) - \psi_{X- \E{Q}{X}}(\lambda)$ where $\psi_{X}(\lambda) := \log \E{Q}{e^{\lambda X}}$. Note that $\E{Q}{Y} = 1$ and $\E{}{e^{U}} = 1$. It follows from the duality formula that 
\begin{align*}
\kl{P}{Q} = \text{Ent}(Y) &\ge \E{}{U\,Y} = \E{}{\lambda(X - \E{Q}{X})Y} - \psi_{X- \E{Q}{X}}(\lambda)\\
&= \lambda(\E{P}{X} - \E{Q}{X})  - \psi_{X- \E{Q}{X}}(\lambda)
\end{align*} or equivalently
\begin{align*}
\psi_{X- \E{Q}{X}}(\lambda) &\ge \lambda(\E{P}{X} - \E{Q}{X})  - \kl{P}{Q},
\end{align*} therefore
\begin{align*}
\log\E{Q}{e^{\lambda(X- \E{Q}{X})}} &\ge \sup_{P \ll Q}\set{\lambda(\E{P}{X} - \E{Q}{X})  - \kl{P}{Q}}.
\end{align*} Conversely, setting
\begin{align*}
U &= \lambda\paren{X - \E{Q}{X}} - \sup_{P \ll Q}\set{\lambda(\E{P}{X} - \E{Q}{X})  - \kl{P}{Q}}
\end{align*} for every non-negative random variable $Y$ such that $\E{}{Y} = 1$,
\begin{align*}
\E{}{UY} \le \text{Ent}(Y).
\end{align*} Hence, $\E{}{e^{U}} \le 1$ by duality theorem, which means that 
\begin{align*}
\log \E{Q}{e^{\lambda\paren{X - \E{Q}{X}}}} &\le \sup_{P \ll Q}\set{\lambda(\E{P}{X} - \E{Q}{X})  - \kl{P}{Q}}. \qed
\end{align*}
\end{proof}

\item \begin{corollary}  (\textbf{Duality Formula of Kullback-Leibler Divergence}) \citep{thomas2006elements, boucheron2013concentration}\\
Let $P$ and $Q$ be two probability distributions on the same space.Then
\begin{align}
\kl{P}{Q} &= \sup_{X}\set{\E{P}{X} - \log \E{Q}{e^{X}} }, \label{eqn: duality_kl_divg}
\end{align} where the supremum is taken over all random variables such that $\E{Q}{\exp\paren{X}} < \infty$.
\end{corollary}
\begin{proof}
If $P \ll Q$, $\kl{P}{Q} = \text{Ent}(dP/dQ)$ and the corollary follows from the alternative formulation of the duality formula. Let $Y=dP/dQ$ and $X = \log(T)$ so that 
\begin{align*}
\kl{P}{Q} = \text{Ent}(Y) &= \sup_{T}\E{}{dP/dQ\paren{\log(T) - \log\paren{\E{}{T}}}} \\
&= \sup_{X}\set{\E{P}{X} - \log\E{Q}{e^{X}} }.
\end{align*}
If $P \not\ll Q$, then there exists an event $A$ such that $P(A) > 0 = Q(A)$, $\kl{P}{Q} = \infty$, and choosing $X_n = n\ind{A}$ and letting $n$ tend to infinity, we observe that the supremum on the right-hand side is infinite.\qed
\end{proof}

\item \begin{remark}
This corollary asserts that if $Q$ remains fixed, $\kl{P}{Q} $ is \emph{the \textbf{convex dual} of the functional}
$X \to \log\E{Q}{e^X}$.
\end{remark}

\item \begin{theorem} (\textbf{The Expected Value Minimizes Expected Bregman Divergence}) \citep{boucheron2013concentration} \\
Let $I \subseteq \bR$ be an open interval and let $f: I \to \bR$ be \textbf{convex} and \textbf{differentiable}. For any $x,y \in I$, \textbf{the Bregman divergence} of $f$ from $x$ to $y$ is $f(y) - f(x) - f'(x)(y-x)$. Let $X$ be an $I$-valued random variable. Then
\begin{align}
\E{}{f(X) - f(\E{}{X}) } &= \inf_{a \in I}\E{}{f(X) - f(a) - f'(a)(X-a)}\label{eqn: bregmann_inf}
\end{align}
\end{theorem}

\item \begin{corollary} (\textbf{Duality Formula of Entropy via Bregman Divergence}) \citep{boucheron2013concentration}\\
Let $X$ be a non-negative random variable such that $\E{}{\Phi(X)} < \infty$. Then 
\begin{align}
\text{Ent}(X) &= \inf_{u > 0}\E{}{X\paren{\log(X) - \log(u)} - (X - u) } \label{eqn: duality_entropy_3_breg}
\end{align}
\end{corollary}
\end{itemize}

\subsection{Wasserstein Distance and Information Inequality}
\begin{itemize}
\item \begin{proposition} (\textbf{Wasserstein Distance and Information Inequality}) \citep{boucheron2013concentration}\\
Let $X$ be a real-valued integrable random variable. Let $\phi$ be a \textbf{convex} and \textbf{continuously differentiable} function on a (possibly unbounded) interval $[0, b)$ and assume that $\phi(0) = \phi'(0) = 0$. Define, for every $x \ge 0$, \textbf{the Legendre transform} $\phi^{*}(x) = \sup_{\lambda \in (0,b)}(\lambda x - \phi(\lambda))$, and let, for every $t \ge 0$, $\phi^{*-1}(t) = \inf\{x \ge 0: \phi^{*}(x) > t\}$, i.e. the \textbf{the generalized inverse} of $\phi^{*}$. Then the following two statements are equivalent:
\begin{enumerate}
\item for every $\lambda \in (0,b)$,
\begin{align*}
\psi_{X - \E{}{X}}(\lambda) &\le \phi(\lambda)
\end{align*} where $\psi_{X}(\lambda):= \log\E{Q}{e^{\lambda X}}$ is the logarithm of moment generating function;
\item for any probability measure $P$ absolutely continuous with respect to $Q$ such that $\kl{P}{Q} < \infty$,
\begin{align}
\E{P}{X} - \E{Q}{X} &\le \phi^{*-1}\paren{\kl{P}{Q}}. \label{ineqn: information_inequality_general}
\end{align} 

In particular, given $\nu > 0$,
\begin{align*}
\psi_{X - \E{}{X}}(\lambda) &\le \frac{\nu\lambda^2}{2}
\end{align*} for every $\lambda >0$ \textbf{if and only if} for any probability measure $P$ absolutely continuous with respect to $Q$ and such that $\kl{P}{Q} < \infty$, 
\begin{align}
\E{P}{X} - \E{Q}{X} &\le \sqrt{2\nu\kl{P}{Q}}. \label{ineqn: information_inequality_sub_gaussian}
\end{align}
\end{enumerate}
\end{proposition}
\begin{proof}
As a direct consequence of Corollary \ref{coro: dual_log_mgf}, we see that (1) holds if and only if for every distribution $P \ll Q$, 
\begin{align*}
\psi_{X - \E{}{X}}(\lambda) &\le  \phi(\lambda) &&\\
\Leftrightarrow  \lambda\paren{\E{P}{X} - \E{Q}{X}} - \kl{P}{Q}  &\le  \phi(\lambda), && \forall P\ll Q\\
\Leftrightarrow \E{P}{X} - \E{Q}{X} &\le \frac{\phi(\lambda) + \kl{P}{Q}}{\lambda}, && \forall P\ll Q, \lambda \in (0,b)\\
\Leftrightarrow \E{P}{X} - \E{Q}{X} &\le \inf_{\lambda \in (0,b)}\set{\frac{\kl{P}{Q} + \phi(\lambda)}{\lambda}} && \forall P\ll Q
\end{align*}
Note that 
\begin{align*}
\phi^{*-1}(t) = \inf_{\lambda \in (0,b)}\brac{\frac{t + \phi(\lambda)}{\lambda}}
\end{align*} Setting $t= \kl{P}{Q}$, we have
\begin{align*}
\psi_{X - \E{}{X}}(\lambda) &\le  \phi(\lambda) \\
\Leftrightarrow \E{P}{X} - \E{Q}{X} &\le  \phi^{*-1}\paren{\kl{P}{Q}}.
\end{align*} which shows that (i) is equivalent to (ii). Applying the previous result with $\phi(\lambda) = \lambda^2 \nu/2$ for every $
\lambda > 0$ leads to the stated special case of equivalence since then $\phi^{*-1}(t) = \sqrt{2\nu t}$. \qed
\end{proof}

\item \begin{remark} (\emph{\textbf{The Quadratic Transportation Cost Inequality / The Information Inequality}}) \citep{boucheron2013concentration, wainwright2019high} \\
The inequality \eqref{ineqn: information_inequality} and \eqref{ineqn: information_inequality_sub_gaussian} are called \emph{\textbf{information inequality}} in \citep{wainwright2019high} due to the role of Kullback-Leibler Divergence in information theory. 

The inequality  \eqref{ineqn: information_inequality_sub_gaussian} is related to what is usually termed a \emph{\textbf{quadratic transportation cost inequality}}. If $\Omega$ is a \emph{metric space}, the probability measure $Q$ is said to satisfy a \emph{quadratic transportation cost inequality} if the last inequality holds for every $X$ which is \emph{Lipschitz} on $\Omega$ with \emph{Lipschitz norm} at most $1$. 
\begin{align}
\cW_1(P, Q) = \sup_{X \in \text{Lip}_1}\set{\E{P}{X} - \E{Q}{X} } &\le \sqrt{2\nu\kl{P}{Q}}. \label{ineqn: information_inequality_sub_gaussian_wasserstein}
\end{align} where $Lip_1 = \set{f \in \bR^{\Omega}: \abs{f(x) - f(y)} \le L\norm{x - y}{}, \;\; L\le 1}$.
\end{remark}
\end{itemize}

\subsection{Pinsker's Inequality}

\subsection{Birg{\'e}'s Inequality}

\subsection{The Brunn-Minkowski Inequality}
\newpage
\bibliographystyle{plainnat}
\bibliography{reference.bib}
\end{document}