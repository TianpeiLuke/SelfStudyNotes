\documentclass[11pt]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent %\usepackage{graphicx}
\usepackage{amsmath,amssymb, mathrsfs,  mathtools, dsfont}
\usepackage{tabularx}
\usepackage{tikz-cd}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{xcolor}
%\usepackage[linkbordercolor ={1 1 1} ]{hyperref}
%\usepackage[sf]{titlesec}
\usepackage{natbib}
%\usepackage{tikz-cd}

\usepackage{../../Tianpei_Report}

%\usepackage{appendix}
%\usepackage{algorithm}
%\usepackage{algorithmic}

%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}



\begin{document}
\title{Lecture 4: The Entropy Methods}
\author{ Tianpei Xie}
\date{Jan. 19th., 2023 }
\maketitle
\tableofcontents
\newpage
\section{Logarithmic Sobolev Inequality}
\subsection{Bernoulli Logarithmic Sobolev Inequality}
\begin{itemize}
\item \begin{remark} (\emph{Setting})\\
Consider a \emph{\textbf{uniformly distributed binary vector}} $Z = (Z_1 \xdotx{,} Z_n)$ on the hypercube $\set{-1, +1}^n$. In other words, the components of $X$ are \emph{independent}, \emph{identically distributed} \emph{\textbf{random sign (Rademacher) variables}} with $\bP\set{Z_i = -1} = \bP\set{Z_i = +1} = 1/2$ (i.e. \emph{symmetric Bernoulli random variables}). 

Let $f: \set{-1, +1}^n \to \bR$ be a real-valued function on \emph{\textbf{binary hypercube}}. $X := f(Z)$ is an induced real-valued random variable. Define $\widetilde{Z}^{(i)} = (Z_1 \xdotx{,} Z_{i-1}, Z_{i}', Z_{i+1} \xdotx{,} Z_n)$ be the sample $Z$ with $i$-th component replaced by an \emph{independent copy} $Z_{i}'$. Since $Z, \widetilde{Z}^{(i)} \in \set{-1, +1}^n$, $\widetilde{Z}^{(i)} = (Z_1 \xdotx{,} Z_{i-1}, -Z_{i}, Z_{i+1} \xdotx{,} Z_n)$, i.e. \emph{the $i$-th sign is \textbf{flipped}}. Also denote the $i$-th \emph{Jackknife sample} as $Z_{(i)} = (Z_1 \xdotx{,} Z_{i-1},  Z_{i+1} \xdotx{,} Z_n)$ by \emph{leaving out} the $i$-th component. $\E{(-i)}{X} := \E{}{X | Z_{(i)}}$.

Denote the $i$-th component of \emph{\textbf{discrete gradient}} of $f$ as
\begin{align*}
\nabla_{i}f(z) &:= \frac{1}{2}\paren{f(z) - f(\widetilde{z}^{(i)})}
\end{align*} and $\nabla f(z) = (\nabla_{1}f(z) \xdotx{,} \nabla_{n}f(z) )$
\end{remark}

\item \begin{remark} (\emph{\textbf{Jackknife Estimate of Variance}})\\
Recall that \emph{the \textbf{Jackknife estimate of variance}}
\begin{align*}
\cE(f) &:= \E{}{\sum_{i=1}^{n}\paren{f(Z) - \E{(-i)}{f(\widetilde{Z}^{(i)})} }^2}\\
& = \frac{1}{2}\E{}{\sum_{i=1}^{n}\paren{f(Z) - f(\widetilde{Z}^{(i)}) }^2}.
\end{align*} Using the notation of discrete gradient of $f$, we see that
\begin{align*}
\cE(f) &:= 2\E{}{\norm{\nabla f(Z)}{2}^2}
\end{align*}
\end{remark}

\item \begin{remark} (\emph{\textbf{Entropy Functional}})\\
Recall that the entropy functional for $f$ is defined as
\begin{align*}
H_{\Phi}(f(Z)) = \text{Ent}(f)&:=  \E{}{f(Z)\log f(Z)} - \E{}{f(Z)}\log\paren{\E{}{f(Z)}}. 
\end{align*}
\end{remark}

\item \begin{proposition} (\textbf{Logarithmic Sobolev Inequality for Rademacher Random Variables}). \citep{boucheron2013concentration}\\
If $f: \set{-1, +1}^n \to \bR$ be an arbitrary real-valued function  defined on the $n$-dimensional \textbf{binary hypercube} and assume that $Z$ is \textbf{uniformly} \textbf{distributed} over $\set{-1, +1}^n$. Then
\begin{align}
\text{Ent}(f^2) &\le \cE(f) \label{ineqn: log_sobolev_inequality_binary_cube} \\
\Leftrightarrow \text{Ent}(f^2(Z)) &\le 2\E{}{\norm{\nabla f(Z)}{2}^2}  \label{ineqn: log_sobolev_inequality_binary_cube_2}
\end{align}
\end{proposition}
\begin{proof}
The key is to apply the tensorization property of $\Phi$-entropy. Let $X = f(Z)$. By tensorization property,
\begin{align*}
\text{Ent}(X^2) &\le \sum_{i=1}^{n}\E{}{\text{Ent}_{(-i)}(X^2) } 
\end{align*} where $\text{Ent}_{(-i)}(X^2) := \E{(-i)}{X^2\log X^2} - \E{(-i)}{X^2}\log\paren{\E{(-i)}{X^2}}$.

It thus suffice to show that for all $i=1\xdotx{,} n$,
\begin{align*}
\text{Ent}_{(-i)}(X^2) &\le \frac{1}{2}\E{(-i)}{\paren{f(Z) - f(\widetilde{Z}^{(i)}) }^2}.
\end{align*} Given any fixed realization of $Z_{(-i)}$, $X = f(Z) = \widetilde{f}(Z_i)$ can only takes two different values with equal probability. Call these two values $a$ and $b$. See that  
\begin{align*}
\text{Ent}_{(-i)}(X^2) &= \frac{1}{2}a^2\log a^2 + \frac{1}{2}b^2\log b^2 - \frac{1}{2}(a^2 + b^2)\log\paren{\frac{a^2 + b^2}{2}} \\
\frac{1}{2}\E{(-i)}{\paren{f(Z) - f(\widetilde{Z}^{(i)}) }^2} &= \frac{1}{2}\paren{a - b}^2.
\end{align*} Thus we need to show
\begin{align*}
\frac{1}{2}a^2\log a^2 + \frac{1}{2}b^2\log b^2 - \frac{1}{2}(a^2 + b^2)\log\paren{\frac{a^2 + b^2}{2}} &\le \frac{1}{2}\paren{a - b}^2. 
\end{align*} By symmetry, we may assume that $a \ge b$. Since $(\abs{a} - \abs{b})^2 \le (a-b)^2$, without loss of generality, we may further assume that $a, b \ge 0$.

Define 
\begin{align*}
h(a) := \frac{1}{2}a^2\log a^2 + \frac{1}{2}b^2\log b^2 - \frac{1}{2}(a^2 + b^2)\log\paren{\frac{a^2 + b^2}{2}} -  \frac{1}{2}\paren{a - b}^2 
\end{align*} for $a \in [b, \infty)$. $h(b) = 0$. It suffice to check that $h'(b) = 0$ and that $h$ is concave on $[b, \infty)$.
Note that
\begin{align*}
h'(a) &= a\log a^2 + 1 - a\log\paren{\frac{a^2 + b^2}{2}} - 1 - (a-b)\\
&= a\log\frac{2a^2}{(a^2 + b^2)} - (a-b).
\end{align*} So $h'(b) = 0$. Moreover, 
\begin{align*}
h''(a) &= \log\frac{2a^2}{(a^2 + b^2)} + 1 - \frac{2a^2}{(a^2 + b^2)} \le 0
\end{align*} due to inequality $\log(x) + 1 \le x$. \qed
\end{proof}

\item \begin{remark} (\emph{\textbf{Logarithmic Sobolev Inequality $\Rightarrow$  Efron-Stein Inequality}}). \citep{boucheron2013concentration}\\
Note that for $f$ non-negative, 
\begin{align*}
\text{Var}(f(Z)) &\le \text{Ent}(f^2(Z)).
\end{align*} Thus \emph{logarithmic Sobolev inequality} \eqref{ineqn: log_sobolev_inequality_binary_cube} implies
\begin{align*}
\text{Var}(f(Z)) &\le  \cE(f) 
\end{align*} which is \emph{the Efron-Stein inequality}.
\end{remark}

\item \begin{corollary} (\textbf{Logarithmic Sobolev Inequality for Asymmetric Bernoulli Random Variables}). \citep{boucheron2013concentration}\\
If $f: \set{-1, +1}^n \to \bR$ be an arbitrary real-valued function and $Z = (Z_1 \xdotx{,} Z_n) \in \set{-1, +1}^n$ with $p= \bP\set{Z_i = +1}$. Then
\begin{align}
\text{Ent}(f^2) &\le c(p)\E{}{\norm{\nabla f(Z)}{2}^2}   \label{ineqn: log_sobolev_inequality_binary_cube_asym}
\end{align} where 
\begin{align*}
c(p) &= \frac{1}{1 - 2p}\log\frac{1-p}{p}
\end{align*} Note that $\lim\limits_{p \to 1/2}c(p) = 2$.
\end{corollary}
\end{itemize}

\subsection{Gaussian Logarithmic Sobolev Inequality}
\begin{itemize}
\item \begin{proposition} (\textbf{Gaussian Logarithmic Sobolev Inequality}). \citep{boucheron2013concentration}\\
Let $f: \bR^n \to \bR$ be a \textbf{continuous differentiable} function and let $Z = (Z_1 \xdotx{,} Z_n)$ be a vector of $n$ \textbf{independent} \textbf{standard Gaussian} random variables. Then
\begin{align}
\text{Ent}(f^2(Z)) &\le 2\E{}{\norm{\nabla f(Z)}{2}^2}.  \label{ineqn: log_sobolev_inequality_gaussian}
\end{align}
\end{proposition}
\begin{proof}
We first prove for $n=1$, where $f: \bR \rightarrow \bR$ is continuous differentiable and $Z$ is standard Gaussian distribution. Without loss of generality, assume that $\E{}{f'(Z)} < \infty$ since it is trivial when  $\E{}{f'(Z)} = \infty$. By density argument, it suffice to prove the proposition when $f$ is \emph{twice differentiable with bounded support}.

Now let $\epsilon_1 \xdotx{,} \epsilon_n$ be \emph{independent Rademacher random variables} and introduce
\begin{align*}
S_n := \frac{1}{\sqrt{n}}\sum_{j=1}^{n}\epsilon_j.
\end{align*} 
Note that $\epsilon_i \in \set{-1, +1}$ with equal probability, thus
\begin{align*}
\E{(-i)}{S_n} &= \frac{1}{2}\brac{ \paren{\frac{1}{\sqrt{n}}\sum_{j \neq i}\epsilon_j + \frac{1}{\sqrt{n}}} + \paren{\frac{1}{\sqrt{n}}\sum_{j \neq i}\epsilon_j - \frac{1}{\sqrt{n}}}  } \\
&= \frac{1}{2}\brac{\paren{S_n + \frac{1 - \epsilon_i}{\sqrt{n}}} + \paren{S_n - \frac{1 + \epsilon_i}{\sqrt{n}}}}.
\end{align*} In the proof of Gaussian Poincar{\'e} inequality, we show that by \emph{central limit theorem}, 
\begin{align*}
\limsup\limits_{n\to \infty}\E{}{\sum_{i=1}^{n}\abs{f\paren{S_n}  - f\paren{S_n - \frac{2\epsilon_i}{\sqrt{n}} }}^2}
&= 4\E{}{(f'(Z))^2}. 
\end{align*}
On the other hands, for any \emph{continuous uniformly bounded function} $f$, by \emph{central limit theorem}, 
\begin{align*}
\lim_{n \to \infty}\text{Ent}\paren{f^2(S_n)} = \text{Ent}(f^2(Z))
\end{align*} The proof is then completed by invoking \emph{the logarithmic Sobolev inequality} for \emph{Rademacher random variables}
\begin{align*}
\text{Ent}\paren{f^2(S_n)} &\le \frac{1}{2}\E{}{\sum_{i=1}^{n}\abs{f\paren{S_n}  - f\paren{S_n - \frac{2\epsilon_i}{\sqrt{n}} }}^2} \\
\Rightarrow  \lim_{n \to \infty}\text{Ent}\paren{f^2(S_n)} &\le \frac{1}{2}\lim_{n \to \infty}\E{}{\sum_{i=1}^{n}\abs{f\paren{S_n}  - f\paren{S_n - \frac{2\epsilon_i}{\sqrt{n}} }}^2} \\
\Rightarrow  \text{Ent}(f^2(Z))&\le 2 \E{}{(f'(Z))^2}.
\end{align*} The extension of the result to dimension $n \ge 1$ follows easily from \emph{the sub-additivity of entropy} which states that
\begin{align*}
\text{Ent}(f^2) &\le \sum_{i=1}^{n}\E{}{\E{(-i)}{f^2(Z)\log f^2(Z)} -\E{(-i)}{f^2(Z)}\log \E{(-i)}{f^2(Z)} } 
\end{align*} where $\E{(-i)}{\cdot}$ denotes the integration with respect to $i$-th variable $Z_i$ only. Thus by induction, for all $i$
\begin{align*}
\E{(-i)}{f^2(Z)\log f^2(Z)} -\E{(-i)}{f^2(Z)}\log \E{(-i)}{f^2(Z)} &\le 2 \E{(-i)}{(\partial_i f(Z))^2}.
\end{align*} Thus
\begin{align*}
\text{Ent}(f^2) &\le 2  \E{}{\E{(-i)}{\sum_{i=1}^{n}(\partial_i f(Z))^2}} = 2\E{}{\norm{\nabla f(Z)}{2}^2}. \qed
\end{align*}
\end{proof}



\item \begin{remark} (\textbf{\emph{Dimension Free Property}}).\\
\emph{The Gaussian logarithmic Sobolev inequality} has a constant $C = 2$ that is \emph{\textbf{independent of dimension}} $n$:
\begin{align*}
\E{\mu}{f^2} &\le 2\E{\mu}{\norm{\nabla f}{2}^2}.
\end{align*} This \emph{dimension-free property} is related to \emph{the \textbf{concentration} of \textbf{Gaussian measure} $\mu$}. As a consequence, this inequality can be extended to functions of \emph{Gaussian measure} on \emph{\textbf{infinite dimensional space}}, such as Gibbs measure, \emph{Gaussian process} etc.
\end{remark}

\item \begin{remark}(\textbf{\emph{Equivalent Form of Gaussian Logarithmic Sobolev Inequality}})\\
Assume $f: \bR^n \to (0, \infty)$ and $\int_{\bR^n} f d\mu = 1$ under Gaussian measure $\mu$. Substituting $f \to \sqrt{f}$,  \emph{the logarithmic Sobolev inequality} becomes
\begin{align}
\text{Ent}_{\mu}(f) = \int f\log f d\mu &\le \frac{1}{2}\, \int \frac{\norm{\nabla f}{2}^2}{f} d\mu \label{ineqn: log_sobolev_inequality_gaussian_v2} 
\end{align} 
\end{remark}

\item \begin{remark} (\textbf{\emph{Gaussian Logarithmic Sobolev Inequality $\Rightarrow$  Gaussian Poincar{\'e} Inequality}}). \citep{boucheron2013concentration}\\
Recall that \emph{the Gaussian Poincar{\'e} inequality}
\begin{align*}
\text{Var}(f(Z)) &\le \E{}{\norm{\nabla f(Z)}{2}^2}
\end{align*} Since
\begin{align*}
(1+t)\log(1+t) = t + \frac{t^2}{2} + o(t^2)
\end{align*} as $t\to 0$, we can get for Gaussian measures,
\begin{align*}
\text{Ent}_{\mu}(1+\epsilon h) &= \frac{\epsilon^2}{2}\text{Var}_{\mu}(h) + o(\epsilon^2).
\end{align*}
Similarly, 
\begin{align*}
\int \frac{\norm{\nabla(1+\epsilon h)}{2}^2}{1+\epsilon h} d\mu &= \epsilon^2 \int \norm{\nabla h}{2}^2 d\mu  + o(\epsilon^2).
\end{align*}
Thus from \emph{the Gaussian logarithmic Sobolev inequality},
\begin{align*}
\text{Ent}_{\mu}(1+\epsilon h) &\le \frac{1}{2}\int \frac{\norm{\nabla(1+\epsilon h)}{2}^2}{1+\epsilon h} d\mu  \\
\Leftrightarrow \frac{\epsilon^2}{2}\text{Var}_{\mu}(h) + o(\epsilon^2) &\le \frac{\epsilon^2}{2} \int \norm{\nabla h}{2}^2 d\mu  + o(\epsilon^2)\\
\Leftrightarrow \text{Var}(f(Z)) &\le \E{}{\norm{\nabla f(Z)}{2}^2} \qquad \text{as } \epsilon \to 0.
\end{align*} Thus \emph{the Gaussian logarithmic Sobolev inequality} implies \emph{the Gaussian Poincar{\'e} inequality}.
\end{remark}
\end{itemize}

\subsection{Information Theory Interpretation}
\begin{itemize}
\item \begin{remark} (\textbf{\emph{Information Interpretation of Gaussian Logarithmic Sobolev Inequality}})\\
Let $\nu,\mu$ be two \emph{probability measures} on $(\cX^n, \srF)$, $\mu = \mu_1 \xdotx{\otimes} \mu_n$ and $\nu \ll \mu$. Define $f:= \frac{d\nu}{d\mu}$ be \emph{the Radon-Nikodym derivative} of $\nu$ with respect to $\mu$ (i.e $f$ is the \emph{probability density function} of $\nu$ with respect to $\mu$). Then the entropy becomes \textbf{\emph{the relative entropy}}
\begin{align*}
\text{Ent}_{\mu}(f) &:= \E{\mu}{f \log f} = \kl{\nu}{\mu}
\end{align*} since $\E{\mu}{f} = \int_{\cX^n} fd\mu = 1$.

On the other hand, \underline{\emph{\textbf{the (relative) Fisher information}}} is defined as 
\begin{align*}
I(\nu \,\|\, \mu) &:= \E{\nu}{\norm{\nabla \log f}{2}^2} \\
&= \int \norm{\frac{\nabla f}{f}}{2}^2  d\nu =  \int \frac{\norm{\nabla f}{2}^2}{f^2}  d\nu \\
&= \int \frac{\norm{\nabla f}{2}^2}{f}  d\mu
\end{align*}
Thus \underline{\emph{\textbf{the information interpretation}}} of \emph{the Gaussian logarithmic Sobolev inequality} is
\begin{align}
\kl{\nu}{\mu} &\le \frac{1}{2}I(\nu \,\|\, \mu) \label{ineqn: log_sobolev_inequality_gaussian_infor} 
\end{align} where $\mu$ is a \emph{Gaussian measure} and $\nu \ll \mu$ with density function $f$.
Note that \emph{the Fisher information metric is \textbf{the Riemannian metric} induced by the relative entropy}.
\end{remark}
\end{itemize}

\subsection{Logarithmic Sobolev Inequality for General Probability Measures}
\begin{itemize}
\item From functional analysis, we have \emph{the Sobolev inequality}, 
\begin{remark} (\emph{\textbf{The Sobolev Inequality}}) \citep{evans2010partial}\\
\emph{\textbf{The Sobolev inequality}} states for smooth function $f: \bR^n \to \bR$ in \emph{Sobolev space} where $n \ge 3$ and $p = \frac{2n}{n-2} > 2$
\begin{align*}
\norm{f}{p}^2 &\le C_n \,\int_{\bR^{n}}\abs{\nabla f}^2 dx.
\end{align*} The inequality is sharp when the constant
\begin{align*}
C_n &:= \frac{1}{\pi n(n-2)}\paren{\frac{\Gamma(n)}{\Gamma(n/2)}}^{2/n}
\end{align*}
\end{remark}

\item \begin{proposition} (\textbf{Euclidean Logarithmic Sobolev Inequality}). \\
Let $f: \bR^n \to \bR$ be a smooth function and $m$ be Lebesgue measure on $\bR^n$, then
\begin{align}
\text{Ent}_{m}(f^2) &\le \frac{n}{2}\log\paren{\frac{2}{n \pi e} \E{m}{\norm{\nabla f}{2}^2} } \label{ineqn: log_sobolev_inequality_euclidean} \\
\Leftrightarrow \int f^2 \log\paren{\frac{f^2}{\int f^2 dx}} dx &\le \frac{n}{2}\log\paren{\frac{2}{n \pi e} \int \abs{\nabla f}^2 dx }
\nonumber
\end{align}
\end{proposition}

\item \begin{definition} (\textbf{\emph{Logarithmic Sobolev Inequality for General Probability Measure}}). \\
\emph{A probability measure} $\mu$ on $\bR^n$ is said to satisfy the \underline{\emph{\textbf{logarithmic Sobolev inequality}}}  for some constant $C > 0$ if for any smooth function $f$
\begin{align}
\text{Ent}_{\mu}(f^2) &\le C\, \E{\mu}{\norm{\nabla f}{2}^2} \label{ineqn: log_sobolev_inequality_general}
\end{align} holds for any \textbf{\emph{continuous differentiable}} function $f: \bR^n \to \bR$.  The left-hand side is called \textbf{\emph{the entropy functional}}, which is defined as
\begin{align*}
\text{Ent}(f^2) &:= \E{\mu}{f^2 \log f^2} - \E{\mu}{f^2}\log\E{\mu}{f^2} \\
&= \int f^2 \log\paren{\frac{f^2}{\int f^2 d\mu}} d\mu.
\end{align*} The right-hand side is defined as
\begin{align*}
\E{\mu}{\norm{\nabla f}{2}^2} &= \int \norm{\nabla f}{2}^2 d\mu.
\end{align*} Thus we can rewrite \emph{the logarithmic Sobolev inequality} in \emph{functional form}
\begin{align}
\int f^2 \log\paren{\frac{f^2}{\int f^2 d\mu}} d\mu &\le C \int \norm{\nabla f}{2}^2 d\mu  \label{ineqn: log_sobolev_inequality_general_functional_form} 
\end{align}
\end{definition}

\item \begin{remark}(\textbf{\emph{Logarithmic Sobolev Inequality}})\\
For non-negative function $f$, we can replace $f \to \sqrt{f}$, so that \emph{the logarithmic Sobolev inequality} becomes
\begin{align}
\text{Ent}_{\mu}(f) &\le C \int \frac{\norm{\nabla f}{2}^2}{f} d\mu \label{ineqn: log_sobolev_inequality_general_v2} 
\end{align} 
\end{remark}

\item \begin{remark}(\textbf{\emph{Modified Logarithmic Sobolev Inequality via Convex Cost and Duality}})\\
For some \emph{\textbf{convex non-negative cost}} $c: \bR^n \to \bR_{+}$, \emph{\textbf{the convex conjugate}} of $c$ (Legendre transform of $c$) is defined as
\begin{align*}
c^{*}(x) := \sup_{y}\set{\inn{x}{y} - c(y) }
\end{align*}
Then we can obtain \emph{\textbf{the modified logarithmic Sobolev inequality}}
\begin{align}
\text{Ent}_{\mu}(f) &\le \int f^2\, c^{*}\paren{\frac{\nabla f}{f}} d\mu \label{ineqn: log_sobolev_inequality_general_modified} 
\end{align} 
\end{remark}
\end{itemize}






\section{The Entropy Methods}
\subsection{Herbst's Argument}
\begin{itemize}
%\item \begin{remark} 
%Recall that the variational formulation of entropy is
%\begin{align*}
%\text{Ent}(X)&= \inf_{u > 0}\E{}{X\paren{\log(X) - \log(u)} - (X - u) }
%\end{align*}
%\end{remark}

%\item \begin{remark} (\textbf{\emph{Tensorization Property of Entropy Functional}})\\
%Let $\mu = \mu_1 \xdotx{\otimes} \mu_n$ be the probability distribution for $Z = (Z_1 \xdotx{,} Z_n)$ on $(\cX^n, \srF)$. For any measurable function $f: \cX^n \to \bR$, let $X = f(Z_1 \xdotx{,} Z_n)$ so that $\E{}{X\log X} < \infty$.  The \emph{sub-additivity of entropy function (i.e. the tensorization property)} states that 
%\begin{align*}
%\text{Ent}_{\mu_1 \xdotx{\otimes} \mu_n}(f) &\le \E{\mu_1 \xdotx{\otimes} \mu_n}{\sum_{i=1}^{n}\text{Ent}_{\mu_i}(f)}
%\end{align*} where the subscript $\mu_i$ indicates that the integration concerns the $i$-th variable only.
%\end{remark}

\item \begin{remark} (\emph{\textbf{Entropy Functional for Moment Generating Function}})\\
Let $X = e^{\lambda Z}$ where $Z$ is a random variable. The entropy function of $X$ becomes
\begin{align*}
\text{Ent}(e^{\lambda Z}) &= \E{}{\lambda Z e^{\lambda Z}} - \E{}{e^{\lambda Z}}\log\paren{\E{}{e^{\lambda Z}}}
\end{align*} Denote $\psi_{Z - \E{}{Z}}(\lambda) := \log\E{}{e^{\lambda (Z - \E{}{Z})}}$. Then
\begin{align*}
\psi_{Z - \E{}{Z}}'(\lambda) &= \frac{d}{d\lambda}\log\E{}{e^{\lambda (Z - \E{}{Z})}}\\
&= \frac{1}{\E{}{e^{\lambda (Z - \E{}{Z})}}}\E{}{\paren{Z - \E{}{Z}} e^{\lambda (Z - \E{}{Z})}}\\
&= \frac{1}{\E{}{e^{\lambda Z}}}e^{\lambda \E{}{Z}}\E{}{\paren{Z - \E{}{Z}} e^{\lambda (Z - \E{}{Z})}}\\
&= \frac{1}{\E{}{e^{\lambda Z}}}\E{}{\paren{Z - \E{}{Z}} e^{\lambda Z}}\\
\lambda\; \psi_{Z - \E{}{Z}}'(\lambda) &= \frac{1}{\E{}{e^{\lambda Z}}}\paren{\E{}{\lambda Z e^{\lambda Z}} - \E{}{\lambda Z} \E{}{e^{\lambda Z}}} 
\end{align*} 
\begin{align*}
\Rightarrow \lambda\; \psi_{Z - \E{}{Z}}'(\lambda) - \psi_{Z - \E{}{Z}}(\lambda) &= \frac{1}{\E{}{e^{\lambda Z}}}\set{\E{}{\lambda Z e^{\lambda Z}} - \E{}{\lambda Z} \E{}{e^{\lambda Z}} - \E{}{e^{\lambda Z}} \log\E{}{e^{\lambda (Z - \E{}{Z})}}}\\
&=  \frac{1}{\E{}{e^{\lambda Z}}}\left\{\E{}{\lambda Z e^{\lambda Z}} - \E{}{\lambda Z} \E{}{e^{\lambda Z}} \right. \\
&\quad \left. + \E{}{e^{\lambda Z}}\E{}{\lambda Z} - \E{}{e^{\lambda Z}} \log\E{}{e^{\lambda Z}}\right\} \\
&= \frac{1}{\E{}{e^{\lambda Z}}}\set{\E{}{\lambda Z e^{\lambda Z}} - \E{}{e^{\lambda Z}} \log\E{}{e^{\lambda Z}}}\\
&= \frac{\text{Ent}(e^{\lambda Z})}{\E{}{e^{\lambda Z}}}
\end{align*}

Thus we have
\begin{align}
\frac{\text{Ent}(e^{\lambda Z})}{\E{}{e^{\lambda Z}}} &= \lambda\; \psi_{Z - \E{}{Z}}'(\lambda) - \psi_{Z - \E{}{Z}}(\lambda). \label{eqn: entropy_log_mgf_differential}
\end{align} 

Our strategy is based on using \eqref{eqn: entropy_log_mgf_differential} \emph{the sub-additivity of entropy} and then univariate calculus to derive \emph{\textbf{upper bounds} for the \textbf{derivative} of $\psi(\lambda)$}. By solving the obtained \emph{\textbf{differential inequality}}, we obtain tail bounds via \emph{Chernoff's bounding}.
\end{remark}

\item \begin{proposition} (\textbf{Herbst's Argument}) \citep{boucheron2013concentration, wainwright2019high}\\
Let $Z$ be an integrable random variable such that for some $\nu > 0$, we have, for every $\lambda > 0$,
\begin{align}
\frac{\text{Ent}(e^{\lambda Z})}{\E{}{e^{\lambda Z}}} &\le \frac{\nu \lambda^2}{2} \label{ineqn: herbst_argument}
\end{align} Then, for every $\lambda >0$, the logarithmic moment generating function of centered random variable $(Z - \E{}{Z})$ satisfies
\begin{align*}
\psi_{Z - \E{}{Z}}(\lambda) := \log\E{}{e^{\lambda (Z - \E{}{Z})}} &\le \frac{\nu \lambda^2}{2} .
\end{align*}
\end{proposition}
\begin{proof}
The condition of the proposition means, via \eqref{eqn: entropy_log_mgf_differential}, that
\begin{align*}
\lambda\; \psi_{Z - \E{}{Z}}'(\lambda) - \psi_{Z - \E{}{Z}}(\lambda) &\le \frac{\nu \lambda^2}{2},
\end{align*} or equivalently,
\begin{align*}
\frac{1}{\lambda}\psi_{Z - \E{}{Z}}'(\lambda) - \frac{1}{\lambda^2}\psi_{Z - \E{}{Z}}(\lambda) &\le \frac{\nu}{2}.
\end{align*} Setting $G(\lambda) = \lambda^{-1}\psi_{Z - \E{}{Z}}(\lambda)$, we see that the differential inequality becomes
\begin{align*}
G'(\lambda) &\le \frac{\nu}{2}.
\end{align*} Since $G(\lambda) \to 0$ as $\lambda \to 0$, which implies that
\begin{align*}
G(\lambda) &\le \frac{\nu \lambda}{2},
\end{align*} and the result follows. \qed
\end{proof}

%\item \begin{remark}
%If we assume $G(\lambda) = \lambda^{-1}\log \E{}{e^{\lambda Z}}$ we see that by \emph{L' Hopital's rule}, 
%\begin{align*}
%G(0) = \lim\limits_{\lambda \to 0}\frac{\psi(\lambda)}{\lambda} &= \psi'(0) = \frac{\frac{d}{d\lambda}\E{}{e^{\lambda Z}}|_{\lambda =0}}{\E{}{e^{\lambda Z}}|_{\lambda =0}} = \E{}{Z}.
%\end{align*} Thus solving the differential inequality, we have
%\begin{align*}
%G(\lambda) = G(0) + \int_{0}^{\lambda}G'(u)du &\le \E{}{Z} + \frac{\nu \lambda}{2}\\
%\Rightarrow \bP\set{Z - \E{}{Z} \ge t} &\le \frac{e^{\lambda G(\lambda) }}{e^{\lambda(\E{}{Z} + t)}} \le e^{-\lambda t + \frac{\nu \lambda^2}{2}}
%\end{align*}
%
%\end{remark}

\item \begin{remark} (\textbf{\emph{Entropy Methods}})\\
\emph{The \textbf{key} strategy of \textbf{entropy methods}} to prove the concentration of function $f(Z)$ of independent variables $Z$  is as follows
\begin{enumerate}
\item First we \emph{\textbf{bound the entropy}} for each individual variables $Z_i$ conditioning on the rest of them, i.e. $Z_{(-i)} := (Z_1 \xdotx{,} Z_{i-1}, Z_{i+1} \xdotx{,} Z_n)$ or  $\widetilde{Z}_{i} := (Z_1 \xdotx{,} Z_{i-1}, Z_{i}', Z_{i+1} \xdotx{,} Z_n)$.  Typically, we use \emph{\textbf{the exponential form}} $e^{\lambda f(Z)}$. The followings are methods that can be used in this step
\begin{itemize}
\item If $Z$ is \emph{Gaussian} or \emph{Bernoulli distribution},  we can use \emph{\textbf{the logarithmic Sobolev inequality}} 
\begin{align*}
\text{Ent}_{\mu_i}(e^{ \lambda f(Z)}) &\le 2 \E{\mu_i}{\norm{\nabla e^{\lambda f(Z)/2}}{2}^2} = \frac{\lambda^2}{2}\E{\mu_i}{e^{\lambda f(Z)} \norm{\nabla f(Z)}{2}^2}.
\end{align*}
\item In general, we can also use \emph{the \textbf{variational formulation of entropy}} 
\begin{align*}
\text{Ent}_{\mu_i}(e^{\lambda f(Z)}) &\le  \E{\mu_i}{e^{\lambda f(Z)}(\lambda (f(Z) - f(\widetilde{Z}_{i})) - (e^{\lambda  f(Z)} - e^{\lambda f(\widetilde{Z}_{i})})}
\end{align*} 
\item If we can control \emph{\textbf{the Jackknife estimate of variance of $f(Z)$}}
\begin{align*}
\sum_{i=1}^{n}\paren{f(Z) - f(\widetilde{Z}_{i})}^2 \le \nu
\end{align*} then we can bound the above variational formulation since $e^{s} - e^{t} \le e^{t}(s - t)$ where $s > t$.
\end{itemize}

\item Apply \emph{\textbf{the tensorization of property}} of \emph{\textbf{the entropy functional}} on $f(Z)$ 
\begin{align*}
\text{Ent}_{\mu_1 \xdotx{\otimes} \mu_n}(e^{\lambda f(Z)}) &\le \E{\mu_1 \xdotx{\otimes} \mu_n}{\sum_{i=1}^{n}\text{Ent}_{\mu_i}(e^{\lambda f(Z)})}
\end{align*} where $\mu := \mu_1 \xdotx{\otimes} \mu_n$ is the distribution of $Z$ and the subscript $\mu_i$ indicates that the integration concerns $Z_i$ only.
\item By \emph{\textbf{the Herbst's argument}}, we obtain a \emph{\textbf{differential inequality}} for \emph{\textbf{the logarithmic moment generating function}} $\psi(\lambda)$
\begin{align*}
\frac{\text{Ent}_{\mu_1 \xdotx{\otimes} \mu_n}(e^{\lambda f(Z)})}{\E{\mu_1 \xdotx{\otimes} \mu_n}{e^{\lambda f(Z)}}} = \lambda \psi'(\lambda) - \psi(\lambda) \le \phi(\lambda) \Leftrightarrow \paren{\frac{\psi(\lambda)}{\lambda}}' \le \frac{\phi(\lambda)}{\lambda} \\
\frac{\psi(\lambda)}{\lambda} \le \lim\limits_{\lambda \to 0}\paren{\frac{\psi(\lambda)}{\lambda}} + \int_{0}^{\lambda} \paren{\frac{\phi(u)}{u}}du = \E{}{f(Z)} + \int_{0}^{\lambda} \frac{\phi(u)}{u}du
\end{align*}

\item Obtain concentration results based on \textbf{\emph{the Cram{\'e}r-Chernoff Method}}
\begin{align*}
\bP\set{f(Z) - \E{}{f(Z)} > t} &\le \inf_{\lambda >0 }\exp\paren{\psi(\lambda) - \lambda t} \le \inf_{\lambda >0 }\exp\paren{ \lambda \int_{0}^{\lambda} \frac{\phi(u)}{u}du - \lambda t}
\end{align*}
\end{enumerate}
\end{remark}

  

\end{itemize}




\subsection{Modified Logarithmic Sobolev Inequalities}
\begin{itemize}
\item \begin{proposition} (\textbf{A Modified Logarithmic Sobolev Inequalities for Moment Generating Function}) \citep{boucheron2013concentration}\\
Consider independent random variables $Z_1 \xdotx{,} Z_n$ taking values in $\cX$, a real-valued function $f: \cX^n \to \bR$ and the random variable $X = f(Z_1 \xdotx{,} Z_n)$. Also denote $Z_{(-i)}= (Z_1 \xdotx{,} Z_{i-1}, Z_{i+1} \xdotx{,} Z_n)$ and $X_{(-i)} = f_i(Z_{(-i)})$ where $f_i: \cX^{n-1} \to \bR$ is an arbitrary function. Let $\phi(x) = e^x -x -1$.
Then for all $\lambda \in \bR$,
\begin{align}
\lambda \E{}{Xe^{\lambda X}} - \E{}{e^{\lambda X}}\log\E{}{e^{\lambda X}} &\le \sum_{i=1}^{n}\E{}{e^{\lambda X}\phi(-\lambda(X - X_{(-i)}))}\label{ineqn: log_sobolev_inequality_mgf}
\end{align}
\end{proposition}
\begin{proof}
Recall the tensorization of entropy
\begin{align*}
\text{Ent}_{\mu_1 \xdotx{\otimes} \mu_n}(Y) &\le \E{\mu_1 \xdotx{\otimes} \mu_n}{\sum_{i=1}^{n}\text{Ent}_{\mu_i}(Y)}.
\end{align*}  We bound each term on the right-hand side by the variational formulation of entropy
\begin{align*}
\text{Ent}_{\mu_i}(Y) &\le \E{\mu_i}{Y(\log Y - \log u) - (Y -u)}
\end{align*} for any $u > 0$. Let $u = Y_{(-i)} = g_i(Z_{(-i)})$. We have
\begin{align*}
\text{Ent}_{\mu_i}(Y) &\le \E{\mu_i}{Y(\log Y - \log Y_{(-i)}) - (Y - Y_{(-i)})}.
\end{align*}Applying above inequality to the variable $Y = e^{\lambda X}$ and $Y_{(-i)} = e^{\lambda X_{(-i)}}$, one obtain
\begin{align*}
\text{Ent}_{\mu_i}(e^{\lambda X}) &\le \E{\mu_i}{e^{\lambda X}(\log e^{\lambda X} - \log e^{\lambda X_{(-i)}}) - (e^{\lambda X} - e^{\lambda X_{(-i)}})}\\
&= \E{\mu_i}{e^{\lambda X}(\lambda (X - X_{(-i)}) - (e^{\lambda X} - e^{\lambda X_{(-i)}})}\\
&= \E{\mu_i}{e^{\lambda X}\paren{\lambda (X - X_{(-i)}) - e^{-\lambda X}(e^{\lambda X} - e^{\lambda X_{(-i)}})}}\\
&= \E{\mu_i}{e^{\lambda X}\paren{\lambda (X - X_{(-i)}) + e^{-\lambda (X - X_{(-i)})} -1}}\\
&= \E{\mu_i}{e^{\lambda X}\phi\paren{-\lambda (X - X_{(-i)})}}
\end{align*} where $\phi(x) = e^x -x -1$. Thus the proof is completed. \qed
\end{proof}


\item \begin{proposition} (\textbf{Symmetrized Modified Logarithmic Sobolev Inequalities}) \citep{boucheron2013concentration}\\
Consider independent random variables $Z_1 \xdotx{,} Z_n$ taking values in $\cX$, a real-valued function $f: \cX^n \to \bR$ and the random variable $X = f(Z_1 \xdotx{,} Z_n)$. Also denote $\widetilde{X}^{(i)} = f(Z_1 \xdotx{,} Z_{i-1}, Z_i', Z_{i+1} \xdotx{,} Z_n)$. Let $\phi(x) = e^x -x -1$.
Then for all $\lambda \in \bR$,
\begin{align}
\lambda \E{}{Xe^{\lambda X}} - \E{}{e^{\lambda X}}\log\E{}{e^{\lambda X}} &\le \sum_{i=1}^{n}\E{}{e^{\lambda X}\phi(-\lambda(X - \widetilde{X}^{(i)}))}\label{ineqn: log_sobolev_inequality_sym_mgf}
\end{align} Moreover, denoting $\tau(x) = x(e^x - 1)$, for all $\lambda \in \bR$,
\begin{align*}
\lambda \E{}{Xe^{\lambda X}} - \E{}{e^{\lambda X}}\log\E{}{e^{\lambda X}} &\le \sum_{i=1}^{n}\E{}{e^{\lambda X}\tau(-\lambda(X - \widetilde{X}^{(i)})_{+})},\\
\lambda \E{}{Xe^{\lambda X}} - \E{}{e^{\lambda X}}\log\E{}{e^{\lambda X}} &\le \sum_{i=1}^{n}\E{}{e^{\lambda X}\tau(\lambda(\widetilde{X}^{(i)} - X)_{-})}.
\end{align*}
\end{proposition}
\begin{proof}
Note that $X_{(-i)}$ and $\widetilde{X}^{(i)}$ are both independent from $Z_i$. The first inequality is the same as the proposition above. For the second inequality, use the fact that
\begin{align*}
\E{\mu_i}{e^{\lambda X}\phi\paren{\lambda (\widetilde{X}^{(i)} - X)_{+}}} &= \E{\mu_i}{e^{\lambda \widetilde{X}^{(i)} }\phi\paren{\lambda (X - \widetilde{X}^{(i)})_{+}}}\\
&= \E{\mu_i}{e^{\lambda X}e^{-\lambda ( X- \widetilde{X}^{(i)}) }\phi\paren{\lambda (X - \widetilde{X}^{(i)})_{+}}}.
\end{align*} and
\begin{align*}
\E{\mu_i}{e^{\lambda X}\phi\paren{-\lambda (\widetilde{X}^{(i)} - X)}} &= \E{\mu_i}{e^{\lambda X}\phi\paren{-\lambda (\widetilde{X}^{(i)} - X)_{+}}} + \E{\mu_i}{e^{\lambda X}\phi\paren{\lambda (\widetilde{X}^{(i)} - X)_{+}}}\\
&= \E{\mu_i}{e^{\lambda X}\set{\phi\paren{-\lambda (\widetilde{X}^{(i)} - X)} + e^{-\lambda ( X- \widetilde{X}^{(i)}) }\phi\paren{\lambda (X - \widetilde{X}^{(i)})_{+}}}}.
\end{align*} Finally note that $\phi(x) + e^x \phi(-x) = \tau(x) = x(e^x - 1)$.\qed
\end{proof}

\item \begin{remark}
Consider
\begin{align*}
h(u) &= (1+ u)\log(1 + u) - u, \quad u \ge -1.
\end{align*}  \emph{The convex conjugate function} of $h(u)$ is
\begin{align*}
h^{*}(x) &= \sup_{u \ge -1}\set{x u - h(u)} \\
&=  \sup_{u \ge -1}\set{x u - (1+ u)\log(1 + u) + u} \\
&= e^x - x - 1 = \phi(x)
\end{align*} where $1+ u^{*}= e^{x}$. 
\end{remark}

\end{itemize}

\subsection{Bounded Difference Inequality via Entropy Methods}
\begin{itemize}
\item \begin{remark}
Given vectors $x, x' \in \cX^n$ and an index $k \in \set{1, 2 \xdotx{,} n}$, we define a new vector $x^{(-k)} \in \cX^n$ via
\begin{align*}
x_j^{(-k)} &= \left\{\begin{array}{cc}
x_j & j \neq k\\
x_k'& j = k
\end{array}
\right.
\end{align*}
With this notation, we say that $f: \cX^n \to \bR$ satisfies \underline{\textbf{\emph{the bounded difference inequality}}} with parameters $(L_1 \xdotx{,} L_n)$ if, for each index $k = 1, 2 \xdotx{,} n$,
\begin{align}
\abs{f(x) - f(x^{(-k)})} \le L_k, \quad\text{ for all }x, x' \in \cX^n. \label{eqn: bounded_difference_property}
\end{align}
\end{remark}

\item \begin{proposition} (\textbf{McDiarmid's Inequality / Bounded Differences Inequality})\citep{boucheron2013concentration, wainwright2019high}\\
Suppose that $f$ satisfies \textbf{the bounded difference property} \eqref{eqn: bounded_difference_property} with parameters $(L_1 \xdotx{,} L_n)$ and that the random vector $X = (X_1, X_2 \xdotx{,} X_n)$ has \textbf{independent} components. Denote
\begin{align*}
\nu &= \frac{1}{4} \sum_{k=1}^{n}L_k^2
\end{align*}
Then
\begin{align*}
\bP\set{f(X) - \E{}{f(X)} \ge t } &\le   \exp\paren{- \frac{t^2}{2 \nu}}. 
\end{align*}
\end{proposition}
\begin{proof}
Recall that for a random variable $Y$ taking its values in $[a, b]$, then we know from \emph{Hoeffding's Lemma} that the logarithmic moment generating functions $\psi(\lambda)$ satisfies
\begin{align*}
\psi(\lambda)'' = \text{Var}(Y) \le \frac{(b-a)^2}{4}
\end{align*} for every $\lambda \in \bR$. Hence, Hoeffding's inequality is obtained since
\begin{align*}
\frac{\text{Ent}(e^{\lambda Y})}{\E{}{e^{\lambda Y}}} = \lambda \psi'(\lambda) - \psi(\lambda) = \int_{0}^{\lambda} s \psi''(s) ds \le \frac{(b-a)^2}{4}  \int_{0}^{\lambda} s ds =  \frac{(b-a)^2 \lambda^2}{8},
\end{align*} Note that by the bounded differences assumption, given $X_{(-i)}$, $f(X)$ is a random variable whose range is in an interval of length at most $L_i$, so 
\begin{align*}
\frac{\text{Ent}_{(-i)}(e^{\lambda f(X)})}{\E{(-i)}{e^{\lambda f(X)}}} &\le  \frac{L_i^2 \lambda^2}{8} 
\end{align*} 
From the  tensorization property of entropy, we can bound the entropy of total function
\begin{align*}
\text{Ent}(e^{\lambda f(X)}) \le \E{}{\sum_{i=1}^{n}\text{Ent}_{(-i)}(e^{\lambda f(X)})} &\le \sum_{i=1}^{n} \frac{L_i^2 \lambda^2}{8} \E{}{\E{(-i)}{e^{\lambda f(X)}}}\\
\frac{\text{Ent}(e^{\lambda f(X)}) }{ \E{}{e^{\lambda f(X)}} } &\le \frac{\sum_{i=1}^{n}L_i^2 \lambda^2}{8} = \frac{\nu \lambda^2}{2}.
\end{align*} Using \emph{Herbst's argument}, it leads to the bound of logarithmic moment generating function:
\begin{align*}
\psi_{f(X)}(\lambda) &\le \frac{\nu \lambda^2}{2}.
\end{align*} Finally, we apply \emph{the Chernoff's inequality}
\begin{align*}
\bP\set{f(X) - \E{}{f(X)} \ge t } &\le  \inf_{\lambda >0 }\exp\paren{\psi_{f(X)}(\lambda) - \lambda t} \le \exp\paren{- \frac{t^2}{2 \nu}}.  \qed
\end{align*}
\end{proof}

\item \begin{remark} (\textbf{\emph{Relaxation of Bounded Difference Conditions}})\\
Consider $X := f(Z)$ where $Z:= (Z_1 \xdotx{,} Z_{n})$ and let 
\begin{align*}
\widetilde{X}_{i} &= \inf_{z_i'} f(Z_1 \xdotx{,} Z_{i-1}, z_{i}', Z_{i+1} \xdotx{,} Z_n)
\end{align*} We can relax the condition defining \emph{the bounded difference function} by the following \emph{boundedness condition on variance}
\begin{align}
&\sum_{i=1}^{n}\paren{X - \widetilde{X}_{i}}^2 \le \nu  \nonumber\\
&\sum_{i=1}^{n}\paren{f(Z_1 \xdotx{,} Z_{n}) - \inf_{z_i'} f(Z_1 \xdotx{,} Z_{i-1}, z_{i}', Z_{i+1} \xdotx{,} Z_n) }^2 \le \nu  \label{ineqn: bounded_variance_condition}
\end{align} The quantity $\nu$ may be interpreted as an \emph{\textbf{upper bound}} for \emph{\textbf{the Efron-Stein estimate}} of the \emph{\textbf{variance}} $\text{Var}(f(Z))$. 
\end{remark}

\item \begin{proposition} \label{prop: bounded_variance_concentration} (\textbf{Concentration with Bounded Variance})\citep{boucheron2013concentration}\\
Assume that $X$ is such that there exists a constant $\nu > 0$ such that, almost surely,
\begin{align*}
\sum_{i=1}^{n}\paren{X - \widetilde{X}_{i}}^2 \le \nu.
\end{align*} Then for all $t > 0$,
\begin{align}
\bP\set{X - \E{}{X} \ge t } &\le   \exp\paren{- \frac{t^2}{2 \nu}}. 
\end{align}
\end{proposition}
\begin{proof}
The result follows easily from \emph{the modified logarithmic Sobolev inequality} proved in the previous section. Observe that for $x > 0$, 
\begin{align*}
\phi(-x) :=e^{-x} + x -1 &\le \frac{x^2}{2},
\end{align*}
 and therefore, for all $\lambda > 0$, 
 \begin{align*}
\text{Ent}(e^{\lambda X})  =  \lambda \E{}{Xe^{\lambda X}} - \E{}{e^{\lambda X}}\log\E{}{e^{\lambda X}} &\le \sum_{i=1}^{n}\E{}{e^{\lambda X}\phi(-\lambda(X - \widetilde{X}_{i}))}\\
 &\le \E{}{e^{\lambda X} \sum_{i=1}^{n}\frac{\lambda^2}{2} \paren{X - \widetilde{X}_{i}}^2}\\
 &\le \frac{\nu \lambda^2}{2} \E{}{e^{\lambda X} }
 \end{align*} where we used the assumption of the theorem. The obtained inequality has the same form as the one we already faced in the proof of bounded difference inequality and the proof may be finished in an identical way. \qed
\end{proof}
\end{itemize}

\subsection{Poisson Logarithmic Sobolev Inequality}
\begin{itemize}
\item \begin{proposition} (\textbf{Modified Logarithmic Sobolev Inequality for Bernoulli Random Variable}). \citep{boucheron2013concentration}\\
Let $f: \set{0, 1} \to (0,\infty)$ be a \textbf{non-negative} real-valued function defined on the binary set $\set{0, 1}$. Define \textbf{the discrete derivative} of $f$ at $x \in \set{0, 1}$ by 
\begin{align*}
\nabla f := f(1 - x) - f(x).
\end{align*} Let $X$ be a Bernoulli random variable with parameter $p \in (0,1)$ (i.e. $\bP\set{X = 1} =p$). Then
\begin{align}
\text{Ent}(f(X)) &\le (p(1-p))\E{}{\nabla f(X) \nabla \log f(X)}.  \label{ineqn: log_sobolev_inequality_bernoulli_v0}
\end{align}
and
\begin{align}
\text{Ent}(f(X)) &\le  (p(1-p))\E{}{\frac{\abs{\nabla f(X)}^2}{f(X)}}.  \label{ineqn: log_sobolev_inequality_bernoulli}
\end{align}
\end{proposition}


\item \begin{proposition} (\textbf{Poisson Logarithmic Sobolev Inequality}). \citep{boucheron2013concentration}\\
Let $f: \bN \to (0,\infty)$ be a \textbf{non-negative} real-valued function defined on the set of non-negative integers $\bN$. Define \textbf{the discrete derivative} of $f$ at $x \in \bN$ by 
\begin{align*}
\nabla f := f(x + 1) - f(x).
\end{align*} Let $X$ be a Poisson random variable. Then
\begin{align}
\text{Ent}(f(X)) &\le (\E{}{X})\E{}{\nabla f(X) \nabla \log f(X)}.  \label{ineqn: log_sobolev_inequality_poisson_v0}
\end{align}
and
\begin{align}
\text{Ent}(f(X)) &\le (\E{}{X})\E{}{\frac{\abs{\nabla f(X)}^2}{f(X)}}.  \label{ineqn: log_sobolev_inequality_poisson}
\end{align}
\end{proposition}
\end{itemize}



\section{Applications}
\subsection{Concentration of Bounded Difference Functions on the Hypercube}
\begin{itemize}
\item \begin{proposition} (\textbf{Bounded Difference Functions of Symmetric Bernoulli Variables}) \citep{boucheron2013concentration} \\
Let $Z = (Z_1 \xdotx{,} Z_n)$ be a vector of $n$ \textbf{independent} \textbf{symmetric Bernoulli} random variables, i.e. \textbf{uniformly distributed} in binary hypercube $\set{-1, +1}^n$.  Let $f : \set{-1, 1}^n \to \bR$ be an arbitary function.  Let $\nu > 0$ be such that
\begin{align*}
\sum_{i=1}^{n}\paren{f(z) - f(\widetilde{z}_{i})}^2 \le \nu.
\end{align*} for all $z, \widetilde{z}_{i} \in \set{-1, +1}^n$, where $\widetilde{z}_{i} = (z_1 \xdotx{,} z_{i-1}, z_{i}', z_{i+1} \xdotx{,} z_n)$ is obtained by replacing $i$-th component of  $z$  by an independent copy of $z_{i}= -z_i$. Then the random variable $X = f(Z)$ satisfies, for all $t > 0$,
\begin{align}
\bP\set{X - \E{}{X} \ge t} &\le \exp\paren{-\frac{t^2}{\nu}} \label{ineqn: fun_sym_bernoulli}\\
\bP\set{X - \E{}{X} \le -t} &\le \exp\paren{-\frac{t^2}{\nu}} \nonumber
\end{align}
\end{proposition}
\begin{proof}
Apply the \emph{logarithmic Sobelev inequality} for \emph{symmetric Bernoulli random variable} when $g = e^{\lambda f/2}$
\begin{align*}
\text{Ent}(e^{\lambda f(Z)}) &\le \cE(e^{\lambda f/2}) = \frac{1}{2}\sum_{i=1}^{n}\E{}{\paren{e^{\lambda f(Z) /2} - e^{\lambda f(\widetilde{Z}_{i}) /2}}^2} \\
&= \sum_{i=1}^{n}\E{}{\paren{e^{\lambda f(Z) /2} - e^{\lambda f(\widetilde{Z}_{i}) /2}}_{+}^2} 
\end{align*} By convexity, $e^{s/2} - e^{t/2} \le e^{t/2}(s - t)/2$ where $s > t$. So
\begin{align*}
\text{Ent}(e^{\lambda f(Z)}) &\le \frac{\lambda^2}{4}\sum_{i=1}^{n}\E{}{e^{\lambda f(Z) /2}\paren{f(Z)  - f(\widetilde{Z}_{i})}_{+}^2}\\
&=  \frac{\lambda^2}{4}\E{}{e^{\lambda f(Z)}\sum_{i=1}^{n}\paren{f(Z)  - f(\widetilde{Z}_{i})}_{+}^2} \\
&\le \frac{\nu \lambda^2}{4}\E{}{e^{\lambda f(Z) }}
\end{align*} where the last inequality is from the bounded variance assumption. Thus
\begin{align*}
\frac{\text{Ent}(e^{\lambda f(Z)})}{\E{}{e^{\lambda f(Z) }}} &\le \frac{\nu \lambda^2}{4}.
\end{align*} By \emph{Herbst's argument}, the logarithmic moment generating function 
\begin{align*}
\psi(\lambda) = \log \E{}{e^{\lambda (f(Z) - \E{}{f(Z)})}} &\le \frac{\nu \lambda^2}{4}.
\end{align*} Finally, by Chernoff bound,
\begin{align*}
\bP\set{f(Z) - \E{}{f(Z)} \ge t} &\le \inf_{\lambda >0}\exp\paren{ \psi(\lambda) - \lambda t } \le \inf_{\lambda >0}\exp\paren{ \frac{\nu \lambda^2}{4} - \lambda t } = \exp\paren{- \frac{t^2}{\nu}}
\end{align*} where $\lambda^{*} = \frac{2t}{ \nu}$. \qed
\end{proof}

\item \begin{remark}
Recall that by \emph{the Efron-Stein inequality}, $\text{Var}(f(Z)) \le \nu/2$. The theorem states much
more: \emph{\textbf{tail probabilities} decrease similarly to \textbf{the tail probabilities of a Gaussian random variable} with \textbf{variance} $\nu/2$}.  The price we pay for such an improved inequality is that a \emph{\textbf{pointwise control}} of 
\begin{align*}
\sum_{i=1}^{n}\paren{f(z) - f(\widetilde{z}_{i})}^2 \le \nu.
\end{align*} is required, while to \emph{bound the variance} it suffices to keep its \emph{\textbf{expected value}} under control. 
\end{remark}

\item \begin{proposition} (\textbf{Functions of Asymmetric Bernoulli Variables}) \citep{boucheron2013concentration} \\
Let $Z = (Z_1 \xdotx{,} Z_n)$ be a vector of $n$ \textbf{independent} \textbf{Bernoulli} random variables with parameter $p$ (i.e. $\bP\set{Z_i = 1} = p$).  Let $f : \set{-1, 1}^n \to \bR$ be an arbitary function.  Let $\nu > 0$ be such that
\begin{align*}
\sum_{i=1}^{n}\paren{f(z) - f(\widetilde{z}_{i})}^2 \le \nu.
\end{align*} for all $z, \widetilde{z}_{i} \in \set{-1, +1}^n$, where $\widetilde{z}_{i} = (z_1 \xdotx{,} z_{i-1}, z_{i}', z_{i+1} \xdotx{,} z_n)$ is obtained by replacing $i$-th component of  $z$  by an independent copy of $z_{i}' = -z_i$. If $f$ is \textbf{nondecreasing} in all of its components,  then the random variable $X = f(Z)$ satisfies, for all $t > 0$,
\begin{align}
\bP\set{X - \E{}{X} \ge t} &\le \exp\paren{-\frac{t^2}{(1-p)c(p)\nu}} \label{ineqn: fun_nonsym_bernoulli_nondecreasing}
\end{align} If $f$ is \textbf{nonincreasing} in all of its components, 
\begin{align}
\bP\set{X - \E{}{X} \ge t} &\le \exp\paren{-\frac{t^2}{pc(p)\nu}} \label{ineqn: fun_nonsym_bernoulli_nonincreasing}
\end{align} where 
\begin{align*}
c(p) := \frac{1}{1 - 2p}\log \frac{1-p}{p}
\end{align*} and $\lim\limits_{p \to 1/2}c(p) = 2$.
\end{proposition}
\end{itemize}

\subsection{Lipschitz Functions of Gaussian Variables}
\begin{itemize}
\item \begin{theorem} (\textbf{Rademacher Theorem}).\\
If $f: U \to \bR$ is a $L$-Lipschitz function where $U \subseteq \bR^n$, then $f$ is \textbf{differentiable almost everywhere} in $U$ and \textbf{the essential supremum} of the \textbf{norm} of its \textbf{derivative}  is \textbf{bounded} by its \textbf{Lipschitz constant}. 
\end{theorem}

\item \begin{theorem} (\textbf{Lipschitz Functions of Gaussian Variables}) \citep{boucheron2013concentration} \\
Let $Z = (Z_1 \xdotx{,} Z_n)$ be a vector of $n$ \textbf{independent} \textbf{standard normal} random variables. Let $f : \bR^n \to \bR$ denote an \textbf{$L$-Lipschitz function}, that is, there exists a constant $L > 0$ such that for all $x, y \in \bR^n$,
\begin{align*}
\abs{f(x) - f(y)} &\le  L\norm{x - y}{}.
\end{align*} Then, for all $\lambda \in \bR$,
\begin{align}
\psi_{f(Z) - \E{}{f(Z)}}(\lambda) := \log \E{}{e^{\lambda\paren{f(Z) - \E{}{f(Z)}}}} &\le \frac{L^2 \lambda^2}{2} \label{ineqn: lip_fun_gaussian}
\end{align}
\end{theorem}
\begin{proof}
By a standard density argument we may assume that $f$ is \emph{differentiable} with \emph{gradient uniformly bounded by $L$} according to \emph{Rademacher theorem}. We may also assume, without loss of generality, that $\E{}{f(Z)} = 0$.  Using the \emph{Gaussian logarithmic Sobolev inequality} for the function $e^{\lambda f/2}$, we obtain
\begin{align*}
\text{Ent}(e^{\lambda f}) &\le 2 \E{}{\norm{\nabla e^{\lambda f/2}}{2}^2} \\
&= 2 \E{}{\norm{\frac{\lambda}{2}e^{\lambda f/2}  \nabla f}{2}^2} \\
&= \frac{\lambda^2}{2} \E{}{e^{\lambda f} \norm{ \nabla f}{2}^2} \\
&\le  \frac{\lambda^2}{2} \E{}{e^{\lambda f}} L^2 \\
\Rightarrow  \frac{\text{Ent}(e^{\lambda f}) }{\E{}{e^{\lambda f}}} &\le \frac{L^2 \lambda^2}{2}
\end{align*} By Herbst's argument, this implies that $\lambda \psi'(\lambda) - \psi(\lambda) \le \frac{L^2 \lambda^2}{2}$. Let $G(\lambda) := \lambda^{-1}\psi(\lambda)$, we have  $G'(\lambda) \le L^2/2$, so we have
\begin{align*}
\psi_{f(Z)}(\lambda) &\le   \frac{L^2 \lambda^2}{2} \qed
\end{align*}
\end{proof}

\item \begin{theorem}  (\textbf{Gaussian Concentration Inequality / The Tsirelson-Ibragimov-Sudakov Inequality}) \citep{boucheron2013concentration, wainwright2019high} \\
Let $Z = (Z_1 \xdotx{,} Z_n)$ be a vector of $n$ \textbf{independent} \textbf{standard normal} random variables. Let $f : \bR^n \to \bR$ denote an \textbf{$L$-Lipschitz function}. Then, for all $t > 0$, 
\begin{align}
\bP\set{f(Z) - \E{}{f(Z)} \ge t } &\le \exp\paren{- \frac{t^2}{2 L^2}}. \label{ineqn: gaussian_concentration_inequality}
\end{align}
\end{theorem}
\begin{proof}
The previous theorem states that $f(Z)$ is \emph{sub-Gaussian distributed} with parameter $\nu = L^2$. By Markov's inequality, 
\begin{align*}
\bP\set{f(Z) - \E{}{f(Z)} \ge t } = \bP\set{e^{\lambda (f(Z) - \E{}{f(Z)})} \ge e^{\lambda t} }&\le \frac{\E{}{e^{\lambda f(Z) - \E{}{f(Z)}}}}{e^{\lambda t}} \\
&\le \inf_{\lambda >0}\exp\paren{\psi(\lambda) - \lambda t} \\
&\le  \inf_{\lambda >0}\exp\paren{\frac{L^2 \lambda^2}{2}  - \lambda t} \\
&=  \exp\paren{-\frac{t^2}{2L^2}}
\end{align*} where $\lambda^{*} = \frac{t}{L^2}$. \qed
\end{proof}

\item \begin{remark} (\textbf{\emph{Dimension-Free Concentration}}) \\
\emph{An \textbf{important} feature} of the theorem is that the right-hand side \emph{\textbf{does not depend on the dimension $n$}}. This inequality has served as a benchmark for the development of concentration inequalities during the last three decades. 
\end{remark}
\end{itemize}



\subsection{Suprema of Gaussian Process}
\begin{itemize}
\item \begin{definition} (\textbf{\emph{Gaussian Process}})\\
Let $T$ be a \emph{metric space}. \emph{A stochastic process} $(X_t)_{t \in T}$ is a \underline{\emph{\textbf{Gaussian process} indexed by $T$}} if for  \emph{any finite collection} $\set{t_1 \xdotx{,}, t_n} \subset T$, the vector $(X_{t_1} \xdotx{,} X_{t_n})$ has a \emph{jointly Gaussian distribution}.

In addition, we assume that $T$ is \emph{\textbf{totally bounded}} (i.e. for every $t > 0$ it can be covered by \emph{finitely many balls} of radius $t$) and that the \emph{Gaussian process} is \emph{\textbf{almost surely continuous}}, that is, with probability $1$, $X_t$ is a \emph{continuous function} of $t$.
\end{definition} 

\item \begin{theorem} (\textbf{Concentration of Suprema of Gaussian Process}) \citep{boucheron2013concentration, vershynin2018high,  wainwright2019high, gine2021mathematical}\\
Let  $(X_t)_{t \in T}$ be an \textbf{almost surely continuous} \textbf{centered Gaussian process} indexed by a \textbf{totally bounded} set $T$. If
\begin{align*}
\sigma^2 &:= \sup_{t \in T}\E{}{X_t^2},
\end{align*} then $Z = \sup_{t \in T}X_t$ satisfies $\text{Var}(Z) \le \sigma^2$, and for all $u > 0$,
\begin{align}
\bP\set{Z - \E{}{Z} \ge u} &\le  \exp\paren{- \frac{u^2}{2 \sigma^2}}  \label{ineqn: gaussian_process_sup_inquality_upper}
\end{align} and
\begin{align}
\bP\set{\E{}{Z} - Z \ge u} &\le  \exp\paren{- \frac{u^2}{2 \sigma^2}}  \label{ineqn: gaussian_process_sup_inquality_upper}
\end{align}
\end{theorem}
\begin{proof}
We assume that $T$ is a \emph{\textbf{finite set}}. The extension to arbitrary \emph{totally bounded} $T$ is based on a \emph{\textbf{separability argument}} and \emph{\textbf{monotone convergence}}. We may assume, for simplicity, that $T = \set{1 \xdotx{,} n}$. Let $\Sigma$ be the covariance matrix of the centered Gaussian vector $X = (X_1 \xdotx{,} X_n)$. Denote by $A$ the \textit{square root} of the positive semidefinite matrix $\Gamma$. If $Y = (Y_1 \xdotx{,} Y_n)$ is a
vector of \emph{independent standard normal random variables}, then 
\begin{align*}
f(Y) &= \max_{i=1 \xdotx{,} n}\paren{A Y}_{i}
\end{align*} has the same distribution as $Z = \max_{i=1 \xdotx{,} n}X_i$.  Hence, we can apply \emph{the Gaussian concentration inequality} by \emph{bounding the Lipschitz constant of $f$}. By \emph{the Cauchy-Schwarz inequality}, for all $u, v \in \bR^n$ and $i = 1 \xdotx{,} n$,
\begin{align*}
\abs{(Au)_i - (Av)_i} &\le \norm{A_{i:}}{2} \norm{u  - v}{2}
\end{align*} Since $\norm{A_{i:}}{2}^2 = \sum_{j}A_{i,j}^2 = \text{Var}(X_i)$, we get
\begin{align*}
\abs{f(u) - f(v)} &\le \max_{i=1 \xdotx{,} n}\abs{(Au)_i - (Av)_i} \le \sigma \norm{u  - v}{2}
\end{align*} Therefore, $f$ is \emph{Lipschitz with constant} $\sigma$ and the tail bounds follow from the Gaussian concentration inequality. The variance bound follows from \emph{the Gaussian Poincar{\'e} inequality}. \qed
\end{proof}

\end{itemize}
\subsection{Concentration of Convex Lipschitz Functions}
\begin{itemize}
\item \begin{theorem} (\textbf{Concentration of Convex Lipschitz Functions on Unit Hypercube}) \citep{boucheron2013concentration}\\
Let  $Z:= (Z_1 \xdotx{,} Z_{n})$ be independent random variables taking values in the interval $[0, 1]$ and let $f : [0, 1]^n \to \bR$ be a \textbf{separately convex function} (i.e. $f$ is convex in each coordinate while the others are fixed) such that
\begin{align*}
\abs{f(x) - f(y)} &\le \norm{x - y}{} \quad \text{for all }x, y \in [0, 1]^n.
\end{align*}
Then $X = f(Z_1 \xdotx{,} Z_{n})$ satisfies, for all $t > 0$,
\begin{align}
\bP\set{f(Z) - \E{}{f(Z)} \ge t } &\le \exp\paren{- \frac{t^2}{2}}. \label{ineqn: convex_lipschitz_concentration}
\end{align}
\end{theorem}
\begin{proof}
We may assume without loss of generality that the partial derivatives of $f$ exist. (Otherwise one may \emph{approximate} $f$ by a smooth function via a standard argument.) It suffices to bound the random variable 
\begin{align*}
\sum_{i=1}^{n}\paren{X - \widetilde{X}_{i}}^2
\end{align*}
where $\widetilde{X}_{i} = \inf_{z_i'} f(Z_1 \xdotx{,} Z_{i-1}, z_{i}', Z_{i+1} \xdotx{,} Z_n)$. Recall that for \emph{separately convex} \emph{differentiable} function $f$ on \emph{compact domain} $[0,1]$, we have
\begin{align*}
\sum_{i=1}^{n}\paren{X - \widetilde{X}_{i}}^2 &= \sum_{i=1}^{n}\paren{f(Z) - f(\widetilde{Z}_{i})}^2  && \text{( \emph{by continuity of $f$ on compact domain})} \\
&\le \sum_{i=1}^{n}\paren{\partdiff{f}{z_i}(Z)}^2\paren{Z - \widetilde{Z}_{i}}^2 && \text{( \emph{by separetely convexity})}\\
&\le \sum_{i=1}^{n}\paren{\partdiff{f}{z_i}(Z)}^2 && (\text{ by domain $[0,1]$ })\\
&= \norm{\nabla f(Z)}{2}^2 && \\
&\le 1 && (\text{\emph{ by Lipschitz property}})
\end{align*} Therefore, we can apply the Proposition \ref{prop: bounded_variance_concentration} with variance bounded above by $\nu = 1$. \qed
\end{proof}
\end{itemize}

\subsection{Exponential Tail Bounds for Self-Bounding Functions}

\subsection{Hypercontractivity for Boolean Polynomials}
\subsection{Gaussian Hypercontractivity}



\newpage
\bibliographystyle{plainnat}
\bibliography{reference.bib}
\end{document}