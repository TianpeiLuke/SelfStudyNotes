\documentclass[11pt]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent %\usepackage{graphicx}
\usepackage{amsmath,amssymb, mathrsfs,  mathtools, dsfont}
\usepackage{tabularx}
\usepackage{tikz-cd}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{xcolor}
%\usepackage[linkbordercolor ={1 1 1} ]{hyperref}
%\usepackage[sf]{titlesec}
\usepackage{natbib}
%\usepackage{tikz-cd}

\usepackage{../../Tianpei_Report}

%\usepackage{appendix}
%\usepackage{algorithm}
%\usepackage{algorithmic}

%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}



\begin{document}
\title{Lecture 6: Concentration via Optimal Transport}
\author{ Tianpei Xie}
\date{Jan. 24th., 2023 }
\maketitle
\tableofcontents
\newpage
\section{Optimal Transport Basis}
\subsection{Optimal Transport Problem and its Dual Problem}
\begin{itemize}
\item \begin{definition} (\textbf{\emph{Pushforward Measure}})  \citep{gabriel2019computational} \\
Let $(\cX, \srB_X)$ and $(\cY, \srB_Y)$ be two topological measurable spaces.  Denote the spaces of  \emph{general (Radon) measures} on $\cX, \cY$  as $\cM(\cX)$ and $\cM(\cY)$. Also let  $\cC(\cX)$ be space of continuous functions on $\cX$. For a \emph{continous} map $T : \cX \rightarrow \cY$,  the \underline{\textbf{\emph{push-forward operator}}} is defined as $T_{\#}: \cM(\cX) \rightarrow \cM(\cY)$ that  satisfies 
\begin{align}
\forall\, h\in \cC(\cX), \quad \int_{\cY}h(y)\; d \paren{T_{\#}\alpha}(y) &= \int_{\cX}h(T(x))\; d\alpha(x). \label{eqn: def_push_forward_opt}\\
\text{or equivalently, } \quad \paren{T_{\#}\alpha}(B)&:= \alpha\paren{\set{x: T(x) \in B \subset \cY }} = \alpha(T^{-1}(B))  \label{eqn: def_push_forward_opt2}
\end{align} where the \textbf{\emph{push-forward measure}} $\beta := T_{\#}\alpha \in \cM(\cY)$ of some $\alpha \in  \cM(\cX)$, $T^{-1}(\cdot)$ is the pre-image of $T$.
\end{definition}

\item \begin{remark} (\textbf{\emph{Density Function of Pushforward Measure}})\\
Assume that $(\alpha, \beta)$ have densities $(\rho_{\alpha}, \rho_{\beta})$ with respect to a fixed measure, and $\beta = T_{\#}\alpha$. We see that $T_{\#}$ acts on a density $\rho_{\alpha}$ linearly to a density $\rho_{\beta}$ as a change of variable, i.e. 
\begin{align}
\rho_{\alpha}(\mb{x}) &= \abs{\det(T'(\mb{x}))}\rho_{\beta}(T(\mb{x}))  \label{eqn: density of push-forward distribution}\\
\abs{\det(T'(\mb{x}))} &= \frac{\rho_{\alpha}(\mb{x}) }{\rho_{\beta}(T(\mb{x})) } \nonumber
\end{align}
\end{remark}


\item \begin{definition} (\textbf{\emph{Optimal Transport Problem, Monge Problem}}) \citep{villani2009optimal, santambrogio2015optimal, gabriel2019computational} \\
Let $(\cX, \srB_X)$ and $(\cY, \srB_Y)$ be two measurable spaces, where $\cX$ and $\cY$ are \emph{complete separable metric spaces}. Denote $\cP(\cX)$ and $\cP(\cY)$ as the space of probabiilty measures on $\cX$ and $\cY$. Define a \emph{\textbf{cost function}} $c: \cX \times \cY \to \bR_{+}$ as non-negative real-valued measurable functions on $\cX \times \cY$. \underline{\emph{\textbf{The optimal transport problem}}} by \emph{Monge} (i.e. \underline{\emph{\textbf{Monge Problem}}}) is defined as follows: given two probability measures $\bP \in \cP(\cX)$ and $\bQ \in \cP(\cY)$, find a \emph{continuous measurable map} $T: \cX \to \cY$ so that 
\begin{align*}
\inf_{T} & \int_{\cX} c(x, T(x)) d\bP(x) \\
\text{s.t. }& \bQ = T_{\#}\bP
\end{align*} The optimal solution $T$ is also called an \emph{\textbf{optimal transportation plan}}.
\end{definition}

\item \begin{definition} (\textbf{\emph{Optimal Transport Problem, Kantorovich Relaxation}}) \citep{villani2009optimal, santambrogio2015optimal, gabriel2019computational} \\
 \underline{\emph{\textbf{The optimal transport problem}}} by \emph{Kantorovich} (i.e. \underline{\emph{\textbf{Kantorovich Relxation}}}) is defined as follows: given two probability measures $\bP \in \cP(\cX)$ and $\bQ \in \cP(\cY)$, find a \emph{joint probability measure} $\gamma \in \Pi(\bP, \bQ)$ so that 
\begin{align*}
\inf_{\gamma}  & \int_{\cX \times \cY} c(x, y) d\gamma(x, y) \\
\text{s.t. }&\gamma \in \Pi(\bP, \bQ) := \set{\gamma \in \cP(\cX \times \cY): \pi_{\cX, \#}\gamma = \bP, \; \pi_{\cY, \#}\gamma = \bQ }
\end{align*} where $\cP(\cX \times \cY)$ is the space of joint probability measure on $\cX \times \cY$, $\pi_{\cX}$ and $\pi_{\cY}$ are the coordinate projection onto $\cX$ and $\cY$. $\pi_{\cX, \#}\gamma = \bP$ means that $\bP$ is the marginal distribution of $\gamma$ on $\cX$. Similarly $\bQ$ is the marginal distribution of $\gamma$ on $\cY$.

Equivalently, let $X$ and $Y$ are \emph{random variables} taking values in $\cX$ and $\cY$. The \emph{joint distribution} of $(X, Y)$ is $\gamma$ with marginal distribution of $X$ and $Y$ being $\bP$ and $\bQ$. Then the problem is
\begin{align*}
\min_{\gamma \in \Pi(\bP, \bQ)} \E{\gamma}{c(X, Y)}
\end{align*} The joint distribution $\gamma \in \Pi(\bP, \bQ)$ such that $X_{\#}\gamma = \bP$ and $Y_{\#}\gamma = \bQ$ is called \emph{\textbf{a coupling}}.
\end{definition}

\item \begin{proposition} (\textbf{Existance of Solution}) \citep{santambrogio2015optimal}\\
Let $\cX, \cY$ be \textbf{complete separable spaces}, $\bP \in \cP(\cX)$, $\bQ \in \cP(\cY)$ and $c: \cX \times \cY \to \bR_{+}$ be \textbf{lower semi-continuous function}. Then the Kantorovich relaxation of optimal transport problem \textbf{admits a solution}. 
\end{proposition}

\item \begin{definition} (\textbf{\emph{Dual Problem of Kantorovich Problem}}) \citep{villani2009optimal, santambrogio2015optimal, gabriel2019computational} \\
The \underline{\textbf{\emph{dual problem}}} of \emph{Kantorovich problem} is described as below:
\begin{align*}
\cL_{c}(\bP, \bQ) = \max_{(\varphi,  \psi) \in \cC(\cX)\times \cC(\cY)} & \int_{\cX}\varphi(x) d\bP(x) + \int_{\cY}\psi(y) d\bQ(y) \\
\text{s.t. }&  \varphi(x) + \psi(y) \le c(x, y),\quad \forall x\in \cX, y \in \cY, 
\end{align*} Here, $(\varphi, \psi)$ is a pair of \emph{continuous functions} on $\cX$ and $\cY$ respectively and they are also the \textbf{\emph{Kantorovich potentials}}. The feasible region is 
 \begin{align*}
\cR(c) := \set{(\varphi,  \psi) \in  \cC(\cX)\times \cC(\cY): \varphi \oplus \psi \le c} 
\end{align*} where $( \varphi \oplus \psi)(x, y)=  \varphi(x) + \psi(y)$. 

In other words, the dual optimization problem is
\begin{align*}
\max_{(\varphi,  \psi) \in \cR(c)} \E{\bP}{\varphi(X)} + \E{\bQ}{\psi(Y)}
\end{align*}
\end{definition}

\item \begin{proposition} (\textbf{Strong Duality})  \citep{santambrogio2015optimal}\\
Let $\cX, \cY$ be \textbf{complete separable spaces}, and $c: \cX \times \cY \to \bR_{+}$ be \textbf{lower semi-continuous and bounded from below}. Then the optimal value of \emph{primal} and \emph{dual problems} are the same
\begin{align*}
\min_{X \sim \bP, Y \sim \bQ} \E{}{c(X, Y)} = \cL_{c}(\bP, \bQ) = \max_{(\varphi,  \psi) \in \cR(c)} \E{\bP}{\varphi(X)} + \E{\bQ}{\psi(Y)}.
\end{align*}
\end{proposition}
\end{itemize}
\subsection{Wasserstein Distance}
\begin{itemize}
\item \begin{definition} (\textbf{\emph{Wasserstein Distance}}) \\
Let $((\cX, d), \srB)$ be \emph{a metric measurable space} with \emph{Borel $\sigma$-algebra} induced by metric $d$. Let $X, Y$ be two random variables taking values in $\cX$ with distribution $\bP$ and $\bQ$. \emph{\textbf{The Wasserstein distance}} between \emph{probability distributions} $\bP$ and $\bQ$ induced by $d$ is defined as
\begin{align}
\cW_1(\bP, \bQ)  \equiv  \cW_d(\bP, \bQ) := \min_{X \sim \bP, Y \sim \bQ} \E{}{d(X, Y)} \label{def: wasserstein_dist}
\end{align} In general, for $p \in [1, \infty)$, we can define \emph{\textbf{Wasserstein $p$-distance}} as
\begin{align}
\cW_p(\bP, \bQ)  \equiv \cW_{p,d}(\bP, \bQ)  := \paren{\min_{X \sim \bP, Y \sim \bQ} \E{}{(d(X, Y))^p}}^{1/p}. \label{def: wasserstein_dist_p_norm}
\end{align}
\end{definition}

\item \begin{remark}
Not to confuse the \emph{\textbf{$2$-Wasserstein distance}} with \emph{\textbf{the Wasserstein distance induced by $L_2$ norm}}:
\begin{align*}
\cW_{\norm{\cdot}{2}}(\bP, \bQ) \equiv  \cW_{1, \norm{\cdot}{2}}(\bP, \bQ)  &:= \min_{X \sim \bP, Y \sim \bQ} \E{}{\norm{X - Y}{2}} \\
\cW_{2}(\bP, \bQ)   \equiv  \cW_{2, d}(\bP, \bQ)  &:= \sqrt{\min_{X \sim \bP, Y \sim \bQ} \E{}{d(X, Y)^2}} 
\end{align*} 
\end{remark}

\item \begin{remark} (\emph{\textbf{Wasserstein $p$-Distance is a Metric in $\cP(\cX)$}}) \\
The \underline{\textbf{\emph{Wasserstein $p$-distance}}} $\cW_{p,d}(\bP, \bQ)  := \paren{\min_{X \sim \bP, Y \sim \bQ} \E{}{(d(X, Y))^p}}^{1/p}$ is a well-defined \emph{metric} in $\cP(\cX)$: for all $\bP, \bQ, \bM \in \cP(\cX)$, 
\begin{enumerate}
\item (\emph{Non-Negativity}):\; $\cW_{p,d}(\bP, \bQ) \ge 0$.
\item (\emph{Definiteness}):\; $\cW_{p,d}(\bP, \bQ) = 0 $ iff $\bP = \bQ$
\item (\emph{Symmetric}):\; $\cW_{p,d}(\bP, \bQ) = \cW_{p,d}(\bQ, \bP)$
\item (\emph{Triangular inequality}): \; $\cW_{p,d}(\bP, \bQ)  \le \cW_{p,d}(\bP, \bM )  + \cW_{p,d}(\bM , \bQ) $
\end{enumerate}
\end{remark}

\item \begin{remark}
\emph{The Wasserstein distance, or Optimal Transport (OT)}, $\cW_{d}(\alpha, \beta)$ depends on the \emph{distance definition} $d$ on the base measurable space $\cX$. In other word, OT can be seen as automatically ``\underline{\textbf{\emph{lifting}}}" a \underline{\emph{ground metric $d$}} in $\cX$ to a \emph{metric} between \textbf{\emph{measures}} on $\cX$
\end{remark}

\item \begin{remark} (\textbf{\emph{Convergence in Wasserstein Space $\Leftrightarrow$ Weak Convergence}} ) \citep{villani2009optimal, santambrogio2015optimal, gabriel2019computational} \\
One of most \textbf{\emph{important}} propery of \emph{Wasserstein distance} is that  it is a \emph{\textbf{weak distance}}, i.e. it allows one to compare singular distributions (for instance, discrete ones) whose \textbf{supports \emph{do not overlap}} and to quantify the spatial shift between the supports of two distributions. 

In fact, $\cW_{p}$ is a way to quantify the \underline{\emph{\textbf{weak$^{*}$ convergence}}} or \emph{\textbf{convergence in distribution} (in law)} \citep{villani2009optimal}: 
\begin{definition}
On a compact domain $\cX$ , $(\alpha_k)_k$ converges \textbf{\textit{weakly}} to $\alpha$ in $\cM_{+}^{1}(\cX)$ (denoted $\alpha_{n}\stackrel{d}{\rightarrow} \alpha$) if and only if \emph{for any \textbf{continuous} function} $g \in \cC(\cX)$, $\int_{\cX} g d\alpha_k \rightarrow \int_{\cX} g d\alpha$. One needs to add additional decay conditions on $g$ on noncompact domains. 
\end{definition}

This notion of weak convergence corresponds to the \textbf{convergence in the distribution} of random vectors. Note the any random variable $X_{n}$ is a continous function on $\Omega$, and its distribution is the push-forward measure $\alpha_{n} = X_{n\#}\bP$. Therefore, $\alpha_{n}  \rightharpoonup \alpha$  is equivalent to $X_{n}\stackrel{d}{\rightarrow} X$. This convergence can be shown (see \citep{villani2009optimal, santambrogio2015optimal}) to be equivalent to 
\begin{align*}
\alpha_{n} \rightharpoonup \alpha \;\; \Leftrightarrow \;\;\cW_{p}(\alpha_{n}, \alpha) \rightarrow 0.
\end{align*}
Thus we can also write the weak convergance as $\alpha_{n}\stackrel{\cW_{d}}{\longrightarrow} \alpha$.
\end{remark}
\end{itemize}
\subsection{Dual Formulation of Wasserstein Distance}
\begin{itemize}
\item \begin{theorem} (\textbf{Kantorovich-Rubenstein Duality}) \citep{villani2009optimal}\\
Let $\cX$ be a \textbf{Polish space}, i.e. $\cX$ a complete separable metric space equipped with a Borel $\sigma$-algebra induced by metric $d$, and $\bP$ and $\bQ$ be probability measures on $\cX$. For fixed $p \in [1, \infty)$, let $Lip_1$ be the space of all 
$1$-\textbf{Lipschitz} function with respect to metric $d$  such that
\begin{align*}
\norm{f}{L}  := \sup_{x, y \in \cX}\set{\frac{\abs{f(x) - f(y)}}{d(x, y)}} \le 1.
\end{align*}
Then 
\begin{align}
\cW_{d}(\bP, \bQ) \equiv \cW_{1,d}(\bP, \bQ) &= \sup_{f \in Lip_1}\set{\E{\bP}{f(X)} - \E{\bQ}{f(Y)}}. \label{eqn: wass_dist_dual}
\end{align} 
\end{theorem}

\item \begin{remark}
This theorem only applies for \emph{Wasserstein $1$-distance}, i.e. $p = 1$.
\end{remark}

\item \begin{example} (\textbf{\emph{Total Variation as $\cW_d$ with respect to Hamming distance $d_H$}})\\
When $d(x ,y) = \sum_i \ind{x_i \neq y_i} = d_H(x, y)$ Hamming distance, the $\cW_{1, d}$ becomes
\begin{align*}
\cW_{1, d_H}(\bP, \bQ) &= \min_{\gamma \in \Pi(\bP,\bQ)}\gamma\set{X \neq Y } \\
&= \sup_{f: \cX \to [0, 1]}\int_{\cX} f \paren{d\bP - d\bQ}\\
& = \sup_{A \subset \cX}\abs{\bP(A) - \bQ(A)} := \norm{\bP - \bQ}{TV}
\end{align*}
\end{example}

\item \begin{example} (\textbf{\emph{$\cW_1$ in $1$-dimensional space $\bR$}}) \\
When $d(x, y) = \abs{x - y}$ in $\bR$, and $F_{\alpha}, F_{\beta}$ are cumulative distribution function  of $\alpha, \beta$,    then $\cW_1$ distance becomes
\begin{align*}
\cW_{1}(\alpha, \beta) &=  \norm{F_{\alpha} - F_{\beta}}{1} := \int_{-\infty}^{\infty}\norm{F_{\alpha}(x) - F_{\beta}(x)}{1} dx \\
&=  \int_{-\infty}^{\infty}\abs{\int_{-\infty}^{x}d(\alpha - \beta) }
\end{align*} which shows that $\cW_1$ on $\bR$ is a \textbf{norm}. An optimal Monge map $T$ such that $T_{\#}\alpha = \beta$ is then defined by
\begin{align*}
T = F_{\beta}^{-1} \circ  F_{\alpha}   
\end{align*} where $F_{\beta}^{-1} = \inf\set{t: F_{\beta} \ge t}$.
\end{example}
\end{itemize}

\section{The Transportation Method}
\subsection{Concentration via Transportation Cost Inequality}
\begin{itemize}
\item \begin{lemma} (\textbf{\emph{Transportation Lemma}}) \citep{boucheron2013concentration} \\
Let $X$ be a real-valued integrable random variable. Let $\phi$ be a \textbf{\emph{convex}} and \textbf{\emph{continuously differentiable}} function on a (possibly unbounded) interval $[0, b)$ and assume that $\phi(0) = \phi'(0) = 0$. Define, for every $x \ge 0$, \textbf{\emph{the Legendre transform}} $\phi^{*}(x) = \sup_{\lambda \in (0,b)}(\lambda x - \phi(\lambda))$, and let, for every $t \ge 0$, $\phi^{*-1}(t) = \inf\{x \ge 0: \phi^{*}(x) > t\}$, i.e. the \textbf{\emph{the generalized inverse}} of $\phi^{*}$. Then the following two statements are equivalent:
\begin{enumerate}
\item for every $\lambda \in (0,b)$,
\begin{align*}
\psi_{X - \E{}{X}}(\lambda) &\le \phi(\lambda)
\end{align*} where $\psi_{X}(\lambda):= \log\E{\bP}{e^{\lambda X}}$ is the logarithm of moment generating function;
\item for any probability measure $\bQ$ absolutely continuous with respect to $\bP$ such that $\kl{\bQ}{\bP} < \infty$,
\begin{align}
\E{\bQ}{X} - \E{\bP}{X} &\le \phi^{*-1}\paren{\kl{\bQ}{\bP}}. \label{ineqn: information_inequality_general}
\end{align} 
\end{enumerate}
In particular, given $\nu > 0$, $X$ follows a \emph{\textbf{sub-Gaussian distribution}}, i.e.
\begin{align*}
\psi_{X - \E{}{X}}(\lambda) &\le \frac{\nu\lambda^2}{2}
\end{align*} for every $\lambda >0$ \textbf{\emph{if and only if}} for any probability measure $\bQ$ absolutely continuous with respect to $\bP$ such that $\kl{\bQ}{\bP} < \infty$, 
\begin{align}
\E{\bQ}{X} - \E{\bP}{X} &\le \sqrt{2\nu\kl{\bQ}{\bP}}. \label{ineqn: information_inequality_sub_gaussian}
\end{align}
\end{lemma}

\item \begin{remark} (\textbf{\emph{Concentration via Transportation Methods}})\\ 
Let $\bP = \otimes_{i=1}^{n}\bP_i$ be the product measure for $Z := (Z_1 \xdotx{,} Z_n)$ on $\cX^n$ and $f: \cX^n  \to \bR$ be \emph{$1$-Lipschitz function}.  Consider a probability measure $\bQ$ on $\cX^n$, absolutely continuous with respect to $\bP$ and let $Y$ be a random variable (defined on the same probability space as $\cX$) such that $Y$ has distribution $\bQ$.

The lemma above suggests that one may prove \emph{sub-Gaussian concentration inequalities} for $X = f(Z_1 \xdotx{,} Z_n)$ by proving a ``\emph{transportation}" \emph{inequality} as above. The key to achieving this relies on \emph{coupling}. In particular, \emph{the Kantorovich-Rubenstein duality} for $\cW_{1,d}$  suggests that 
\begin{align*}
\E{\bQ}{f(Y)} - \E{\bP}{f(Z)} \le \min_{\gamma \in \Pi (\bQ, \bP)} \E{\gamma}{d(Y, Z)} := \cW_{1,d}( \bQ, \bP)
\end{align*} Thus, it suffices to \emph{upper bound} the \emph{$1$-Wasserstein distance} between $\bQ$ and $\bP$.
\end{remark}

\item \begin{definition} (\emph{\textbf{$d$-Transportation Cost Inequality}}) \citep{wainwright2019high}\\
Let $(\cX, d)$ be a \emph{metric space} with metric $d$,  and $(\cX, \srB)$ be a \emph{measurable space}, where $\srB$ is \emph{the Borel $\sigma$-algebra} induced by metric $d$, \emph{\textbf{the probability measure}} $\bP$ is said to satisfy a \underline{\emph{\textbf{$d$-transportation cost inequality}}} with parameter $\nu > 0$ if
\begin{align}
\cW_{1,d}( \bQ, \bP) &\le \sqrt{2\nu\kl{\bQ}{\bP}}  \label{def: transportation_cost_inequality}
\end{align} for all probability measure $\bQ \ll \bP$ on $\srB$.
\end{definition}


\item \begin{theorem} (\textbf{Isoperimetric Inequality via Transportation Cost})\citep{wainwright2019high}\\
Consider a metric measure space $(\cX, \srB, \bP)$ with metric $d$, and suppose that $\bP$ satisfies the \textbf{$d$-transportation cost inequality} with parameter $\nu/2 >0$ in \eqref{def: transportation_cost_inequality} Then its \textbf{concentration function} satisfies the bound
\begin{align}
\alpha_{\bP, (\cX, d)}(t) &\le  \exp\paren{- \frac{(t -t_0)_{+}^2}{2 \nu}}, \text{ for }t  \ge t_0 \label{ineqn: concentration_function_transport_cost}
\end{align} where $t_0 := \sqrt{2\nu \log 2}$.  Moreover, for any $Z \sim \bP$ and any $L$-Lipschitz function $f : \cX \to \bR$, we have the \textbf{concentration inequality}
\begin{align}
\bP\set{\abs{f(Z) - \E{}{f(Z)}} \ge t} &\le 2 \exp\paren{- \frac{t^2}{2 \nu L^2}}.  \label{ineqn: lipschitz_concentration_transport_cost}
\end{align}
\end{theorem}
\begin{proof}
We begin by proving the bound \eqref{ineqn: concentration_function_transport_cost}. For any set $A$ with $\bP(A) \ge 1/2$ and a given $t> 0$, consider the set
\begin{align*}
A_t^c = \set{x \in \cX: d(x, A) \ge t}.
\end{align*} If $\bP(A_t) = 1$, then the proof is complete, so that we may assume that $P(A_t^c) > 0$. By construction, we have $d(A, A_t^c) := \inf_{x\in A_t^c} \inf_{y \in A}d(x, y) \ge t$. On the other hand, let $\bP_A := \bP(\cdot | A)$ and $\bP_{A_t} := \bP(\cdot| A_t^c)$ denote the distributions of $\bP$ conditioned on $A$ and $A_t^c$, and let $\gamma$ denote any \emph{coupling} of this pair. Since the marginals of $\gamma$ are supported on $A$ and $A_t^c$, respectively, we have
\begin{align*}
d(A, A_t^c) &\le \int_{\cX \times \cX} d(x, x') d\gamma(x, x').
\end{align*} Taking the \emph{infimum} over all \emph{couplings}, we conclude that
\begin{align*}
t \le d(A, A_t^c)\le \inf_{\gamma \in \Pi(\bP_A, \bP_{A_t^c})}\int_{\cX \times \cX} d(x, x') d\gamma(x, x') := \cW_{1, d}(\bP_A, \bP_{A_t^c})
\end{align*} Now applying the triangle inequality, we have
\begin{align*}
t \le  \cW_{1, d}(\bP_A, \bP_{A_t^c}) &\le  \cW_{1, d}(\bP_A, \bP) + \cW_{1, d}(\bP, \bP_{A_t^c}) \\
&\le \sqrt{2\nu \kl{\bP_A}{\bP}} + \sqrt{2\nu \kl{\bP_{A_t^c}}{\bP}} %\\
%&\le \sqrt{2\nu} \paren{ \kl{\bP_A}{\bP} + \kl{\bP_{A_t^c}}{\bP}}^{1/2}
\end{align*} %The last inequality follows $(a + b)^2 \le 2a^2 + 2b^2 $. 
It remains to compute the \emph{Kullback-Leibler divergences}. For any measurable set $C$, we
have 
\begin{align*}
\bP_A(C) &= \frac{\bP(C \cap A)}{\bP(A)}\\
g = \frac{d\bP_A }{d\bP} &= \frac{1}{\bP(A)}\ind{A}\\
\kl{\bP_A}{\bP} &= \int \log \paren{\frac{d\bP_A }{d\bP}}d\bP_A  =  \log \frac{1}{\bP(A)}
\end{align*}
Similarly, we have $\kl{\bP_{A_t^c}}{\bP} = \log \frac{1}{\bP(A_t^c)}$. Combining the pieces, we have
\begin{align*}
t \le \cW_{1, d}(\bP_A, \bP_{A_t^c}) &\le \sqrt{2\nu  \log \frac{1}{\bP(A)}} + \sqrt{2\nu  \log \frac{1}{\bP(A_t^c)}} 
%\\
%& \le  \sqrt{2\nu  \log 2} + \sqrt{2\nu  \log \frac{1}{\bP(A_t^c)}} \quad \text{since }\bP(A) \ge 1/2\\
%\Rightarrow  \bP(A_t^c) &\le  \exp\paren{- \frac{\paren{t -t_0}_{+}^2}{2\nu} } , \text{ for }t \ge t_0 = \sqrt{2\nu  \log 2} %\exp\paren{-\frac{(t -  \sqrt{2\nu  \log 2})_{+}^2}{2\nu}}, \text{ for }t > \sqrt{2\nu  \log 2} \\
\end{align*} 
Denote $u = \sqrt{2\nu  \log \frac{1}{\bP(A)}}$, we have 
\begin{align*}
\paren{t - u}_{+} &\le \sqrt{2\nu  \log \frac{1}{\bP(A_t^c)}} \\
\bP(A_t^c) &\le \exp\paren{- \frac{\paren{t - u}_{+}^2}{2\nu} } , \text{ for }t \ge u.
\end{align*} Since $\bP(A) \ge 1/2$ so $u \le \sqrt{2\nu \log 2}$. Thus for $t \ge \sqrt{2\nu  \log 2}$, the concentration function
\begin{align*}
\alpha_{\bP, (\cX, d)}(t)  = \sup_{A\subset\cX: \bP(A) \ge 1/2}\bP(A_t^c) \le \exp\paren{- \frac{\paren{t - \sqrt{2\nu  \log 2}}_{+}^2}{2\nu} },
\end{align*} which proves \eqref{ineqn: concentration_function_transport_cost}.


%we conclude that
%\begin{align*}
%t^2 \le 2\nu \paren{ \log \frac{1}{\bP(A)} +  \log \frac{1}{\bP(A_t^c)}} = 2\nu \log \frac{1}{\bP(A)\bP(A_t^c)} \\
%\bP(A)\bP(A_t^c) \le \exp\paren{- \frac{t^2}{2\nu}}
%\end{align*} Since $\bP(A) \ge 1/2$, we conclude that 
%\begin{align*}
%\alpha_{\bP, (\cX, d)}(t)  = \sup_{A\subset\cX: \bP(A) \ge 1/2}\bP(A_t^c) \le 2 \exp\paren{- \frac{t^2}{2\nu}}.
%\end{align*} Thus proves \eqref{ineqn: concentration_function_transport_cost}.

To show \eqref{ineqn: lipschitz_concentration_transport_cost}, we see that for $L$-Lipschitz function:
\begin{align*}
\E{\bQ}{f(Y)} - \E{\bP}{f(Z)} \le L\min_{\gamma \in \Pi (\bQ, \bP)} \E{\gamma}{d(Y, Z)} = L \;\cW(\bQ, \bP) \le \sqrt{2 L^2 \nu \kl{\bQ}{\bP}}
\end{align*} where the first inequality follows \emph{the Kantorovich-Rubenstein duality} and the second inequality follows the assumption. By \emph{the transportation lemma}, 
\begin{align*}
\psi_{f(Z) - \E{}{f(Z)}}(\lambda) = \E{\bP}{e^{\lambda \paren{f(Z) - \E{}{f(Z)}}}} &\le \frac{\nu L^2 \lambda^2}{2}
\end{align*} The upper tail bound thus follows by the Chernoff bound. The same argument can be applied to  $-f$ , which yields the lower tail bound. \qed
\end{proof}

\end{itemize}
\subsection{Tensorization for Transportation Cost}
\begin{itemize}
\item \begin{proposition} (\textbf{Tensorization for Transportation Cost}) \citep{boucheron2013concentration}\\
Suppose that, for each $k = 1, 2 \xdotx{,} n$, the univariate distribution $\bP_k$ satisfies a \textbf{$d_k$-transportation cost inequality} with parameter $\nu_k$. Then \textbf{the product distribution} $\bP = \otimes_{k=1}^n \bP_k$ satisfies the transportation cost inequality
\begin{align}
\cW_{1, d}(\bQ, \bP) &= \sqrt{2\paren{\sum_{k=1}^n \nu_k} \kl{\bQ}{\bP}}, \quad \text{ for all distributions $\bQ \ll \bP$ } \label{ineqn: tensorization_transport_cost}
\end{align}  where the Wasserstein metric is defined using the distance $d(x, y) :=  \sum_{k=1}^{n}d_k(x_k, y_k)$.
\end{proposition}
\end{itemize}

%\subsection{Bounded Difference Inequality via Transportation Methods}
\subsection{Marton's Transportation Inequality}
\begin{itemize}
\item \begin{theorem} (\textbf{Marton's Transportation Inequality}) \citep{boucheron2013concentration}\\
Let $\bP = \otimes_{k=1}^n \bP_k$ be a product probability measure on $\cX^n$, and let $\bQ$ be a probability measure absolutely continuous with respect to $\bP$. Define two random vectors $X = (X_1 \xdotx{,} X_n), Y = (Y_1 \xdotx{,} Y_n)$ in $\cX^n$ with distribution $\bP$ and $\bQ$ respectively.  Then
\begin{align}
 \min_{\gamma \in \Pi(\bQ, \bP)}\sum_{i=1}^n \gamma^2\set{X_i \neq Y_i} &\le \frac{1}{2}\kl{\bQ}{\bP} \ \label{ineqn: marton_transport_cost_inequality} 
\end{align}
\end{theorem}

\item \begin{proof} (\textbf{\emph{Proof of Bounded Difference Inequality}}) \\
Any  function with \emph{\textbf{bounded difference property}} is \emph{\textbf{Lipschitz function}} with respect to \emph{\textbf{Hamming distance}}. This implies that for all $x, y \in \cX^n$,
\begin{align*}
f(y) - f(x) \le \sum_{i=1}^{n}L_i \ind{x_i \neq y_i} \equiv d_{H,L}(x, y).
\end{align*} Note that for coupling $\gamma \in \Pi(\bQ, \bP)$ where $Y \sim \bQ$ and $X \sim \bP$, 
\begin{align*}
\E{\bQ}{f(Y)} - \E{\bP}{f(X)} &= \E{\gamma}{f(Y) - f(X)} \\
&\le \sum_{i=1}^n L_i \E{\gamma}{\ind{X_i \neq Y_i}} \\
&\le \paren{\sum_{i=1}^{n}L_i^2}^{1/2}\paren{\sum_{i=1}^{n}(\E{\gamma}{\ind{X_i \neq Y_i}})^2}^{1/2}
\end{align*}

We want to prove the concentration using transportation cost inequality. That is, to bound the term
\begin{align*}
\min_{\gamma \in \Pi(\bQ, \bP)}\sum_{i=1}^{n}(\E{\gamma}{\ind{X_i \neq Y_i}})^2 = \min_{\gamma \in \Pi(\bQ, \bP)}\sum_{i=1}^{n}\gamma^2\set{X_i \neq Y_i}.
\end{align*} We have shown that 
\begin{align*}
\min_{\gamma \in \Pi(\bQ, \bP)}\gamma\set{X \neq Y} = \cW_{1, d_H}(\bQ, \bP) = \sup_{A \in \cX}\abs{\bQ(A) - \bP(A)}  \equiv \norm{\bQ - \bP}{TV}.
\end{align*}  For each independent variable $X_i, Y_i$, and their marginal distribution $\bP_i, \bQ_i$ where $\bQ_i \ll \bP_i$, by Pinsker's inequality,
\begin{align*}
 \min_{\gamma \in \Pi(\bQ_i, \bP_i)}\gamma\set{X_i \neq Y_i}  &\le \sqrt{\frac{1}{2}\kl{\bQ_i}{\bP_i}} \\
 \min_{\gamma \in \Pi(\bQ_i, \bP_i)}\gamma^2\set{X_i \neq Y_i} &\le \frac{1}{2}\kl{\bQ_i}{\bP_i}
\end{align*} Thus by induction lemma, 
\begin{align*}
\min_{\gamma \in \Pi(\bQ, \bP)}\sum_{i=1}^{n}\gamma^2\set{X_i \neq Y_i} &\le \frac{1}{2}\kl{\bQ}{\bP}
\end{align*} which is the \emph{Marton's transportation inequality}. Finally, we have
\begin{align*}
\E{\bQ}{f(Y)} - \E{\bP}{f(X)} &\le \paren{\sum_{i=1}^{n}L_i^2}^{1/2}\paren{\sum_{i=1}^{n}(\E{\gamma}{\ind{X_i \neq Y_i}})^2}^{1/2} \\
&\le \sqrt{\frac{\paren{\sum_{i=1}^{n}L_i^2}}{2}\kl{\bQ}{\bP}}.
\end{align*} Then we can apply the transportation lemma with $\nu := \frac{1}{4}\sum_{i=1}^{n}L_i^2$, which proves the bounded difference inequality. \qed 
\end{proof}

\item \begin{theorem} (\textbf{Marton's Conditional Transportation Inequality}) \citep{boucheron2013concentration}\\
Let $\bP = \otimes_{k=1}^n \bP_k$ be a product probability measure on $\cX^n$, and let $\bQ$ be a probability measure absolutely continuous with respect to $\bP$. Define two random vectors $X = (X_1 \xdotx{,} X_n), Y = (Y_1 \xdotx{,} Y_n)$ in $\cX^n$ with distribution $\bP$ and $\bQ$ respectively.  Then
\begin{align}
 \min_{\gamma \in \Pi(\bQ, \bP)}\E{\gamma}{\sum_{i=1}^n(\gamma^2\set{X_i \neq Y_i | X_i} + \gamma^2\set{X_i \neq Y_i | Y_i})} &\le 2\kl{\bQ}{\bP} \label{ineqn: marton_conditional_transport_cost_inequality}
\end{align}
\end{theorem}

\item \begin{proposition} (\textbf{Concentration of Lipschitz Function with Function Weighted Hamming Distance}) \citep{boucheron2013concentration}\\
Let $f: \cX^n \to \bR$ be a measurable function and let $Z_1 \xdotx{,} Z_n$ be independent random variables taking their values in $\cX$. Define $X = f(Z_1 \xdotx{,} Z_n)$. Assume that there exist \textbf{measurable functions} $c_i: \cX_n \to [0, \infty)$ such that for all $x, y \in \cX^n$, 
\begin{align*}
f(y) - f(z) \le \sum_{i=1}^{n}c_i(z) \ind{y_i \neq z_i}.
\end{align*} Setting
\begin{align*}
\nu = \E{}{\sum_{i=1}^{n}c_i^2(Z)} \qquad \text{ and }\qquad \nu_{\infty} = \sup_{z \in \cX^n}\sum_{i=1}^{n}c_i^2(z)
\end{align*} for all $\lambda  > 0$, we have 
\begin{align*}
\psi_{X - \E{}{X}}(\lambda) \le \frac{\nu \lambda^2}{2} \qquad \text{ and }\qquad \psi_{- X + \E{}{X}}(\lambda) \le \frac{\nu_{\infty} \lambda^2}{2}
\end{align*} In particular, for all $t > 0$,
\begin{align}
\bP\set{X \ge \E{}{X}  + t} &\le \exp\paren{-\frac{t^2}{2\nu}} \nonumber\\
\bP\set{X \le \E{}{X}  - t}   &\le \exp\paren{-\frac{t^2}{2\nu_{\infty}}}. \label{ineqn: weakly_self_bounding_sub_gaussian}
\end{align}
\end{proposition}

\item \begin{remark}
The condition in above proposition covers 
\begin{enumerate}
\item \emph{Lipschitz functions} such as \emph{functions with bounded difference},  
\item \emph{\textbf{self-bounding functions}} including \emph{\textbf{configuration functions}}:
Let $f$ be such a configuration function. For any $z \in \cX^n$, fix a \emph{maximal sub-sequence} $(z_{i,1} \xdotx{,} z_{i,m})$ satisfying property $\Pi$ (so that $f(z) = m$). Let $c_i(z)$ denote \emph{the indicator that $z_i$ belongs to the sub-sequence} $(z_{i,1} \xdotx{,} z_{i,m})$. Thus,
\begin{align*}
\sum_{i=1}^n c_i^2(z) = \sum_{i=1}^n c_i(z) = f(z).
\end{align*} It follows from the definition of a configuration function that for all $z, y \in \cX^n$,
\begin{align*}
f(y) \ge f(z) -  \sum_{i=1}^n c_i(z)\ind{z_i \neq y_i}
\end{align*} So $g = -f$ satisfies the condition in above proposition. 
\item  \emph{\textbf{weakly self-bounding functions}}
\item \emph{\textbf{convex distance function}}
\begin{align*}
d_{T}(z, A) &:= \sup_{\alpha \in \bR_{+}^{n}: \norm{\alpha}{2} = 1}\inf_{y \in A}\sum_{i=1}^n\alpha_i \ind{z_i \neq y_i}
\end{align*} Denote by $c(z) = (c_1(z) \xdotx{,} c_n(z)) = \alpha^{*}$ the vector of nonnegative components \emph{in the unit ball} for which \emph{the supremum is achieved}.  Thus
\begin{align*}
d_{T}(z, A) - d_{T}(y, A) &\le  \inf_{z' \in A}\sum_{i=1}^n c_i(z) \ind{z_i \neq z_i'} -  \inf_{y' \in A}\sum_{i=1}^n c_i(z) \ind{y_i \neq y_i'}\\
&\le \sum_{i=1}^n c_i(z) \ind{z_i \neq y_i}
\end{align*}
\end{enumerate}


\end{remark}
\end{itemize}
%\subsection{Convex Distance Inequality via Conditional Transportation Cost}
\subsection{Talagrand's Gaussian Transportation Inequality}
\begin{itemize}
\item \begin{theorem} (\textbf{Talagrand's Gaussian Transportation Inequality}) \citep{boucheron2013concentration}\\
Let $\bP$ be be the standard Gaussian probability measure on $\bR^n$, and let $\bQ$ be a probability measure absolutely continuous with respect to $\bP$. Define two random vectors $X = (X_1 \xdotx{,} X_n), Y = (Y_1 \xdotx{,} Y_n)$ in $\cX^n$ with distribution $\bP$ and $\bQ$ respectively.  Then
\begin{align}
\cW_{2, d}(\bQ, \bP) := \sqrt{\min_{\gamma \in \Pi(\bQ, \bP)}\sum_{i=1}^n\E{\gamma}{(X_i - Y_i)^2}   } &\le \sqrt{2\kl{\bQ}{\bP}} \label{ineqn: talagrand_gaussian_transport_cost_inequality} \\
\Leftrightarrow  \min_{\gamma \in \Pi(\bQ, \bP)}\sum_{i=1}^n\E{\gamma}{(X_i - Y_i)^2}   &\le 2\kl{\bQ}{\bP} \nonumber
\end{align}
\end{theorem}

\item \begin{remark} (\textbf{\emph{Gaussian Transportation Inequality $\Rightarrow$ Gaussian Concentration Inequality}}) \citep{boucheron2013concentration}\\
\emph{Talagrand's \textbf{Gaussian transportation inequality}} implies \emph{the Tsirelson-Ibragimov-Sudakov inequality} (i.e. \emph{\textbf{the dimension-free concentration}} of \emph{Lipschitz function} of Gaussian vectors), which we proved based on \emph{the Gaussian logarithmic Sobolev inequality} and \emph{Herbst's argument}.

Assume that $f: \bR^n \to \bR$ is a \emph{Lipschitz function} with respect to \emph{Euclidean distance}, that is, for all $x, y \in \bR^n$,
\begin{align*}
f(y) - f(x) \le L \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
\end{align*} Then, by \emph{Jensen's inequality}, for every \emph{coupling} $\gamma \in \Pi(\bP, \bQ)$, one has
\begin{align*}
\E{\bQ}{f(Y)} - \E{\bP}{f(X)} &= \E{\gamma}{f(Y) - f(X)} \\
&\le L \E{\gamma}{\paren{\sum_{i=1}^n (X_i - Y_i)^2}^{1/2}} \\
&\le L \paren{\sum_{i=1}^n \E{\gamma}{(X_i - Y_i)^2}}^{1/2} = L\; \cW_{2}(\bQ, \bP) \\
&\le \sqrt{2 L^2 \kl{\bQ}{\bP}} \quad \text{ by Gaussian Transportation Inequality }
\end{align*} By transportation lemma, we show that $f(X) - \E{}{f(X)}$ is \emph{sub-Gaussian distributed} with parameter $L^2$. This implies \emph{\textbf{the Gaussian concentration inequality}}.
\end{remark}
\end{itemize}
\subsection{Transportation Cost Inequalities for Markov Chains}

\newpage
\bibliographystyle{plainnat}
\bibliography{reference.bib}
\end{document}