\documentclass[11pt]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent %\usepackage{graphicx}
\usepackage{amsmath,amssymb, mathrsfs,  mathtools, dsfont}
\usepackage{tabularx}
\usepackage{tikz-cd}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{xcolor}
%\usepackage[linkbordercolor ={1 1 1} ]{hyperref}
%\usepackage[sf]{titlesec}
\usepackage{natbib}
\usepackage{../../Tianpei_Report}

%\usepackage{appendix}
%\usepackage{algorithm}
%\usepackage{algorithmic}

%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}



\begin{document}
\title{Lecture 12: Tensors}
\author{ Tianpei Xie}
\date{Oct. 20th., 2022}
\maketitle
\tableofcontents
\newpage
\section{Multilinear Algebra}
\subsection{Multilinear Functions and Tensor Product}
\begin{itemize}
\item 
\begin{definition}
Suppose $V_1,\ldots,V_k$, and $W$ are \emph{vector spaces}. A map $F: V_1\times \ldots \times V_k \rightarrow W$ is said to be \underline{\emph{\textbf{multilinear}}} if it is \emph{\textbf{linear}} as a function of \emph{each variable \textbf{separately}} when the others are held \emph{\textbf{fixed}}: for each $i$,
\begin{align*}
F(v_1,\ldots,\,av_i + a'v_i',\, \ldots, v_k) = a\,F(v_1,\ldots,\,v_i,\, \ldots, v_k)  + a'F(v_1,\ldots,\, v_i',\, \ldots, v_k).
\end{align*} 

\emph{A multilinear function} of \emph{\textbf{one variable}} is just \emph{\textbf{a linear function}}, and a multilinear
function of \emph{\textbf{two variables}} is generally called \emph{\textbf{bilinear}}.
\end{definition}

\item \begin{remark}
Let us write $L(V_1,\ldots,V_k ; W)$ for \emph{\textbf{the set of all multilinear maps from $V_1\times \ldots \times V_k$ to $W$}}. It is a \underline{\emph{\textbf{vector space}}} under the usual operations of \emph{pointwise addition} and \emph{scalar multiplication}:
\begin{align*}
(F'+F)(v_1,\ldots,\,v_i,\, \ldots, v_k) &= F(v_1,\ldots,\,v_i,\, \ldots, v_k)  +  F'(v_1,\ldots,\,v_i,\, \ldots, v_k),\\
(aF)(v_1,\ldots,\,v_i,\, \ldots, v_k)  &= a\,F(v_1,\ldots,\,v_i,\, \ldots, v_k).
\end{align*}
\end{remark}

\item \begin{example} (\emph{\textbf{Some Familiar Multilinear Functions}}).\\
\begin{enumerate}
\item The \underline{\emph{\textbf{dot product}}}  in $\bR^n$ is a \emph{\textbf{scalar-valued bilinear function}} of two vectors, used
to compute \emph{\textbf{lengths}} of vectors and \emph{\textbf{angles}} between them.
\item The \underline{\emph{\textbf{cross product}}}, $(\cdot \times \cdot)$ in $\bR^3$ is a \emph{\textbf{vector-valued bilinear function}} of two vectors, used
to compute \emph{\textbf{areas}} of parallelograms and to find a third vector \emph{\textbf{orthogonal}} to two given ones.
\item The \underline{\emph{\textbf{determinant}}}, $\det(\cdot)$ is a \emph{\textbf{real-valued multilinear function}} of \emph{$n$ vectors} in $\bR^n$, used
to detect \emph{\textbf{linear independence}} and to compute the \emph{\textbf{volume}} of the parallelepiped spanned by the vectors.
\item The \underline{\emph{\textbf{bracket in a Lie algebra $\mathfrak{g}$}}} is a \emph{\textbf{$\mathfrak{g}$-valued bilinear function}} of two elements of $\mathfrak{g}$.
\end{enumerate}
\end{example}

\item \begin{example}(\emph{\textbf{Tensor Products of Covectors}}).\\
Suppose $V$ is a vector space, and $\omega, \eta \in V^{*}$. Define a function $\omega \otimes \eta: V \times V \rightarrow \bR$ by
\begin{align*}
(\omega \otimes \eta)(v_1,v_2) =\omega(v_1) \eta(v_2),
\end{align*} where the product on the right is just ordinary multiplication of real numbers. The linearity of $\omega$ and $\eta$ guarantees that $\omega \otimes \eta$ is a \emph{bilinear function} of $v_1$ and $v_2$, so it is an element of $L(V,V ; \bR)$. For example, if $(e^1, e^2)$ denotes the standard dual basis for $\bR^2$, then $e^1 \otimes e^2: \bR^2 \times \bR^2 \times \bR$ is the bilinear function
\begin{align*}
e^1 \otimes e^2((w,x), (y,z)) = wz.
\end{align*}
\end{example}

\item \begin{definition}
let $V_1,\ldots,V_k$, $W_1,\ldots,W_l$ be real vector spaces, and suppose $F \in L(V_1,\ldots,V_k ; \bR)$ and $G \in L(W_1,\ldots,W_l; \bR)$. Define a function
$F\, \otimes \, G: V_1\times \ldots \times V_k \times W_1\times \ldots \times W_l \rightarrow \bR$
by
\begin{align}
(F\, \otimes \, G)(v_1,\ldots,\,v_k,\,w_1 \ldots, w_l)&= F(v_1,\ldots,\,v_k)\,G(w_1 \ldots, w_l) \label{eqn: tensor_product}
\end{align} It follows from the multilinearity of $F$ and $G$ that $(F\, \otimes \, G)(v_1,\ldots,\,v_k,\,w_1 \ldots, w_l)$ depends \emph{linearly} on each argument $v_i$ or $w_j$ \emph{separately}, so $F\, \otimes \, G$ is an element of $L(V_1,\ldots,V_k, W_1,\ldots,W_l; \bR)$ called \underline{\emph{\textbf{the tensor product of $F$ and $G$}}}.
\end{definition}

\item \begin{remark}
We can write tensor products of three or more multilinear functions unambiguously without parentheses. If $F_1,\ldots,F_l$ are \emph{multilinear functions} depending on $k_1, \ldots, k_l$ variables, respectively, their tensor product $F_1 \times \ldots \times F_l$ is a \emph{multilinear function} of $k= k_1+ \ldots + k_l$ variables, whose action on $k$ vectors is given by inserting the first $k_1$ vectors into $F_1$, the next $k_2$ vectors into $F_2$, and so forth, and multiplying the results together. 
\end{remark}

\item \begin{remark}
The definition of multilinear function as well as tensor product show a \emph{\textbf{recursion}}. It \emph{breaks} the complicated multi-variate calculation into product of smaller bivariate or univariate calculation. 
\end{remark}

\item \begin{remark}
If $\omega^j \in V_{j}^{*}$ for $j = 1,\ldots,k$, then $\omega^1\otimes \ldots \otimes \omega^k \in L(V_1,\ldots,V_k ; \bR)$ is the \emph{\textbf{multilinear function}} given by
\begin{align}
(\omega^1\otimes \ldots \otimes \omega^k)(v_1, \ldots, v_k)&= \omega^1(v_1)\,\ldots \omega^{k}(v_k).  \label{eqn: tensor_product_functionals}
\end{align} We can see that $\omega^1\otimes \ldots \otimes \omega^k$ is a multilinear extension of the linear functional $\omega$.
\end{remark}

\item 
\begin{proposition} (\textbf{A Basis for the Space of Multilinear Functions}). \\
Let $V_1,\ldots,V_k$ be real vector spaces of dimensions $n_1, \ldots, n_k$, respectively. For each $j \in set{1,\ldots,k}$, let $(E_1^{(j)},\ldots, E_{n_j}^{(j)})$ be a \emph{\textbf{basis}} for $V_j$, and let $(\epsilon_{(j)}^{1}, \ldots, \epsilon_{(j)}^{n_j})$ be the corresponding \textbf{dual basis} for $V_j^{*}$. Then the set
\begin{align*}
\mathfrak{B} &=\set{\epsilon_{(1)}^{i_1} \otimes \ldots \otimes \epsilon_{(k)}^{i_k}: \; 1 \le i_j \le n_j, j=1,\ldots, k}
\end{align*}
\textbf{is a basis} for $L(V_1,\ldots,V_k ; \bR)$, which therefore has \textbf{dimension equal to} $n_1 \ldots  n_k$.
\end{proposition}
\end{itemize}

\subsection{Abstract Tensor Products of Vector Spaces}
\begin{itemize}
\item We extend our result to abstract tensor product on multiple vector spaces. We need to first define the linear combinations.

\item \begin{definition}
For any set $S$, \emph{\textbf{a formal linear combination of elements of $S$}} is \emph{a function} $f: S \rightarrow \bR$ such that $f(s) = 0$ for all \emph{but} \emph{finitely many} $s \in S$. The \underline{\emph{\textbf{free (real) vector space}}} on $S$, denoted by $\cF(S)$, is \emph{the set of all formal linear combinations of elements of $S$}. Under pointwise addition and scalar multiplication, $\cF(S)$ becomes \emph{\textbf{a vector space over $\bR$}}.
\end{definition}

\item \begin{remark}
For each element $x \in S$, there is a function $\delta_{x} \in \cF(S)$ that takes the value $1$ on $x$ and zero on all other elements of $S$; typically we identify this function with $x$ itself, and thus think of $S$ as a subset of $\cF(S)$. 

Every element $f \in \cF(S)$ can then be written uniquely in the form $f = \sum_i^{m}a_i\,x_i$, where $x_1,\ldots, x_m$ are the elements of $S$ for which $f(x_i) \neq 0$, and $a_i = f(x_i)$. Thus, \emph{\textbf{$S$ is a basis}} for $\cF(S)$, which is therefore \emph{\textbf{finite-dimensional}} if and only if $S$ is \emph{\textbf{a finite set}}. 
\end{remark}

\item \begin{remark}
Normally the linear combinations are introduced in \emph{vector space} $V$ in which \emph{the scalar multiplication} and \emph{addition} are defined. Here we \textbf{generalize} it to \textbf{any set $S$} through a special function $f$ that only take nonzero values in finite number of elements in $S$. A typical example of such function is the indicator function $\delta_x(s) = \ind{s= x}$. Since the function taking values in $\bR$ which equipped with a proper definition of addition and scalar multiplication, we can represent any function $f$ in terms of a linear combination of functions $\delta_{x_i}$ instead of $x_i$ itself. This way helps us to circumvent the need to define algebraic structure on $S$.
\end{remark}

\item \begin{proposition} (\textbf{Characteristic Property of the Free Vector Space}). \\
For any set $S$ and any vector space $W$, every map $A: S \rightarrow W$ has a \textbf{unique extension} to a \textbf{linear map} $\bar{A}: \cF(S) \rightarrow W$.
\end{proposition}

\item \begin{definition}
Now let $V_1,\ldots,V_k$ be real vector spaces. We begin by forming \emph{\textbf{the free vector space}} $\cF(V_1\times \ldots \times V_k)$, which is the set of all finite formal linear combinations of k-tuples $(v_1,\ldots,\,v_k)$ with $v_i \in V_i$ for $i = 1,\ldots,k$. Let $\cR$ be the \emph{\textbf{subspace}} of $\cF(V_1\times \ldots \times V_k)$ \emph{spanned} by all elements of the following forms:
\begin{align}
&(v_1,\ldots,\,a\,v_i, \ldots, v_k) - a\,(v_1,\ldots,\,v_i, \ldots, v_k) \nonumber\\
&(v_1,\ldots,\,v_i + v_i', \ldots, v_k) - (v_1,\ldots,\,v_i, \ldots, v_k) -  (v_1,\ldots,\ v_i', \ldots, v_k)  \label{eqn: abstract_tensor_product_subspace_equiv}
\end{align} with $v_j, v_j' \in V_j$, $i \in \set{1,\ldots,k}$, and $a \in \bR$.

Define \underline{\emph{\textbf{the tensor product of the spaces $V_1,\ldots,V_k$}}}, denoted by \underline{$V_1 \otimes \ldots \otimes V_k$},
to be the following \emph{\textbf{quotient vector space}}:
\begin{align*}
V_1 \otimes \ldots \otimes V_k &= \cF(V_1\times \ldots \times V_k) / \cR
\end{align*}
and let $\Pi:  \cF(V_1\times \ldots \times V_k)  \rightarrow V_1 \otimes \ldots \otimes V_k$ be \emph{\textbf{the natural projection}}. The \emph{\textbf{equivalence class}} of an element $(v_1, \ldots, v_k)$ in $V_1 \otimes \ldots \otimes V_k$ is denoted by
\begin{align}
v_1 \otimes \ldots \otimes v_k &= \Pi(v_1, \ldots, v_k)\label{eqn: abstract_tensor_product_space_element}
\end{align}
and is called \underline{\emph{\textbf{the (abstract) tensor product of $(v_1, \ldots, v_k)$}}}. 

It follows from the definition that abstract tensor products satisfy
\begin{align*}
v_1 \otimes \ldots \otimes (a\,v_i)  \otimes \ldots \otimes v_k  &= a (v_1  \otimes \ldots  \otimes v_i \otimes \ldots \otimes v_k),\\
v_1 \otimes \ldots \otimes (v_i + v_i') \otimes \ldots \otimes v_k &= (v_1 \otimes \ldots \otimes v_i \otimes \ldots \otimes v_k) +  (v_1 \otimes \ldots \otimes v_i' \otimes \ldots \otimes v_k) 
\end{align*}
\end{definition}

\item \begin{remark}
Note that the definition implies that every element of  $V_1 \otimes \ldots \otimes V_k$ can be expressed as a linear combination of elements of the form $(v_1 \otimes \ldots \otimes v_k)$ for $v_i \in V_i$; but it is not true in general that every element of the tensor product space is of the form $(v_1 \otimes \ldots \otimes v_k)$.
\end{remark}

\item \begin{remark}
Intuitively, we want to define the tensor product $v_{1} \otimes \ldots \otimes v_{k}$ by concatenating all vectors into $k$-tuple $(v_{1}, \ldots, v_{k})$. But this naive construction is not enough. We have the following challenges:
\begin{enumerate}
\item The product space $V_1\times \ldots \times V_k$ is \emph{not necessarily a \textbf{vector space}} since we have not define the addition and scalar product for $k$-tuple $(v_{1}, \ldots, v_{k})$ 
\item We want the \emph{\textbf{multilinearity holds}} for the operator on $k$-tuple $(v_{1}, \ldots, v_{k})$, i.e. we want 
\begin{align}
(v_{1}, \ldots, a\,v_i' + b\,v_{i}'',  v_{k}) &= a\,(v_{1}, \ldots, v_i',  v_{k}) + b\,(v_{1}, \ldots, v_{i}'',  v_{k}) \label{eqn: abstract_tensor_product_multilinear}
\end{align} for any $i\in\set{1,\ldots,k}$ and any $a, b\in \bR$.
\end{enumerate}
The above constructions aim to solve these challenges:
\begin{enumerate}
\item Instead of defining the algebraic structure on product space $V_1\times \ldots \times V_k$, we extend it to \emph{\textbf{the free vector space}} $\cF(V_1\times \ldots \times V_k)$, the set of \emph{all linear combinations} of $k$-tuples $(v_{1}, \ldots, v_{k})$. By construction $\cF(V_1\times \ldots \times V_k) \supseteq V_1\times \ldots \times V_k$ and \emph{it is a vector space without defining the algebraic structure} since it use an indicator function to map to $\bR$.

\item Instead of enforcing the \emph{multilinearity} to hold,  we \emph{\textbf{partition the space}} $\cF(V_1\times \ldots \times V_k)$ \emph{\textbf{according to the multilinearitiy rule}}. That is, the set of tuples $(v_{1}, \ldots, a\,v_i' + b\,v_{i}'',  v_{k})$ and $(v_{1}, \ldots, v_i',  v_{k}), (v_{1}, \ldots, v_{i}'',  v_{k})$ that satisfies the equation \eqref{eqn: abstract_tensor_product_multilinear} \emph{will be grouped together} via the equivalence relationship.  The rule is actually a subspace of linear combinations of (difference of) tuples, denoted as $\cR \subseteq \cF(V_1\times \ldots \times V_k)$.  

Now we instead focusing on the equivalent class itself. By construction, \emph{\textbf{the equivalence class will satisfies the multilinear rule}} \eqref{eqn: abstract_tensor_product_multilinear} (The representer of the equivalence class follow the rule). Thsu $V_1 \otimes \ldots \otimes V_k = \cF(V_1\times \ldots \times V_k) / \cR$ is the tensor product space that we wants. 
\end{enumerate}
\end{remark}

\item \begin{remark}
To understand a tensor product of vectors $v_{1} \otimes \ldots \otimes v_{k}$, we need to know that it can be seen as \emph{an equivalent class} of $k$-tuple $(v_{1}, \ldots, v_{k}) $. For  tuples $(w_{1}, \ldots, w_{k}) \in (v_{1}, \ldots, v_{k}) + \cR$, we have $ (w_{1}, \ldots, w_{k}) - (v_{1}, \ldots, v_{k}) \in \cR$.
\begin{align*}
 &\Leftrightarrow (w_{1}, \ldots, w_{k}) - (v_{1}, \ldots, v_{k}) \in  \text{span}\set{(v_{1}, \ldots, v_{k}) \text{ follows rule }\eqref{eqn: abstract_tensor_product_subspace_equiv}} \\
&\Leftrightarrow \text{for some }\,j \in \set{1,\ldots, k}, \text{ so that }w_{j} = a^{j}\,w_{j}', \\
&\quad \text{ then } (v_{1}, \ldots, v_j, \ldots, v_{k}) = a^{j}(v_{1}, \ldots, w_{j}', \ldots v_{k})
\end{align*}
\end{remark}

\item \begin{proposition} (\textbf{Characteristic Property of the Tensor Product Space}).\\
Let  $V_1,\ldots,V_k$ be finite-dimensional real vector spaces. If $A: V_1\times \ldots \times V_k \rightarrow X$ is \textbf{any multilinear map} into a vector space $X$, then there is a \textbf{unique linear map} $\widetilde{A}: V_1\otimes \ldots \otimes V_k \rightarrow X$ such that the following diagram commutes:
\begin{equation}
  \begin{tikzcd}
    V_1\times \ldots \times V_k  \arrow{r}{A}  \arrow[swap]{d}{\pi}  & X  \\
    V_1\otimes \ldots \otimes V_k \arrow[ur,  dashrightarrow, swap, "\widetilde{A}"] &,  
  \end{tikzcd} \label{eqn: diagram_tensor_product}
\end{equation} where $\pi$ is the map $\pi(v_1, \ldots, v_k) = v_1 \otimes \ldots \otimes v_k$.
\end{proposition}

\item \begin{remark}
The \emph{characteristic property of the tensor product space} states that \emph{any mulilinear function} $\tau: V_1\times \ldots \times V_k \rightarrow \bR$ descends into a \emph{linear map} $\widetilde{\tau}:V_1\otimes \ldots \otimes V_k \rightarrow \bR$ so that any linear combinations of tensor products $v_{i_1} \otimes \ldots \otimes v_{i_k}$ is expressed as
\begin{align*}
\widetilde{\tau}\paren{a^{i_1\ldots i_k}\,v_{i_1} \otimes \ldots \otimes v_{i_k}} &= a^{i_1\ldots i_k}\,\tau(v_{i_1}, \ldots, v_{i_k})
\end{align*}
\end{remark}


\item \begin{proposition} (\textbf{A Basis for the Tensor Product Space}).\\
Suppose $V_1,\ldots,V_k$ are real vector spaces of dimensions $n_1 \ldots  n_k$, respectively. For each $j = 1,\ldots,k$, suppose $(E_1^{(j)},\ldots, E_{n_j}^{(j)})$ is a \textbf{basis} for $V_j$. Then the set
\begin{align*}
\mathfrak{C}&= \set{E^{(1)}_{i_1} \otimes \ldots \otimes E^{(k)}_{i_k}: \; 1 \le i_j \le n_j, j=1,\ldots, k}
\end{align*} \textbf{is a basis} for $V_1\otimes \ldots \otimes V_k$, which therefore has \textbf{dimension equal to} $n_1 \ldots  n_k$.
\end{proposition}

\item \begin{proposition} (\textbf{Associativity of Tensor Product Spaces}).\\
Let $V_1$, $V_2$, $V_3$ be finite-dimensional real vector spaces. There are \textbf{unique isomorphisms}
\begin{align*}
V_1 \otimes (V_2 \otimes V_3) \simeq V_1 \otimes V_2 \otimes V_3 \simeq (V_1 \otimes V_2) \otimes V_3 
\end{align*}
under which elements of the forms $v_1 \otimes (v_2 \otimes v_3)$, $v_1 \otimes v_2 \otimes v_3$ and $(v_1 \otimes v_2) \otimes v_3$ all correspond.
\end{proposition}

\item The connection between tensor products in this abstract setting and the more concrete tensor products of \emph{\textbf{multilinear functionals}} that we defined earlier is based on the following proposition.
\begin{proposition} (\textbf{Abstract vs. Concrete Tensor Products}).\citep{lee2003introduction}\\
If $V_1,\ldots,V_k$  are finite-dimensional vector spaces, there is a \underline{\textbf{canonical isomorphism}}
\begin{align}
V_1^{*}\otimes \ldots \otimes V_k^{*} &\simeq L(V_1,\ldots,V_k ; \bR) \label{eqn: abstract_covector_tensor_product_iso}
\end{align} under which the \textbf{abstract tensor product} defined by \eqref{eqn: abstract_tensor_product_space_element} corresponds to the \textbf{tensor product of covectors} defined by \eqref{eqn: tensor_product_functionals}.
\end{proposition}
\begin{proof}
First, define a map $\Phi: V_1^{*}\times \ldots \times V_k^{*} \rightarrow L(V_1,\ldots,V_k ; \bR)$ by
\begin{align*}
\Phi(\omega^{1}, \ldots, \omega^{k})(v_1, \ldots, v_k) &= \omega^{1}(v_1)\, \ldots  \omega^{k}(v_k).
\end{align*} The expression on the right depends linearly on each $v_i$ , so $\Phi(\omega^{1}, \ldots, \omega^{k})$ is indeed an element of the space $ L(V_1,\ldots,V_k ; \bR)$.  It is easy to check that $\Phi$ is multilinear as a function of $(\omega^{1}, \ldots, \omega^{k})$,  so by the characteristic property it descends uniquely to a linear map $\widetilde{\Phi}$ from $V_1^{*}\otimes \ldots \otimes V_k^{*}$ to $L(V_1,\ldots,V_k ; \bR)$, which satisfies
\begin{align*}
\widetilde{\Phi}(\omega^{1} \otimes  \ldots \otimes \omega^{k})(v_1, \ldots, v_k) &= \omega^{1}(v_1)\, \ldots  \omega^{k}(v_k).
\end{align*} It follows immediately from the definition that $\widetilde{\Phi}$ takes abstract tensor products to tensor products of covectors. It also takes the basis of $V_1^{*}\otimes \ldots \otimes V_k^{*}$  to the basis for $L(V_1,\ldots,V_k ; \bR)$, so it is an isomorphism. (Although we used bases to prove that  $\widetilde{\Phi}$  is an isomorphism, $\widetilde{\Phi}$ itself is canonically defined without reference to any basis.) \qed
\end{proof}



\item \begin{remark}
Using this canonical isomorphism, we henceforth use the notation $V_1^{*}\otimes \ldots \otimes V_k^{*}$ to denote either the abstract tensor product space or the space $L(V_1,\ldots,V_k ; \bR)$, focusing on whichever interpretation is more convenient for the problem at hand.
\end{remark}

\item \begin{remark}
Through this identification, an element $\omega^{1} \otimes  \ldots \otimes \omega^{k} \in V_1^{*}\otimes \ldots \otimes V_k^{*}$ is considered as a \emph{\textbf{multi-linear functional}}
\begin{align*}
(\omega^{1} \otimes  \ldots \otimes \omega^{k})(v_1, \ldots, v_k) = \omega^{1}(v_1)\, \ldots  \omega^{k}(v_k)
\end{align*} which can also descend into a linear map on tensor product of $v_i$
\begin{align*}
\widetilde{\omega}^{1,2,\ldots, k}(v_1 \otimes \ldots \otimes v_k) = \omega^{1}(v_1)\, \ldots  \omega^{k}(v_k)
\end{align*}

\end{remark}

\item \begin{remark}
Since we are assuming the vector spaces are all finite-dimensional, we can also identify each $V_j$ with its second dual space $V_j^{**}$, and thereby obtain \emph{\textbf{another canonical identification}}
\begin{align}
V_1\otimes \ldots \otimes V_k &\simeq L(V_1^{*},\ldots,V_k^{*}; \bR) \label{eqn: abstract_vector_tensor_product_iso}
\end{align}
\end{remark}

\item \begin{remark}
As we see, the space of tensor product defines a set of \underline{\emph{\textbf{parallel linear systems}}}. All \emph{\textbf{sub-systems}} are \underline{\emph{\textbf{independent}}}.  Each sub-system has its own \emph{\textbf{basis}}, its own \emph{\textbf{linear operations}} and its own \emph{\textbf{representation}}. The tensor product operation \emph{group} these independent linear systems together and \emph{consider them as a whole}. 

For \emph{\textbf{the whole system perspective}}, its representations are collected locally and then concatenated together. The linear map on \emph{\textbf{the concatenated representation}} is essentially the same as \emph{\textbf{applying linear map}} in each sub-system and \emph{\textbf{multiplying}} them together. This is the same as computing the joint distribution by product of marginal distributions. \emph{\textbf{The multiplication principle}} is applied when counting the size of the whole system.

The space of tensor product $V_1\otimes \ldots \otimes V_k$ reflect a \emph{\textbf{divide-and-conquer strategy}} and a \emph{\textbf{local-global strategy}} to study the complex functions such as \emph{multilinear functionals} $\alpha(v_1, \ldots, v_k)$. In the former, we study it by \emph{\textbf{perturbing} the input of each sub-system}. In the latter, we think of it as \emph{\textbf{a linear map}} on the k-tensors $v_1 \otimes \ldots \otimes v_k$.
\end{remark}
\end{itemize}

\subsection{Covariant and Contravariant Tensors on a Vector Space}
\begin{itemize}
\item \begin{definition}
Let $V$ be a finite-dimensional real vector space. If $k$ is a positive integer, \underline{\emph{\textbf{a covariant $k$-tensor}}} on $V$ is an element of \emph{the $k$-fold tensor product} $V^{*}\otimes \ldots \otimes V^{*}$, which we typically think of as \underline{\emph{\textbf{a real-valued multilinear function of $k$ elements of $V$}}}:
\begin{align*}
\alpha: \underbrace{V\times \ldots \times V}_{k} \rightarrow \bR
\end{align*}
The number $k$ is called \emph{\textbf{the rank of $\alpha$}}. A $0$-tensor is, by convention, just a real number (a real-valued function depending multilinearly on \emph{no vectors}!).  

We denote \underline{\emph{\textbf{the vector space}} of \emph{\textbf{all covariant $k$-tensors on $V$}}} by the shorthand notation
\begin{align*}
T^{k}V^{*} &= \underbrace{V^{*}\otimes \ldots \otimes V^{*}}_{k}
\end{align*}
\end{definition}

\item \begin{example} (\emph{\textbf{Covariant Tensors}}). \\
Let $V$ be a finite-dimensional vector space.
\begin{enumerate}
\item \emph{Every linear functional} $\omega: V \rightarrow \bR$ is multilinear, so \emph{\textbf{a covariant $1$-tensor is just a covector}}. Thus, $T^{1}(V^{*})$ is equal to $V^{*}$.
\item  \emph{A covariant $2$-tensor} on $V$ is \emph{a real-valued \textbf{bilinear function}} of two vectors, also called \emph{\textbf{a bilinear form}}. One example is \emph{the dot product} on $\bR^n$. More generally, \emph{\textbf{every inner product is a covariant $2$-tensor}}.
\item \emph{The determinant}, thought of as a function of $n$ vectors, is \emph{\textbf{a covariant $n$-tensor on $\bR^n$}}.
\end{enumerate}
\end{example}

\item \begin{definition}
For any finite-dimensional real vector space $V$, we define \\ \underline{\emph{the space of \textbf{contravariant tensors} on $V$} of \emph{\textbf{rank $k$}}} to be the vector space
\begin{align*}
T^{k}V &= \underbrace{V\otimes \ldots \otimes V}_{k}
\end{align*} In particular, $T^1(V) = V$, and by convention $T^0(V) = \bR$. Because we are assuming that $V$ is finite-dimensional, it is possible to identify this space with \emph{the set of \textbf{multilinear functionals of $k$ covectors}}:
\begin{align*}
T^{k}V &= \set{\text{multilinear functionals }\alpha: \underbrace{V^{*}\times \ldots \times V^{*}}_{k} \rightarrow \bR }
\end{align*}
\end{definition}

\item \begin{definition}
Even more generally, for any nonnegative integers $k, l$, we define \emph{the space of \underline{\textbf{mixed tensors on $V$ of type $(k,l)$}}} as
\begin{align*}
T^{(k,l)}V &= \underbrace{V\otimes \ldots \otimes V}_{k} \otimes \underbrace{V^{*}\otimes \ldots \otimes V^{*}}_{l}
\end{align*}
\end{definition}

\item \begin{remark} Some of these spaces are identical:
\begin{align*}
T^{(0,0)}V &= T^0V = T^{0}V^{*} =  \bR\\
T^{(0,1)}V &= T^{1}V^{*} = V^{*}\\
T^{(1,0)}V &= T^{1}V = V\\
T^{(k,0)}V &= T^{k}V \\
T^{(0,k)}V &= T^{k}V^{*}
\end{align*}
\end{remark}

\item \begin{corollary}
Let $V$ be an n-dimensional real vector space. Suppose $(E_i)$ is any basis for $V$ and $(\epsilon^{j})$ is the dual basis for $V^{*}$. Then the following sets constitute
bases for the tensor spaces over $V$:
\begin{align}
&\set{\epsilon^{i_1} \otimes \ldots \otimes \epsilon^{i_k}: \; 1 \le i_s \le n, s=1,\ldots, k} \text{ is basis for }T^{k}V^{*};  \nonumber\\
&\set{E_{i_1} \otimes \ldots \otimes E_{i_k}: \; 1 \le i_s \le n, s=1,\ldots, k}   \text{ is basis for }T^{k}V; \nonumber\\
&\set{E_{i_1} \otimes \ldots \otimes E_{i_k} \otimes \epsilon^{j_1} \otimes \ldots \otimes \epsilon^{j_l}: \; 1 \le i_1, \ldots i_k, j_1,\ldots, j_l \le n} \text{ is basis for }T^{(k, l)}V;  \label{eqn: basis_tensor_space_rank_k}
\end{align}
Therefore, $\text{dim }T^{k}V^{*} = \text{dim }T^{k}V = n^{k}$ and $\text{dim }T^{(k, l)}V = n^{k + l}$
\end{corollary}

\item \begin{remark} (\emph{\textbf{Coordinate Representation of Covariant $k$-Tensor}})\\
In particular, once a basis is chosen for $V$, every \emph{\textbf{covariant $k$-tensor}} $\alpha \in T^{k}(V^{*})$ can be written uniquely in the form
\begin{align}
\alpha &= \alpha_{i_1,i_2,\ldots, i_k} \epsilon^{i_1} \otimes \ldots \otimes \epsilon^{i_k}  \label{eqn: coordinate_represent_covariant_k_tensor}
\end{align}
where the $n^k$ coefficients $\alpha_{i_1,i_2,\ldots, i_k}$ are determined by
\begin{align}
\alpha_{i_1,i_2,\ldots, i_k} &= \alpha\paren{E_{i_1} , \ldots , E_{i_k}} \label{eqn: coordinate_represent_covariant_k_tensor_coefficient}
\end{align}

For instance, covariant $2$-tensor is bilinear form. Every \emph{bilinear form} can be written as $\beta = \beta_{i,j} \epsilon^1 \otimes \epsilon^2$, for some uniquely determined $n\times n$ matrix $(\beta_{i,j})$.
\end{remark}

\item \begin{exercise}
Let $v_1 = \sin(y)\partdiff{}{x}|_{(1, \pi/2)}   - \frac{1}{2}x^2\partdiff{}{y}|_{(1, \pi/2)}$ and $v_2 = \cos(y)\partdiff{}{x}|_{(1, \pi/2)}  + (x+y)\partdiff{}{y}|_{(1, \pi/2)}$. 
$\omega_1 = 2x dx|_{(1, \pi/2)} + \cos(y)dy|_{(1, \pi/2)}$, $\omega_2 = 2\cos(y) dx|_{(1, \pi/2)} - (x^2 + y^2)dy|_{(1, \pi/2)}$.
\begin{align*}
\omega_1 \otimes \omega_2 &= \paren{2x dx_{(1, \pi/2)} + \cos(y)dy_{(1, \pi/2)}} \otimes \paren{2\cos(y) dx_{(1, \pi/2)} - (x^2 + y^2)dy|_{(1, \pi/2)}}\\
&= \paren{2 dx } \otimes \paren{ - (1 + (\pi/2)^2)dy} = -(2 + \pi^2/2) dx|_{(1, \pi/2)} \otimes dy|_{(1, \pi/2)} \\
\omega_1 \otimes \omega_2(v_1, v_2) &= \omega_1(v_1)\omega_2(v_2)\\
&= -(2 + \pi^2/2) dx|_{(1, \pi/2)}\paren{ \sin(y)\partdiff{}{x}|_{(1, \pi/2)}   - \frac{1}{2}x^2\partdiff{}{y}|_{(1, \pi/2)}} \\
&\quad dy|_{(1, \pi/2)}\paren{\cos(y)\partdiff{}{x}|_{(1, \pi/2)}  + (x+y)\partdiff{}{y}|_{(1, \pi/2)}}\\
&= -(2 + \pi^2/2) dx|_{(1, \pi/2)}\paren{\partdiff{}{x}|_{(1, \pi/2)} - \frac{1}{2}\partdiff{}{y}|_{(1, \pi/2)}}dy|_{(1, \pi/2)}\paren{(1+ \pi/2)\partdiff{}{y}|_{(1, \pi/2)}}\\
&=  -(2 + \pi^2/2)(1+ \pi/2) \qed
\end{align*}
\end{exercise}
\end{itemize}
\section{Symmetric and Alternating Tensors}
\subsection{Symmetric Tensors}
\begin{itemize}
\item 
\begin{definition}
Let $V$ be a finite-dimensional vector space. A \emph{\textbf{covariant $k$-tensor $\alpha$}} on $V$ is said to be \underline{\emph{\textbf{symmetric}}} if its value is \textit{\textbf{unchanged}} by \emph{\textbf{interchanging}} any pair of arguments:
\begin{align*}
\alpha\paren{v_1, \ldots, v_i, \ldots, v_j, \ldots, v_k} &= \alpha\paren{v_1, \ldots, v_j, \ldots, v_i, \ldots, v_k} 
\end{align*} whenever $i \le i< j \le k$.
\end{definition}

\item 
\begin{remark}
The following statements are equivalent for a covariant $k$-tensor $\alpha$:
\begin{enumerate}
\item $\alpha$ is symmetric;
\item For any vectors $v_1,\ldots,v_k \in V$, the value of $\alpha\paren{v_1, \ldots,  v_k}$ is unchanged when $v_1,\ldots,v_k$ are rearranged in any order.
\item The components $\alpha_{i_1,\ldots,i_k}$ of $\alpha$ with respect to any basis are unchanged by any permutation of the indices.
\end{enumerate}
\end{remark}


\item \begin{definition}
The set of \emph{\textbf{symmetric covariant $k$-tensors}} is a linear subspace of the space $T^k(V^{*})$ of all covariant $k$-tensors on $V$; we denote this subspace by \underline{$\Sigma^{k}(V^{*})$}
\end{definition}

\item \begin{definition}
There is a \emph{\textbf{natural projection}} from $T^k(V^{*})$ to $\Sigma^{k}(V^{*})$ defined as follows. First, let
$S_k$ denote \emph{\textbf{the symmetric group on $k$ elements}}, that is, the group of \emph{\textbf{permutations}} of the set $\set{1,\ldots,k}$. Given a $k$-tensor $\alpha$ and a permutation $\sigma \in S_{k}$, we define a new $k$-tensor $^{\sigma}\alpha$ by
\begin{align*}
^{\sigma}\alpha(v_1, \ldots, v_k) &= \alpha(v_{\sigma(1)}, \ldots, v_{\sigma(k)})
\end{align*}
Note that $^{\tau}(^{\sigma}\alpha) = ^{\tau \sigma}\alpha$ where $\tau\sigma$ represents the composition of $\tau$ and $\sigma$, that is, $\tau\sigma(i) = \tau(\sigma(i))$.(\emph{This is the reason for putting $\sigma$ before $\alpha$ in the notation $^{\sigma}\alpha$ instead of after it.}) 

We define a \emph{\textbf{projection}} $\text{Sym}: T^k(V^{*}) \rightarrow \Sigma^{k}(V^{*})$ called \underline{\emph{\textbf{symmetrization}}} by
\begin{align*}
\text{Sym }\alpha &= \frac{1}{k!}\sum_{\sigma \in S_{k}}{^{\sigma}\alpha}
\end{align*}
More explicitly, this means that
\begin{align*}
\text{Sym }\alpha(v_1, \ldots, v_k) &= \frac{1}{k!}\sum_{\sigma \in S_{k}} \alpha(v_{\sigma(1)}, \ldots, v_{\sigma(k)})
\end{align*}
\end{definition}

\item \begin{proposition} (\textbf{Properties of Symmetrization}).\\
 Let $\alpha$ be a covariant tensor on a finite-dimensional vector space.
 \begin{enumerate}
 \item $\text{Sym }\alpha $ is symmetric.
 \item $\text{Sym }\alpha = \alpha$ if and only if $\alpha$ is symmetric.
 \end{enumerate}
\end{proposition}

\item \begin{remark}
If $\alpha$ and $\beta$ are symmetric tensors on $V$, then $\alpha\otimes \beta$ is not symmetric in general. However, using the symmetrization operator, it is possible to define a new product that takes a pair of symmetric tensors and yields another symmetric tensor. 
\end{remark}

\item \begin{definition}
If $\alpha \in \Sigma^{k}(V^{*})$ and $\beta \in \Sigma^{k}(V^{*})$, we define their \emph{\textbf{symmetric product}} to be the $(k + l)$-tensor $\alpha\,\beta$(denoted by juxtaposition with no intervening product symbol) given by
\begin{align*}
\alpha\,\beta &= \text{Sym }(\alpha \otimes \beta)
\end{align*}
More explicitly, the action of $\alpha\,\beta$ on vectors $v_1,\ldots,v_{k+l}$ is given by
\begin{align*}
\alpha\,\beta(v_1,\ldots,v_{k+l}) &= \frac{1}{(k+l)!}\sum_{\sigma \in S_{k+l}}\alpha(v_{\sigma(1)},\ldots,v_{\sigma(k)})\beta(v_{\sigma(k+1)},\ldots,v_{\sigma(k+l)})
\end{align*}
\end{definition}

\item \begin{proposition} (Properties of the Symmetric Product).\\
\begin{enumerate}
\item The symmetric product is \textbf{symmetric} and \textbf{bilinear}: for all symmetric tensors $\alpha, \beta, \gamma$ and all $a, b \in \bR$,
\begin{align*}
\alpha\, \beta &= \beta\, \alpha \\
(a\,\alpha + b\,\beta)\,\gamma &= a\,\alpha\,\gamma + b\,\beta\,\gamma = \gamma\, (a\,\alpha + b\,\beta)
\end{align*}
\item  If $\alpha$ and $\beta$ are \textbf{covectors}, then
\begin{align*}
\alpha\,\beta &=\frac{1}{2}\paren{\alpha \otimes \beta + \beta \otimes \alpha}.
\end{align*}
\end{enumerate}
\end{proposition}
\end{itemize}

\subsection{Alternating Tensors}
\begin{itemize}
\item \begin{definition}
Assume that $V$ is a finite-dimensional real vector space. \emph{\textbf{A covariant k-tensor}} $\alpha$ on V is said to be \emph{\textbf{alternating}} (or \emph{\textbf{antisymmetric}} or \emph{\textbf{skew-symmetric}}) if it \emph{\textbf{changes sign}} whenever two of its arguments are \emph{interchanged}. This means that for all vectors $v_1,\ldots,v_k \in V$ and every pair of distinct indices $i$, $j$ it satisfies
\begin{align*}
\alpha\paren{v_1, \ldots, v_i, \ldots, v_j, \ldots, v_k} &= -\alpha\paren{v_1, \ldots, v_j, \ldots, v_i, \ldots, v_k} 
\end{align*}
\emph{Alternating covariant k-tensors} are also variously called \underline{\emph{\textbf{exterior forms}}, \emph{\textbf{multicovectors}}}, or \underline{\emph{\textbf{k-covectors}}}. 

The subspace of \emph{\textbf{all alternating covariant $k$-tensors}} on $V$ is denoted by \underline{$\Lambda^{k}(V^{*}) \subseteq T^k(V^{*}) $}.
\end{definition}

\item \begin{definition}
Recall that for any permutation $\sigma \in S_k$, \emph{\textbf{the sign of $\sigma$}}, denoted by $\text{sgn }\sigma$, is equal to $+1$ if \emph{$\sigma$ is \textbf{even}} (i.e., can be written as \emph{a composition of an \textbf{even} number of transpositions}), and $-1$ if $\sigma$ is \emph{\textbf{odd}}.
\end{definition}

\item 
\begin{remark}
The following statements are equivalent for a covariant $k$-tensor $\alpha$:
\begin{enumerate}
\item $\alpha$ is alternating;
\item For any vectors $v_1,\ldots,v_k \in V$, and any permutation $\sigma \in S_k$
\begin{align*}
\alpha\paren{v_{\sigma(1)}, \ldots,  v_{\sigma(k)}} &=(\text{sgn }\sigma)\alpha\paren{v_1, \ldots,  v_k}
\end{align*}
\item With respect to any basis, the components $\alpha_{i_1,\ldots,i_k}$ of $\alpha$ change sign whenever two indices are interchanged.
\end{enumerate}
\end{remark}

\item \begin{remark} Regarding the symmetric and alternating tensors:
\begin{enumerate}
\item Every $0$-tensor (which is just a real number) is both \textit{symmetric} and \emph{alternating}.
\item Every $1$-tensor is both \emph{symmetric} and \emph{alternating}.
\item An \textit{\textbf{alternating}} $2$-tensor on V is \emph{\textbf{a skew-symmetric bilinear form}}.
\item Every \emph{\textbf{covariant $2$-tensor $\beta$}} can be expressed as a \emph{\textbf{sum} of an \textbf{alternating tensor} and a \textbf{symmetric one}}, because
\begin{align*}
\beta(v, w) &= \frac{1}{2}\paren{\beta(v, w) - \beta(w, v)} + \frac{1}{2}\paren{\beta(v, w)+ \beta(w, v)} \equiv \alpha(v, w) + \sigma(v, w)
\end{align*}
where $\alpha(v,w) = \frac{1}{2}\paren{\beta(v, w) - \beta(w, v)}$ is an alternating tensor, and $ \sigma(v, w) =  \frac{1}{2}(\beta(v, w)+ \beta(w, v))$ is symmetric.
\item The above is not true for general higher tensor.
\end{enumerate}
\end{remark}
\end{itemize}

\section{Tensors and Tensor Fields on Manifolds}
\subsection{Definitions}
\begin{itemize}
\item \begin{definition}
Now let $M$ be a smooth manifold with or without boundary. We define the \underline{\emph{\textbf{bundle of covariant $k$-tensors}}} on $M$ by
\begin{align*}
T^kT^{*}M &= \bigsqcup_{p \in M}T^k\paren{T_{p}^{*}M} 
\end{align*}

Analogously, we define \underline{\emph{\textbf{the bundle of contravariant $k$-tensors}}} by
\begin{align*}
T^kTM &= \bigsqcup_{p \in M}T^k\paren{T_{p}M} 
\end{align*}
and \underline{\emph{\textbf{the bundle of mixed tensors of type $(k,l)$}}} by
\begin{align*}
T^{(k,l)}TM &= \bigsqcup_{p \in M}T^{(k,l)}\paren{T_{p}M}
\end{align*}
\end{definition}

\item \begin{remark}
There are natural identifications
\begin{align*}
T^{(0,0)}TM &= T^{0}T^{*}M = T^{0}TM = M \times \bR;\\
T^{(0,1)}TM &= T^{1}T^{*}M = T^{*}M;\\
T^{(1,0)}TM &= T^{1}TM = TM;\\
T^{(0,k)}TM &= T^{k}T^{*}M; \\
T^{(k,0)}TM &= T^{k}TM.
\end{align*}
Any one of these bundles is called \emph{\textbf{a tensor bundle over $M$}}. (Thus, the tangent and cotangent bundles are special cases of tensor bundles.) 
\end{remark}


\item \begin{definition}
\emph{A \textbf{section} of a tensor bundle} is called \emph{a \textbf{(covariant, contravariant, or mixed) \underline{tensor field}} on $M$}. \emph{\textbf{A smooth tensor field}} is a section that is smooth in the usual sense of smooth sections of vector bundles. 

So \emph{\textbf{contravariant $1$-tensor fields}} are the same as \emph{\textbf{vector fields}}, and \emph{\textbf{covariant $1$-tensor fields}} are \emph{\textbf{covector fields}}.
\end{definition}

\item \begin{remark}
The \emph{\textbf{spaces of smooth sections}} of these tensor bundles, \underline{$\Gamma\paren{T^kT^{*}M}$},  \underline{$\Gamma\paren{T^kTM}$}, and  \underline{$\Gamma(T^{(k,l)}TM)$}, are \underline{\emph{\textbf{infinite-dimensional vector spaces} over} $\bR$}, and \emph{\textbf{modules}} over $\cC^{\infty}(M)$. 

We also denote the \emph{space of smooth covariant tensor fields} as 
\begin{align*}
\mathcal{T}^{k}(M) &= \Gamma\paren{T^kT^{*}M}.
\end{align*}
\end{remark}

\item \begin{remark} (\emph{\textbf{Coordinate Representation of Tensor Fields}})\\
In any smooth local coordinates $(x^i)$, sections of these bundles can be written (using the summation convention) as
\begin{align}
A &= \left\{
\begin{array}{ll}
A_{i_1,\ldots,i_k}\;dx^{i_1}\otimes \ldots \otimes dx^{i_k},                                        & A \in \Gamma\paren{T^kT^{*}M};\\[10pt]
A^{i_1,\ldots,i_k}\;\dfrac{\partial}{\partial x^{i_1}} \otimes \ldots \otimes \dfrac{\partial}{\partial x^{i_k}}, & A \in \Gamma\paren{T^kTM};\\[10pt]
A^{i_1,\ldots,i_k}_{j_1,\ldots,j_l}\;\dfrac{\partial}{\partial x^{i_1}} \otimes \ldots \otimes \dfrac{\partial}{\partial x^{i_k}} \otimes dx^{j_1}\otimes \ldots \otimes dx^{j_k} 
&  A \in \Gamma\paren{T^{(k,l)}TM};
\end{array}
 \right.\label{eqn: coordinate_tensor_fields}
\end{align} The functions $A_{i_1,\ldots,i_k}$, $A^{i_1,\ldots,i_k}$, or $A^{i_1,\ldots,i_k}_{j_1,\ldots,j_l}$ are called the \emph{\textbf{component functions}} of $A$
in the chosen coordinates. 
\end{remark}

\item \begin{proposition} (\textbf{Smoothness Criteria for Tensor Fields}).\\
Let $M$ be a smooth manifold with or without boundary, and let $A: M \rightarrow T^kT^{*}M$ be a rough section. The following are equivalent.
\begin{enumerate}
\item $A$ is smooth.
\item In \textbf{every} smooth coordinate chart, the \textbf{component functions} of $A$ are smooth.
\item Each point of $M$ is contained in \textbf{some} coordinate chart in which A has \textbf{smooth component functions}.
\item If $X_1,\ldots,X_k \in \mathfrak{X}(M)$, then the function $A(X_1,\ldots,X_k): M \rightarrow \bR$, defined by
\begin{align}
A(X_1,\ldots,X_k)(p) &= A_{p}\paren{X_1\bigr|_{p},\ldots,X_k\bigr|_{p}} \label{eqn: covariant_tensor_field_induce_smooth_function}
\end{align} is smooth
\item Whenever $X_1,\ldots,X_k$ are smooth vector fields defined on \textbf{some open subset} $U\subseteq M$, the function $A(X_1,\ldots,X_k)$ is smooth on $U$.
\end{enumerate}
\end{proposition}

\item \begin{proposition}
Suppose $M$ is a smooth manifold with or without boundary, $A \in \cT^k(M)$, $B \in \cT^{l}(M)$, and $f \in \cC^{\infty}(M)$. Then $fA$ and $A \otimes B$ are also \textbf{smooth tensor fields}, whose \textbf{components} in any smooth local coordinate chart are
\begin{align*}
\paren{fA}_{i_1,\ldots, i_k} &= fA_{i_1,\ldots,i_k}, \\
\paren{A \otimes B}_{i_1,\ldots, i_{k+l}} &= A_{i_1,\ldots,i_k}B_{i_{k+1},\ldots,i_{k+l}}.
\end{align*}
\end{proposition}

\item \begin{lemma} (\textbf{Tensor Characterization Lemma)}.\citep{lee2003introduction}\\
A map 
\begin{align}
\mathcal{A}: \underbrace{\mathfrak{X}(M)\times \ldots \times \mathfrak{X}(M)}_{k} \rightarrow  \cC^{\infty}(M).  \label{eqn: tensor_characterization}
\end{align} is \textbf{induced} by a \textbf{smooth covariant $k$-tensor field} $A$ as in \eqref{eqn: covariant_tensor_field_induce_smooth_function} if and only if it is \textbf{multilinear} over $\cC^{\infty}(M)$.
\end{lemma}
\begin{remark}
Note that for any $f, f' \in \cC^{\infty}(M)$, any $X_i, X_i' \in \mathfrak{X}(M)$
\begin{align*}
A(X_1,\ldots, fX_i + f'X_i', \ldots, X_k) &= fA(X_1,\ldots, X_i , \ldots, X_k) + f'A(X_1,\ldots, X_i', \ldots, X_k) 
\end{align*}
\end{remark}

\item \begin{definition} For symmetric and alternating tensor field, we have the following definition:
\begin{enumerate}
\item \emph{\textbf{A symmetric tensor field}} on a manifold (with or without boundary) is simply \emph{a covariant tensor field} whose value at each point is a \emph{symmetric tensor}. 

The \emph{\textbf{symmetric product}} of two or more tensor fields is defined pointwise, just like the tensor product. Thus, for example, if $A$ and $B$ are \emph{smooth covector fields}, their symmetric product is \emph{\textbf{the smooth $2$-tensor field $AB$}}, which is given by
\begin{align*}
AB &= \frac{1}{2}\paren{A \otimes B} + \frac{1}{2}\paren{B \otimes A}.
\end{align*}


\item \emph{\textbf{Alternating tensor fields}} are called \underline{\emph{\textbf{differential forms}}};
\end{enumerate}
\end{definition}
\end{itemize}
\subsection{Pullbacks of Tensor Fields}
\begin{itemize}
\item \begin{definition}
Suppose $F: M \rightarrow N$ is a smooth map. For any point $p \in M$ and any \emph{$k$-tensor} $\alpha \in T^k(T^{*}_{F(p)}N)$, we define a tensor $dF_p^{*}(\alpha) \in T^k\paren{T^{*}_{p}M}$, called \underline{\emph{\textbf{the pointwise pullback}}} of $\alpha$ by $F$ at $p$, by
\begin{align*}
dF_p^{*}(\alpha)(v_1, \ldots, v_k) &= \alpha\paren{dF_p(v_1), \ldots, dF_p(v_k)}
\end{align*} for any $v_1,\ldots,v_k \in T_pM$. 
\end{definition}

\item \begin{definition}
If $A$ is \emph{a covariant k-tensor field on $N$}, we define a rough\textbf{\emph{$k$-tensor field $F^{*}A$ on $M$}}; called  \underline{\emph{\textbf{the pullback of $A$ by $F$}}}, by
\begin{align*}
(F^{*}A)_{p} &= dF_p^{*}(A_{F(p)}).
\end{align*}
This tensor field acts on vectors $v_1,\ldots,v_k \in T_pM$ by
\begin{align*}
(F^{*}A)_{p}(v_1, \ldots, v_k) &= A_{F(p)}\paren{dF_p(v_1), \ldots, dF_p(v_k)}.
\end{align*}
\end{definition}

\item \begin{proposition} (\textbf{Properties of Tensor Pullbacks}). \\
Suppose $F: M \rightarrow N$ and $G: N \rightarrow $P are smooth maps, $A$ and $B$ are covariant tensor fields on $N$, and $f$ is a real-valued function on $N$.
\begin{enumerate}
\item $F^{*}(fB) = (f \circ F)\,F^{*}(B)$
\item $F^{*}(A \otimes B) = F^{*}A \otimes F^{*}(B)$
\item $F^{*}(A + B) = F^{*}A + F^{*}(B)$
\item $F^{*}(B)$ is a (\textbf{continuous}) \textbf{tensor field}, and is \textbf{smooth} if $B$ is \textbf{smooth}.
\item $(G \circ F)^{*}B = F^{*}(G^{*}B).$
\item $(\text{Id}_N)^{*}B = B.$
\end{enumerate}
\end{proposition}

\item \begin{remark}
If $f$ is a continuous real-valued function (i.e., a $0$-tensor field) and $B$ is a $k$-tensor field, then it is consistent with our definitions to interpret $f \circ B$ as $fB$, and
$F^{*}f$ as $f \circ F$. 
\end{remark}

\item \begin{corollary} (\textbf{Coordinate Representation of Pullback Tensor Fields})\\
Let $F: M \rightarrow N$  be smooth, and let $B$ be a covariant $k$-tensor field on $N$. If $p \in M$ and $(y^i)$ are smooth coordinates for $N$ on a neighborhood of $F(p)$, then $F^{*}B$ has the following expression in a neighborhood of $p$:
\begin{align*}
F^{*}\paren{B_{i_1,\ldots,i_k}\,dy^{i_1} \otimes \ldots \otimes dy^{i_k}} &=  \paren{B_{i_1,\ldots,i_k} \circ F} d\paren{y^{i_1} \circ F} \otimes \ldots \otimes \paren{y^{i_k} \circ F}.
\end{align*}
\end{corollary}

\item \begin{remark}
$F^{*}B$ is computed as follows: whereaver you see $y^i$ in the expression for $B$, just substitute the $i$th component function of $F$ and expand.
\end{remark}

\item \begin{exercise} (\textbf{Pullback of a Tensor Field}). \\
Let $M = set{(r, \theta): r > 0, \abs{\theta} < \pi/ 2}$ and $N= \set{(x, y): x > 0}$, and let $F: M \rightarrow \bR^2$ be the smooth map $F(r, \theta) = (r\cos(\theta), r\sin(\theta))$. The pullback of the tensor field $A = x^{-2}dy \otimes dy$ by $F$ can be computed easily by substituting $x = r\cos(\theta)$, $y =r\sin(\theta)$ and simplifying:
\begin{align*}
F^{*}(A) &= F^{*}\paren{x^{-2}dy \otimes dy}  = \paren{r\cos(\theta)}^{-2}\,d\paren{r\sin(\theta)} \otimes d\paren{r\sin(\theta)} \\
&= \paren{r\cos \theta}^{-2}\,\paren{\sin\theta dr + r\cos\theta d\theta} \otimes \paren{\sin \theta dr + r\cos \theta d\theta} \\
&= r^{-2} \tan^2\theta dr \otimes dr + r^{-1} \tan\theta\paren{d\theta \otimes dr + dr \otimes d\theta} + d\theta \otimes d\theta \qed
\end{align*}
\end{exercise}

\end{itemize}

\subsection{Contraction}
\begin{itemize}
\item \begin{proposition} \label{prop: contraction}
Let $V$ be a finite-dimensional vector space. There is a natural (basis-independent) \textbf{isomorphism} between $T^{(k+1, l)}V$ and the space of \textbf{multilinear}
maps
\begin{align*}
\underbrace{V^{*} \xdotx{\times} V^{*}}_{k} \times \underbrace{V \xdotx{\times} V}_{l} \rightarrow V
\end{align*}
\end{proposition}

\item \begin{remark}
For instance, there is an \emph{isomorphism} $: T^{(1,1)}V \rightarrow L(V, V)$ that $(V \otimes \omega) \mapsto (F_{j}^{i})$
where under basis $(E_i)$ and co-basis $(\epsilon^j)$
 \begin{align*}
 V \otimes \omega &= F_{j}^{i}\; E_i \otimes \epsilon^j \\
 \Rightarrow F_{j}^{i} &= (V \otimes \omega)(\epsilon^i, E_j) = \epsilon^i(V)\,\omega(E_j) =V^i\, \omega_j = (\omega(V))_{j}^{i}
 \end{align*}
 \end{remark}


\item \begin{definition}
We can use the result of Proposition \ref{prop: contraction} to define a natural operation called \underline{\emph{\textbf{trace}}} or \underline{\emph{\textbf{contraction}}}, which \emph{lowers the rank of a tensor by $2$}. 

For $F = v\otimes \omega \in T^{(1,1)}V$.  Define the operator $\text{tr}: T^{(1,1)}V  \rightarrow \bR$ is just \emph{\textbf{the trace of $F$}} for  i.e. the sum of the diagonal entries of any matrix representation of $F$. More generally, we define $\text{tr }: T^{(k+1, l+1)}V  \rightarrow T^{(k,l)}V$ by letting $\text{tr }F(\omega^1 \xdotx{,} \omega^k, v_1 \xdotx{,} v_l)$ be the \emph{\textbf{trace}} of the \emph{\textbf{$(1,1)$-tensor}}
\begin{align*}
F(\omega^1 \xdotx{,} \omega^k, \cdot, v_1 \xdotx{,} v_l, \cdot)  \in T^{(1,1)}V
\end{align*}

In terms of a basis, the \emph{\textbf{components}} of $\text{tr }F$ are
\begin{align*}
(\text{tr }F)_{j_1 \xdotx{,} j_l}^{i_1 \xdotx{,} i_k} &= F_{j_1 \xdotx{,} j_l, m}^{i_1 \xdotx{,} i_k, m}.
\end{align*} In other words, just \emph{\textbf{set the last upper and lower indices} \textbf{equal}} and \textbf{\emph{sum}}.
\end{definition}

\item \begin{remark}
We consider a $(1,1)$-tensor $F = v \otimes \omega$. Under standard basis, $v = v^i E_i$ and $\omega = \omega_j\, \epsilon^j$, $F$ has representation
\begin{align*}
 F  &= v \otimes \omega \\
 &= (v^i E_i) \otimes  (\omega_j\, \epsilon^j) \\
 &= (\omega_j\,v^i) E_i \otimes \epsilon^j := F_{j}^{i} \;E_i \otimes \epsilon^j.
\end{align*} There is an isomorphism $T^{(1,1)}V \rightarrow L(V;V)$ as $F \mapsto [F_{j}^{i}]_{j,i}$.
Then the \emph{\textbf{trace}} of $F$ is 
\begin{align*}
\text{tr }(v \otimes \omega) &= \omega(v) \\
&= \omega_i \, v^i\\
&= \text{tr }\paren{\brac{\begin{array}{ccc}
\omega_1 \, v^1 & \ldots & \omega_1 \, v^n\\
\vdots & \ddots & \vdots \\
\omega_n \, v^1 & \ldots & \omega_n \, v^n
\end{array}
}} = \text{tr }[F_{j}^{i}]_{j,i}.
\end{align*}
\end{remark}
\end{itemize}

\subsection{Lie Derivatives of Tensor Fields}



\newpage
\bibliographystyle{plainnat}
\bibliography{book_reference.bib}
\end{document}