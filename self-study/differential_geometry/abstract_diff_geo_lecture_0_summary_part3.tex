\documentclass[11pt]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent %\usepackage{graphicx}
\usepackage{amsmath,amssymb, mathrsfs,  mathtools, dsfont}
\usepackage{tabularx}
\usepackage{tikz-cd}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{xcolor}
%\usepackage[linkbordercolor ={1 1 1} ]{hyperref}
%\usepackage[sf]{titlesec}
\usepackage{natbib}
\usepackage{../../Tianpei_Report}

%\usepackage{appendix}
%\usepackage{algorithm}
%\usepackage{algorithmic}

%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}



\begin{document}
\title{Lecture 0: Summary (part 3)}
\author{ Tianpei Xie}
\date{Oct. 27th., 2022}
\maketitle
\tableofcontents
\newpage
\section{Tensors}
\begin{itemize}
\item 
\begin{definition}
Suppose $V_1,\ldots,V_k$, and $W$ are \emph{vector spaces}. A map $F: V_1\times \ldots \times V_k \rightarrow W$ is said to be \underline{\emph{\textbf{multilinear}}} if it is \emph{\textbf{linear}} as a function of \emph{each variable \textbf{separately}} when the others are held \emph{\textbf{fixed}}: for each $i$,
\begin{align*}
F(v_1,\ldots,\,av_i + a'v_i',\, \ldots, v_k) = a\,F(v_1,\ldots,\,v_i,\, \ldots, v_k)  + a'F(v_1,\ldots,\, v_i',\, \ldots, v_k).
\end{align*} 

\emph{A multilinear function} of \emph{\textbf{one variable}} is just \emph{\textbf{a linear function}}, and a multilinear
function of \emph{\textbf{two variables}} is generally called \emph{\textbf{bilinear}}.
\end{definition}

\item \begin{remark}
Let us write $L(V_1,\ldots,V_k ; W)$ for \emph{\textbf{the set of all multilinear maps from $V_1\times \ldots \times V_k$ to $W$}}. It is a \underline{\emph{\textbf{vector space}}} under the usual operations of \emph{pointwise addition} and \emph{scalar multiplication}:
\begin{align*}
(F'+F)(v_1,\ldots,\,v_i,\, \ldots, v_k) &= F(v_1,\ldots,\,v_i,\, \ldots, v_k)  +  F'(v_1,\ldots,\,v_i,\, \ldots, v_k),\\
(aF)(v_1,\ldots,\,v_i,\, \ldots, v_k)  &= a\,F(v_1,\ldots,\,v_i,\, \ldots, v_k).
\end{align*}
\end{remark}

\item \begin{example} (\emph{\textbf{Some Familiar Multilinear Functions}}).\\
\begin{enumerate}
\item The \underline{\emph{\textbf{dot product}}}, $\inn{\cdot}{\cdot}$ in $\bR^n$ is a \emph{\textbf{scalar-valued bilinear function}} of two vectors, used
to compute \emph{\textbf{lengths}} of vectors and \emph{\textbf{angles}} between them.
\item The \underline{\emph{\textbf{cross product}}}, $(\cdot \times \cdot)$ in $\bR^3$ is a \emph{\textbf{vector-valued bilinear function}} of two vectors, used
to compute \emph{\textbf{areas}} of parallelograms and to find a third vector \emph{\textbf{orthogonal}} to two given ones.
\item The \underline{\emph{\textbf{determinant}}}, $\det(\cdot)$ is a \emph{\textbf{real-valued multilinear function}} of \emph{$n$ vectors} in $\bR^n$, used
to detect \emph{\textbf{linear independence}} and to compute the \emph{\textbf{volume}} of the parallelepiped spanned by the vectors.
\item The \underline{\emph{\textbf{bracket in a Lie algebra $\mathfrak{g}$}}}, $[\cdot, \cdot]$ is a \emph{\textbf{$\mathfrak{g}$-valued bilinear function}} of two elements of $\mathfrak{g}$.
\end{enumerate}
\end{example}

\item \begin{definition}
let $V_1,\ldots,V_k$, $W_1,\ldots,W_l$ be real vector spaces, and suppose $F \in L(V_1,\ldots,V_k ; \bR)$ and $G \in L(W_1,\ldots,W_l; \bR)$. Define a function
$F\, \otimes \, G: V_1\times \ldots \times V_k \times W_1\times \ldots \times W_l \rightarrow \bR$
by
\begin{align}
(F\, \otimes \, G)(v_1,\ldots,\,v_k,\,w_1 \ldots, w_l)&= F(v_1,\ldots,\,v_k)\,G(w_1 \ldots, w_l) \label{eqn: tensor_product}
\end{align} It follows from the multilinearity of $F$ and $G$ that $(F\, \otimes \, G)(v_1,\ldots,\,v_k,\,w_1 \ldots, w_l)$ depends \emph{linearly} on each argument $v_i$ or $w_j$ \emph{separately}, so $F\, \otimes \, G$ is an element of $L(V_1,\ldots,V_k, W_1,\ldots,W_l; \bR)$ called \underline{\emph{\textbf{the tensor product of $F$ and $G$}}}.
\end{definition}

\item \begin{remark}
If $\omega^j \in V_{j}^{*}$ for $j = 1,\ldots,k$, then $\omega^1\otimes \ldots \otimes \omega^k \in L(V_1,\ldots,V_k ; \bR)$ is the \emph{\textbf{multilinear function}} given by
\begin{align}
(\omega^1\otimes \ldots \otimes \omega^k)(v_1, \ldots, v_k)&= \omega^1(v_1)\,\ldots \omega^{k}(v_k).  \label{eqn: tensor_product_functionals}
\end{align} We can see that $\omega^1\otimes \ldots \otimes \omega^k$ is a multilinear extension of the linear functional $\omega$.
\end{remark}

\item 
\begin{proposition} (\textbf{A Basis for the Space of Multilinear Functions}). \\
Let $V_1,\ldots,V_k$ be real vector spaces of dimensions $n_1, \ldots, n_k$, respectively. For each $j \in set{1,\ldots,k}$, let $(E_1^{(j)},\ldots, E_{n_j}^{(j)})$ be a \emph{\textbf{basis}} for $V_j$, and let $(\epsilon_{(j)}^{1}, \ldots, \epsilon_{(j)}^{n_j})$ be the corresponding \textbf{dual basis} for $V_j^{*}$. Then the set
\begin{align*}
\mathfrak{B} &=\set{\epsilon_{(1)}^{i_1} \otimes \ldots \otimes \epsilon_{(k)}^{i_k}: \; 1 \le i_j \le n_j, j=1,\ldots, k}
\end{align*}
\textbf{is a basis} for $L(V_1,\ldots,V_k ; \bR)$, which therefore has \textbf{dimension equal to} $n_1 \ldots  n_k$.
\end{proposition}
\end{itemize}
\subsection{Abstract Tensor Product}
\begin{itemize}
\item \begin{remark}
Intuitively, we want to define the tensor product $v_{1} \otimes \ldots \otimes v_{k}$ by concatenating all vectors into $k$-tuple $(v_{1}, \ldots, v_{k})$. But this naive construction is not enough. We have the following challenges:
\begin{enumerate}
\item The product space $V_1\times \ldots \times V_k$ is \emph{not necessarily a \textbf{vector space}} since we have not define the addition and scalar product for $k$-tuple $(v_{1}, \ldots, v_{k})$ 
\item We want the \emph{\textbf{multilinearity holds}} for the operator on $k$-tuple $(v_{1}, \ldots, v_{k})$, i.e. we want 
\begin{align}
(v_{1}, \ldots, a\,v_i' + b\,v_{i}'',  v_{k}) &= a\,(v_{1}, \ldots, v_i',  v_{k}) + b\,(v_{1}, \ldots, v_{i}'',  v_{k}) \label{eqn: abstract_tensor_product_multilinear}
\end{align} for any $i\in\set{1,\ldots,k}$ and any $a, b\in \bR$.
\end{enumerate}
The above constructions aim to solve these challenges:
\begin{enumerate}
\item Instead of defining the algebraic structure on product space $V_1\times \ldots \times V_k$, we extend it to \emph{\textbf{the free vector space}} $\cF(V_1\times \ldots \times V_k)$, the set of \emph{all linear combinations} of $k$-tuples $(v_{1}, \ldots, v_{k})$. By construction $\cF(V_1\times \ldots \times V_k) \supseteq V_1\times \ldots \times V_k$ and \emph{it is a vector space without defining the algebraic structure} since it use an indicator function to map to $\bR$.

\item Instead of enforcing the \emph{multilinearity} to hold,  we \emph{\textbf{partition the space}} $\cF(V_1\times \ldots \times V_k)$ \emph{\textbf{according to the multilinearitiy rule}}. That is, the set of tuples $(v_{1}, \ldots, a\,v_i' + b\,v_{i}'',  v_{k})$ and $(v_{1}, \ldots, v_i',  v_{k}), (v_{1}, \ldots, v_{i}'',  v_{k})$ that satisfies the equation \eqref{eqn: abstract_tensor_product_multilinear} \emph{will be grouped together} via the equivalence relationship.  The rule is actually a set of linear combinations of (difference of) tuples, denoted as $\cR$.  

Now we instead focusing on the equivalent class itself. By construction, \emph{\textbf{the equivalence class will satisfies the multilinear rule}} \eqref{eqn: abstract_tensor_product_multilinear} (The representer of the equivalence class follow the rule). Thsu $V_1 \otimes \ldots \otimes V_k = \cF(V_1\times \ldots \times V_k) / \cR$ is the tensor product space that we wants. 
\end{enumerate}
\end{remark}

\item \begin{definition}
Now let $V_1,\ldots,V_k$ be real vector spaces. We begin by forming \emph{\textbf{the free vector space}} $\cF(V_1\times \ldots \times V_k)$, which is the set of all finite formal linear combinations of k-tuples $(v_1,\ldots,\,v_k)$ with $v_i \in V_i$ for $i = 1,\ldots,k$. Let $\cR$ be the \emph{\textbf{subspace}} of $\cF(V_1\times \ldots \times V_k)$ \emph{spanned} by all elements of the following forms:
\begin{align}
&(v_1,\ldots,\,a\,v_i, \ldots, v_k) - a\,(v_1,\ldots,\,v_i, \ldots, v_k) \nonumber\\
&(v_1,\ldots,\,v_i + v_i', \ldots, v_k) - (v_1,\ldots,\,v_i, \ldots, v_k) -  (v_1,\ldots,\ v_i', \ldots, v_k)  \label{eqn: abstract_tensor_product_subspace_equiv}
\end{align} with $v_j, v_j' \in V_j$, $i \in \set{1,\ldots,k}$, and $a \in \bR$.

Define \underline{\emph{\textbf{the tensor product of the spaces $V_1,\ldots,V_k$}}}, denoted by $V_1 \otimes \ldots \otimes V_k$,
to be the following \emph{\textbf{quotient vector space}}:
\begin{align*}
V_1 \otimes \ldots \otimes V_k &= \cF(V_1\times \ldots \times V_k) / \cR
\end{align*}
and let $\Pi:  \cF(V_1\times \ldots \times V_k)  \rightarrow V_1 \otimes \ldots \otimes V_k$ be \emph{\textbf{the natural projection}}. The \emph{\textbf{equivalence class}} of an element $(v_1, \ldots, v_k)$ in $V_1 \otimes \ldots \otimes V_k$ is denoted by
\begin{align}
v_1 \otimes \ldots \otimes v_k &= \Pi(v_1, \ldots, v_k)\label{eqn: abstract_tensor_product_space_element}
\end{align}
and is called \underline{\emph{\textbf{the (abstract) tensor product of $(v_1, \ldots, v_k)$}}}. 

It follows from the definition that abstract tensor products satisfy
\begin{align*}
v_1 \otimes \ldots \otimes (a\,v_i)  \otimes \ldots \otimes v_k  &= a (v_1  \otimes \ldots  \otimes v_i \otimes \ldots \otimes v_k),\\
v_1 \otimes \ldots \otimes (v_i + v_i') \otimes \ldots \otimes v_k &= (v_1 \otimes \ldots \otimes v_i \otimes \ldots \otimes v_k) +  (v_1 \otimes \ldots \otimes v_i' \otimes \ldots \otimes v_k) 
\end{align*}
\end{definition}

\item \begin{proposition} (\textbf{Characteristic Property of the Tensor Product Space}).\\
Let  $V_1,\ldots,V_k$ be finite-dimensional real vector spaces. If $A: V_1\times \ldots \times V_k \rightarrow X$ is \textbf{any multilinear map} into a vector space $X$, then there is a \textbf{unique linear map} $\widetilde{A}: V_1\otimes \ldots \otimes V_k \rightarrow X$ such that the following diagram commutes:
\begin{equation}
  \begin{tikzcd}
    V_1\times \ldots \times V_k  \arrow{r}{A}  \arrow[swap]{d}{\pi}  & X  \\
    V_1\otimes \ldots \otimes V_k \arrow[ur,  dashrightarrow, swap, "\widetilde{A}"] &,  
  \end{tikzcd} \label{eqn: diagram_tensor_product}
\end{equation} where $\pi$ is the map $\pi(v_1, \ldots, v_k) = v_1 \otimes \ldots \otimes v_k$.
\end{proposition}

\item \begin{remark}
The \emph{characteristic property of the tensor product space} states that \emph{any mulilinear function} $\tau: V_1\times \ldots \times V_k \rightarrow \bR$ descends into a \emph{linear map} $\widetilde{\tau}:V_1\otimes \ldots \otimes V_k \rightarrow \bR$ so that any linear combinations of tensor products $v_{i_1} \otimes \ldots \otimes v_{i_k}$ is expressed as
\begin{align*}
\widetilde{\tau}\paren{a^{i_1\ldots i_k}\,v_{i_1} \otimes \ldots \otimes v_{i_k}} &= a^{i_1\ldots i_k}\,\tau(v_{i_1}, \ldots, v_{i_k})
\end{align*}
\end{remark}

\item \begin{proposition} (\textbf{A Basis for the Tensor Product Space}).\\
Suppose $V_1,\ldots,V_k$ are real vector spaces of dimensions $n_1 \ldots  n_k$, respectively. For each $j = 1,\ldots,k$, suppose $(E_1^{(j)},\ldots, E_{n_j}^{(j)})$ is a \textbf{basis} for $V_j$. Then the set
\begin{align*}
\mathfrak{C}&= \set{E^{(1)}_{i_1} \otimes \ldots \otimes E^{(k)}_{i_k}: \; 1 \le i_j \le n_j, j=1,\ldots, k}
\end{align*} \textbf{is a basis} for $V_1\otimes \ldots \otimes V_k$, which therefore has \textbf{dimension equal to} $n_1 \ldots  n_k$.
\end{proposition}

\item \begin{proposition} (\textbf{Abstract vs. Concrete Tensor Products}).\citep{lee2003introduction}\\
If $V_1,\ldots,V_k$  are finite-dimensional vector spaces, there is a \underline{\textbf{canonical isomorphism}}
\begin{align}
V_1^{*}\otimes \ldots \otimes V_k^{*} &\simeq L(V_1,\ldots,V_k ; \bR) \label{eqn: abstract_covector_tensor_product_iso}
\end{align} under which the \textbf{abstract tensor product} defined by \eqref{eqn: abstract_tensor_product_space_element} corresponds to the \textbf{tensor product of covectors} defined by \eqref{eqn: tensor_product_functionals}.
\end{proposition}

\item \begin{remark}
Since we are assuming the vector spaces are all finite-dimensional, we can also identify each $V_j$ with its second dual space $V_j^{**}$, and thereby obtain \emph{\textbf{another canonical identification}}
\begin{align}
V_1\otimes \ldots \otimes V_k &\simeq L(V_1^{*},\ldots,V_k^{*}; \bR) \label{eqn: abstract_vector_tensor_product_iso}
\end{align}
\end{remark}

\item \begin{remark} (\textbf{Kronecker Product vs. Tensor Product})\\
The two notions represent operations on different objects: Kronecker product on matrices; tensor product on linear maps between vector spaces. But there is a connection: Given two matrices, we can think of them as representing linear maps between vector spaces equipped with a chosen basis. \emph{\textbf{The Kronecker product of the two matrices then represents the tensor product of the two linear maps}}. (This claim makes sense because the tensor product of two vector spaces with distinguished bases comes with a distinguish basis.)
\end{remark}

\item \begin{remark}
As we see, the space of tensor product defines a set of \underline{\emph{\textbf{parallel linear systems}}}. All \emph{\textbf{sub-systems}} are \underline{\emph{\textbf{independent}}}.  Each sub-system has its own \emph{\textbf{basis}}, its own \emph{\textbf{linear operations}} and its own \emph{\textbf{representation}}. The tensor product operation \emph{group} these independent linear systems together and \emph{consider them as a whole}. 

For \emph{\textbf{the whole system perspective}}, its representations are collected locally and then concatenated together. The linear map on \emph{\textbf{the concatenated representation}} is essentially the same as \emph{\textbf{applying linear map}} in each sub-system and \emph{\textbf{multiplying}} them together. This is the same as computing the joint distribution by product of marginal distributions. \emph{\textbf{The multiplication principle}} is applied when counting the size of the whole system.

The space of tensor product $V_1\otimes \ldots \otimes V_k$ reflect a \emph{\textbf{divide-and-conquer strategy}} and a \emph{\textbf{local-global strategy}} to study the complex functions such as \emph{multilinear functionals} $\alpha(v_1, \ldots, v_k)$. In the former, we study it by \emph{\textbf{perturbing} the input of each sub-system}. In the latter, we think of it as \emph{\textbf{a linear map}} on the k-tensors $v_1 \otimes \ldots \otimes v_k$.
\end{remark}

\end{itemize}
\subsection{Covariant and Contravariant Tensors on a Vector Space}
\begin{itemize}
\item \begin{definition}
Let $V$ be a finite-dimensional real vector space. If $k$ is a positive integer, \underline{\emph{\textbf{a covariant $k$-tensor}}} on $V$ is an element of \emph{the $k$-fold tensor product} $V^{*}\otimes \ldots \otimes V^{*}$, which we typically think of as \underline{\emph{\textbf{a real-valued multilinear function of $k$ elements of $V$}}}:
\begin{align*}
\alpha: \underbrace{V\times \ldots \times V}_{k} \rightarrow \bR
\end{align*}
The number $k$ is called \emph{\textbf{the rank of $\alpha$}}. A $0$-tensor is, by convention, just a real number (a real-valued function depending multilinearly on \emph{no vectors}!).  

We denote \underline{\emph{\textbf{the vector space}} of \emph{\textbf{all covariant $k$-tensors on $V$}}} by the shorthand notation
\begin{align*}
T^{k}V^{*} &= \underbrace{V^{*}\otimes \ldots \otimes V^{*}}_{k}
\end{align*}
\end{definition}

\item \begin{example} (\emph{\textbf{Covariant Tensors}}). \\
Let $V$ be a finite-dimensional vector space.
\begin{enumerate}
\item \emph{Every linear functional} $\omega: V \rightarrow \bR$ is multilinear, so \emph{\textbf{a covariant $1$-tensor is just a covector}}. Thus, $T^{1}(V^{*})$ is equal to $V^{*}$.
\item  \emph{A covariant $2$-tensor} on $V$ is \emph{a real-valued \textbf{bilinear function}} of two vectors, also called \emph{\textbf{a bilinear form}}. One example is \emph{the dot product} on $\bR^n$. More generally, \emph{\textbf{every inner product is a covariant $2$-tensor}}.
\item \emph{The determinant}, thought of as a function of $n$ vectors, is \emph{\textbf{a covariant $n$-tensor on $\bR^n$}}.
\end{enumerate}
\end{example}

\item \begin{definition}
For any finite-dimensional real vector space $V$, we define \emph{the space of \textbf{contravariant tensors} on $V$} of \emph{\textbf{rank $k$}} to be the vector space
\begin{align*}
T^{k}V &= \underbrace{V\otimes \ldots \otimes V}_{k}
\end{align*} In particular, $T^1(V) = V$, and by convention $T^0(V) = \bR$. Because we are assuming that $V$ is finite-dimensional, it is possible to identify this space with \emph{the set of \textbf{multilinear functionals of $k$ covectors}}:
\begin{align*}
T^{k}V &= \set{\text{multilinear functionals }\alpha: \underbrace{V^{*}\times \ldots \times V^{*}}_{k} \rightarrow \bR }
\end{align*}
\end{definition}

\item \begin{definition}
Even more generally, for any nonnegative integers $k, l$, we define \emph{the space of \underline{\textbf{mixed tensors on $V$ of type $(k,l)$}}} as
\begin{align*}
T^{(k,l)}V &= \underbrace{V\otimes \ldots \otimes V}_{k} \otimes \underbrace{V^{*}\otimes \ldots \otimes V^{*}}_{l}
\end{align*}
\end{definition}

\item \begin{corollary}
Let $V$ be an n-dimensional real vector space. Suppose $(E_i)$ is any basis for $V$ and $(\epsilon^{j})$ is the dual basis for $V^{*}$. Then the following sets constitute
bases for the tensor spaces over $V$:
\begin{align}
&\set{\epsilon^{i_1} \otimes \ldots \otimes \epsilon^{i_k}: \; 1 \le i_s \le n, s=1,\ldots, k} \text{ is basis for }T^{k}V^{*};  \nonumber\\
&\set{E_{i_1} \otimes \ldots \otimes E_{i_k}: \; 1 \le i_s \le n, s=1,\ldots, k}   \text{ is basis for }T^{k}V; \nonumber\\
&\set{E_{i_1} \otimes \ldots \otimes E_{i_k} \otimes \epsilon^{j_1} \otimes \ldots \otimes \epsilon^{j_l}: \; 1 \le i_1, \ldots i_k, j_1,\ldots, j_l \le n} \text{ is basis for }T^{(k, l)}V;  \label{eqn: basis_tensor_space_rank_k}
\end{align}
Therefore, $\text{dim }T^{k}V^{*} = \text{dim }T^{k}V = n^{k}$ and $\text{dim }T^{(k, l)}V = n^{k + l}$
\end{corollary}

\item \begin{remark} (\emph{\textbf{Coordinate Representation of Covariant $k$-Tensor}})\\
In particular, once a basis is chosen for $V$, every \emph{\textbf{covariant $k$-tensor}} $\alpha \in T^{k}(V^{*})$ can be written uniquely in the form
\begin{align}
\alpha &= \alpha_{i_1,i_2,\ldots, i_k} \epsilon^{i_1} \otimes \ldots \otimes \epsilon^{i_k}  \label{eqn: coordinate_represent_covariant_k_tensor}
\end{align}
where the $n^k$ coefficients $\alpha_{i_1,i_2,\ldots, i_k}$ are determined by
\begin{align}
\alpha_{i_1,i_2,\ldots, i_k} &= \alpha\paren{E_{i_1} , \ldots , E_{i_k}} \label{eqn: coordinate_represent_covariant_k_tensor_coefficient}
\end{align}

For instance, covariant $2$-tensor is bilinear form. Every \emph{bilinear form} can be written as $\beta = \beta_{i,j} \epsilon^1 \otimes \epsilon^2$, for some uniquely determined $n\times n$ matrix $(\beta_{i,j})$.
\end{remark}
\end{itemize}
\subsection{Symmetric and Alternating Tensors}
\subsubsection{Symmetric Tensors}
\begin{itemize}
\item  \begin{definition}
Let $V$ be a finite-dimensional vector space. A \emph{covariant $k$-tensor $\alpha$} on $V$ is said to be \emph{\textbf{symmetric}} if its value is \textit{unchanged} by \emph{\textbf{interchanging}} any pair of arguments:
\begin{align*}
\alpha\paren{v_1, \ldots, v_i, \ldots, v_j, \ldots, v_k} &= \alpha\paren{v_1, \ldots, v_j, \ldots, v_i, \ldots, v_k} 
\end{align*} whenever $i \le i< j \le k$.
\end{definition}

\item \begin{definition}
The set of \emph{\textbf{symmetric covariant $k$-tensors}} is a linear subspace of the space $T^k(V^{*})$ of all covariant $k$-tensors on $V$; we denote this subspace by \underline{$\Sigma^{k}(V^{*})$}

There is a \emph{\textbf{natural projection}} from $T^k(V^{*})$ to $\Sigma^{k}(V^{*})$ defined as follows. First, let
$S_k$ denote \emph{\textbf{the symmetric group on $k$ elements}}, that is, the group of \emph{\textbf{permutations}} of the set $\set{1,\ldots,k}$. Given a $k$-tensor $\alpha$ and a permutation $\sigma \in S_{k}$, we define a new $k$-tensor $^{\sigma}\alpha$ by
\begin{align*}
^{\sigma}\alpha(v_1, \ldots, v_k) &= \alpha(v_{\sigma(1)}, \ldots, v_{\sigma(k)})
\end{align*}
Note that $^{\tau}(^{\sigma}\alpha) = ^{\tau \sigma}\alpha$ where $\tau\sigma$ represents the composition of $\tau$ and $\sigma$, that is, $\tau\sigma(i) = \tau(\sigma(i))$.(\emph{This is the reason for putting $\sigma$ before $\alpha$ in the notation $^{\sigma}\alpha$ instead of after it.}) 

We define a \emph{\textbf{projection}} $\text{Sym}: T^k(V^{*}) \rightarrow \Sigma^{k}(V^{*})$ called \underline{\emph{\textbf{symmetrization}}} by
\begin{align*}
\text{Sym }\alpha &= \frac{1}{k!}\sum_{\sigma \in S_{k}}{^{\sigma}\alpha}
\end{align*}
More explicitly, this means that
\begin{align*}
\text{Sym }\alpha(v_1, \ldots, v_k) &= \frac{1}{k!}\sum_{\sigma \in S_{k}} \alpha(v_{\sigma(1)}, \ldots, v_{\sigma(k)})
\end{align*}
\end{definition}

\item \begin{proposition} (\textbf{Properties of Symmetrization}).\\
 Let $\alpha$ be a covariant tensor on a finite-dimensional vector space.
 \begin{enumerate}
 \item $\text{Sym }\alpha $ is symmetric.
 \item $\text{Sym }\alpha = \alpha$ if and only if $\alpha$ is symmetric.
 \end{enumerate}
\end{proposition}

\item \begin{definition}
If $\alpha \in \Sigma^{k}(V^{*})$ and $\beta \in \Sigma^{k}(V^{*})$, we define their \emph{\textbf{symmetric product}} to be the $(k + l)$-tensor $\alpha\,\beta$(denoted by juxtaposition with no intervening product symbol) given by
\begin{align*}
\alpha\,\beta &= \text{Sym }(\alpha \otimes \beta)
\end{align*}
More explicitly, the action of $\alpha\,\beta$ on vectors $v_1,\ldots,v_{k+l}$ is given by
\begin{align*}
\alpha\,\beta(v_1,\ldots,v_{k+l}) &= \frac{1}{(k+l)!}\sum_{\sigma \in S_{k+l}}\alpha(v_{\sigma(1)},\ldots,v_{\sigma(k)})\beta(v_{\sigma(k+1)},\ldots,v_{\sigma(k+l)})
\end{align*}
\end{definition}

\item \begin{proposition} (\textbf{Properties of the Symmetric Product}).\\
\begin{enumerate}
\item The symmetric product is \textbf{symmetric} and \textbf{bilinear}: for all symmetric tensors $\alpha, \beta, \gamma$ and all $a, b \in \bR$,
\begin{align*}
\alpha\, \beta &= \beta\, \alpha \\
(a\,\alpha + b\,\beta)\,\gamma &= a\,\alpha\,\gamma + b\,\beta\,\gamma = \gamma\, (a\,\alpha + b\,\beta)
\end{align*}
\item  If $\alpha$ and $\beta$ are \textbf{covectors}, then
\begin{align*}
\alpha\,\beta &=\frac{1}{2}\paren{\alpha \otimes \beta + \beta \otimes \alpha}.
\end{align*}
\end{enumerate}
\end{proposition}
\end{itemize}
\subsubsection{Alternating Tensors}
\begin{itemize}
\item \begin{definition}
Assume that $V$ is a finite-dimensional real vector space. \emph{\textbf{A covariant k-tensor}} $\alpha$ on V is said to be \emph{\textbf{alternating}} (or \emph{\textbf{antisymmetric}} or \emph{\textbf{skew-symmetric}}) if it \emph{\textbf{changes sign}} whenever two of its arguments are \emph{interchanged}. This means that for all vectors $v_1,\ldots,v_k \in V$ and every pair of distinct indices $i$, $j$ it satisfies
\begin{align*}
\alpha\paren{v_1, \ldots, v_i, \ldots, v_j, \ldots, v_k} &= -\alpha\paren{v_1, \ldots, v_j, \ldots, v_i, \ldots, v_k} 
\end{align*}
\emph{Alternating covariant k-tensors} are also variously called \underline{\emph{\textbf{exterior forms}}, \emph{\textbf{multicovectors}}}, or \underline{\emph{\textbf{k-covectors}}}. 

The subspace of \emph{\textbf{all alternating covariant $k$-tensors}} on $V$ is denoted by \underline{$\Lambda^{k}(V^{*}) \subseteq T^k(V^{*}) $}.
\end{definition}

\item \begin{definition}
Recall that for any permutation $\sigma \in S_k$, \emph{\textbf{the sign of $\sigma$}}, denoted by $\text{sgn }\sigma$, is equal to $+1$ if \emph{$\sigma$ is \textbf{even}} (i.e., can be written as \emph{a composition of an \textbf{even} number of transpositions}), and $-1$ if $\sigma$ is \emph{\textbf{odd}}.
\end{definition}

\item 
\begin{lemma}
The following statements are equivalent for a covariant $k$-tensor $\alpha$:
\begin{enumerate}
\item $\alpha$ is \textbf{alternating};
\item For any vectors $v_1,\ldots,v_k \in V$, and \textbf{any permutation} $\sigma \in S_k$
\begin{align*}
\alpha\paren{v_{\sigma(1)}, \ldots,  v_{\sigma(k)}} &=(\text{sgn }\sigma)\alpha\paren{v_1, \ldots,  v_k}
\end{align*}
\item With respect to any basis, the components $\alpha_{i_1,\ldots,i_k}$ of $\alpha$ change sign whenever two indices are interchanged.
\end{enumerate}
\end{lemma}

\item \begin{lemma}
Let $\alpha$ be a covariant $k$-tensor on a finite-dimensional vector space $V$. The following are equivalent:
\begin{enumerate}
\item $\alpha$ is \textbf{alternating}.
\item $\alpha(v_1,\ldots, v_k) = 0$ whenever the $k$-tuple $(v_1,\ldots, v_k)$ is \textbf{linearly dependent}.
\item $\alpha$ gives the value zero whenever \textbf{two of its arguments} are \textbf{equal}:
\begin{align*}
\alpha(v_1, \ldots, w, \ldots, w, v_k) &= 0.
\end{align*}
\end{enumerate}
\end{lemma}

\item \begin{definition}
We define a projection $\text{Alt }: T^k(V^{*}) \rightarrow \Lambda^k(V^{*})$, called \underline{\emph{\textbf{alternation}}}, as follows:
\begin{align*}
\text{Alt }\alpha &=\frac{1}{k!} \sum_{\sigma \in S_k} \paren{\sign{\sigma}}\paren{^{\sigma}\alpha}
\end{align*} where $S_k$ is \emph{the symmetric group on $k$ elements}. More explicitly, this means
\begin{align*}
\text{Alt }\alpha(v_1, \ldots, v_k) &= \frac{1}{k!} \sum_{\sigma \in S_k} \paren{\sign{\sigma}}\alpha(v_{\sigma(1)}, \ldots, v_{\sigma(k)}).
\end{align*}
\end{definition}

\item \begin{proposition}(\textbf{Properties of Alternation}). \\
Let $\alpha$ be a covariant tensor on a finite-dimensional vector space.
\begin{enumerate}
\item $\text{Alt }\alpha$ is alternating.
\item $\text{Alt }\alpha = \alpha$ if and only if $\alpha$ is alternating.
\end{enumerate}
\end{proposition}
\end{itemize}


\subsection{Tensor Fields and Tensor Bundle}
\begin{itemize}
\item \begin{definition}
Now let $M$ be a smooth manifold with or without boundary. We define the \underline{\emph{\textbf{bundle of covariant $k$-tensors}}} on $M$ by
\begin{align*}
T^kT^{*}M &= \bigsqcup_{p \in M}T^k\paren{T_{p}^{*}M} 
\end{align*}

Analogously, we define \underline{\emph{\textbf{the bundle of contravariant $k$-tensors}}} by
\begin{align*}
T^kTM &= \bigsqcup_{p \in M}T^k\paren{T_{p}M} 
\end{align*}
and \underline{\emph{\textbf{the bundle of mixed tensors of type $(k,l)$}}} by
\begin{align*}
T^{(k,l)}TM &= \bigsqcup_{p \in M}T^{(k,l)}\paren{T_{p}M}
\end{align*}
\end{definition}

\item \begin{remark}
There are natural identifications
\begin{align*}
T^{(0,0)}TM &= T^{0}T^{*}M = T^{0}TM = M \times \bR;\\
T^{(0,1)}TM &= T^{1}T^{*}M = T^{*}M;\\
T^{(1,0)}TM &= T^{1}TM = TM;\\
T^{(0,k)}TM &= T^{k}T^{*}M; \\
T^{(k,0)}TM &= T^{k}TM.
\end{align*}
Any one of these bundles is called \emph{\textbf{a tensor bundle over $M$}}. (Thus, the tangent and cotangent bundles are special cases of tensor bundles.) 
\end{remark}


\item \begin{definition}
\emph{A \textbf{section} of a tensor bundle} is called \emph{a \textbf{(covariant, contravariant, or mixed) tensor field} on $M$}. \emph{\textbf{A smooth tensor field}} is a section that is smooth in the usual sense of smooth sections of vector bundles. 
\end{definition}

\item \begin{remark}
The \emph{\textbf{spaces of smooth sections}} of these tensor bundles, $\Gamma\paren{T^kT^{*}M}$, $\Gamma\paren{T^kTM}$, and $\Gamma(T^{(k,l)}TM)$, are \underline{\emph{\textbf{infinite-dimensional vector spaces} over} $\bR$}, and \emph{\textbf{modules}} over $\cC^{\infty}(M)$. 
We also denote the \emph{space of smooth covariant tensor fields} as 
\begin{align*}
\mathcal{T}^{k}(M) &= \Gamma\paren{T^kT^{*}M}.
\end{align*}
\end{remark}

\item \begin{remark} (\emph{\textbf{Coordinate Representation of Tensor Fields}})\\
In any smooth local coordinates $(x^i)$, sections of these bundles can be written (using the summation convention) as
\begin{align}
A &= \left\{
\begin{array}{ll}
A_{i_1,\ldots,i_k}\;dx^{i_1}\otimes \ldots \otimes dx^{i_k},                                        & A \in \Gamma\paren{T^kT^{*}M};\\[10pt]
A^{i_1,\ldots,i_k}\;\dfrac{\partial}{\partial x^{i_1}} \otimes \ldots \otimes \dfrac{\partial}{\partial x^{i_k}}, & A \in \Gamma\paren{T^kTM};\\[10pt]
A^{i_1,\ldots,i_k}_{j_1,\ldots,j_l}\;\dfrac{\partial}{\partial x^{i_1}} \otimes \ldots \otimes \dfrac{\partial}{\partial x^{i_k}} \otimes dx^{j_1}\otimes \ldots \otimes dx^{j_k} 
&  A \in \Gamma\paren{T^{(k,l)}TM};
\end{array}
 \right.\label{eqn: coordinate_tensor_fields}
\end{align} The functions $A_{i_1,\ldots,i_k}$, $A^{i_1,\ldots,i_k}$, or $A^{i_1,\ldots,i_k}_{j_1,\ldots,j_l}$ are called the \emph{\textbf{component functions}} of $A$
in the chosen coordinates. 
\end{remark}

\item \begin{proposition} (\textbf{Smoothness Criteria for Tensor Fields}).\\
Let $M$ be a smooth manifold with or without boundary, and let $A: M \rightarrow T^kT^{*}M$ be a rough section. The following are equivalent.
\begin{enumerate}
\item $A$ is smooth.
\item In \textbf{every} smooth coordinate chart, the \textbf{component functions} of $A$ are smooth.
\item Each point of $M$ is contained in \textbf{some} coordinate chart in which A has \textbf{smooth component functions}.
\item If $X_1,\ldots,X_k \in \mathfrak{X}(M)$, then the function $A(X_1,\ldots,X_k): M \rightarrow \bR$, defined by
\begin{align}
A(X_1,\ldots,X_k)(p) &= A_{p}\paren{X_1\bigr|_{p},\ldots,X_k\bigr|_{p}} \label{eqn: covariant_tensor_field_induce_smooth_function}
\end{align} is smooth
\item Whenever $X_1,\ldots,X_k$ are smooth vector fields defined on \textbf{some open subset} $U\subseteq M$, the function $A(X_1,\ldots,X_k)$ is smooth on $U$.
\end{enumerate}
\end{proposition}

\item \begin{lemma} (\textbf{Tensor Characterization Lemma)}.\citep{lee2003introduction}\\
A map 
\begin{align}
\mathcal{A}: \underbrace{\mathfrak{X}(M)\times \ldots \times \mathfrak{X}(M)}_{k} \rightarrow  \cC^{\infty}(M).  \label{eqn: tensor_characterization}
\end{align} is \textbf{induced} by a \textbf{smooth covariant $k$-tensor field} $A$ as in \eqref{eqn: covariant_tensor_field_induce_smooth_function} if and only if it is \textbf{multilinear} over $\cC^{\infty}(M)$.
\end{lemma}

\item \begin{definition} For symmetric and alternating tensor field, we have the following definition:
\begin{enumerate}
\item \emph{\textbf{A symmetric tensor field}} on a manifold (with or without boundary) is simply \emph{a covariant tensor field} whose value at each point is a \emph{symmetric tensor}. 

The \emph{\textbf{symmetric product}} of two or more tensor fields is defined pointwise, just like the tensor product. Thus, for example, if $A$ and $B$ are \emph{smooth covector fields}, their symmetric product is \emph{\textbf{the smooth $2$-tensor field $AB$}}, which is given by
\begin{align*}
AB &= \frac{1}{2}\paren{A \otimes B} + \frac{1}{2}\paren{B \otimes A}.
\end{align*}


\item \emph{\textbf{Alternating tensor fields}} are called \underline{\emph{\textbf{differential forms}}};
\end{enumerate}
\end{definition}
\end{itemize}

\subsection{Pullbasks of Tensor Fields}
\begin{itemize}
\item \begin{definition}
Suppose $F: M \rightarrow N$ is a smooth map. For any point $p \in M$ and any \emph{$k$-tensor} $\alpha \in T^k(T^{*}_{F(p)}N)$, we define a tensor $dF_p^{*}(\alpha) \in T^k\paren{T^{*}_{p}M}$, called \underline{\emph{\textbf{the pointwise pullback}}} of $\alpha$ by $F$ at $p$, by
\begin{align*}
dF_p^{*}(\alpha)(v_1, \ldots, v_k) &= \alpha\paren{dF_p(v_1), \ldots, dF_p(v_k)}
\end{align*} for any $v_1,\ldots,v_k \in T_pM$. 
\end{definition}

\item \begin{definition}
If $A$ is \emph{a covariant k-tensor field on $N$}, we define a rough\textbf{\emph{$k$-tensor field $F^{*}A$ on $M$}}; called  \underline{\emph{\textbf{the pullback of $A$ by $F$}}}, by
\begin{align*}
(F^{*}A)_{p} &= dF_p^{*}(A_{F(p)}).
\end{align*}
This tensor field acts on vectors $v_1,\ldots,v_k \in T_pM$ by
\begin{align*}
(F^{*}A)_{p}(v_1, \ldots, v_k) &= A_{F(p)}\paren{dF_p(v_1), \ldots, dF_p(v_k)}.
\end{align*}
\end{definition}

\item \begin{proposition} (\textbf{Properties of Tensor Pullbacks}). \\
Suppose $F: M \rightarrow N$ and $G: N \rightarrow $P are smooth maps, $A$ and $B$ are covariant tensor fields on $N$, and $f$ is a real-valued function on $N$.
\begin{enumerate}
\item $F^{*}(fB) = (f \circ F)\,F^{*}(B)$
\item $F^{*}(A \otimes B) = F^{*}A \otimes F^{*}(B)$
\item $F^{*}(A + B) = F^{*}A + F^{*}(B)$
\item $F^{*}(B)$ is a (\textbf{continuous}) \textbf{tensor field}, and is \textbf{smooth} if $B$ is \textbf{smooth}.
\item $(G \circ F)^{*}B = F^{*}(G^{*}B).$
\item $(\text{Id}_N)^{*}B = B.$
\end{enumerate}
\end{proposition}

\item \begin{corollary} (\textbf{Coordinate Representation of Pullback Tensor Fields})\\
Let $F: M \rightarrow N$  be smooth, and let $B$ be a covariant $k$-tensor field on $N$. If $p \in M$ and $(y^i)$ are smooth coordinates for $N$ on a neighborhood of $F(p)$, then $F^{*}B$ has the following expression in a neighborhood of $p$:
\begin{align*}
F^{*}\paren{B_{i_1,\ldots,i_k}\,dy^{i_1} \otimes \ldots \otimes dy^{i_k}} &=  \paren{B_{i_1,\ldots,i_k} \circ F} d\paren{y^{i_1} \circ F} \otimes \ldots \otimes \paren{y^{i_k} \circ F}.
\end{align*}
\end{corollary}

\item \begin{remark}
$F^{*}B$ is computed as follows: whereaver you see $y^i$ in the expression for $B$, just substitute the $i$th component function of $F$ and expand.
\end{remark}
\end{itemize}
\subsection{Contraction}
\begin{itemize}
\item \begin{proposition} \label{prop: contraction}
Let $V$ be a finite-dimensional vector space. There is a natural (basis-independent) \textbf{isomorphism} between $T^{(k+1, l)}V$ and the space of \textbf{multilinear}
maps
\begin{align*}
\underbrace{V^{*} \xdotx{\times} V^{*}}_{k} \times \underbrace{V \xdotx{\times} V}_{l} \rightarrow V
\end{align*}
\end{proposition}


\item \begin{definition}
We can use the result of Proposition \ref{prop: contraction} to define a natural operation called \underline{\emph{\textbf{trace}}} or \underline{\emph{\textbf{contraction}}}, which \emph{lowers the rank of a tensor by $2$}. 

For $F = v\otimes \omega \in T^{(1,1)}V$.  Define the operator $\text{tr}: T^{(1,1)}V  \rightarrow \bR$ is just \emph{\textbf{the trace of $F$}} for  i.e. the sum of the diagonal entries of any matrix representation of $F$. More generally, we define $\text{tr }: T^{(k+1, l+1)}V  \rightarrow T^{(k,l)}V$ by letting $\text{tr }F(\omega^1 \xdotx{,} \omega^k, v_1 \xdotx{,} v_l)$ be the \emph{\textbf{trace}} of the \emph{\textbf{$(1,1)$-tensor}}
\begin{align*}
F(\omega^1 \xdotx{,} \omega^k, \cdot, v_1 \xdotx{,} v_l, \cdot)  \in T^{(1,1)}V
\end{align*}

In terms of a basis, the \emph{\textbf{components}} of $\text{tr }F$ are
\begin{align*}
(\text{tr }F)_{j_1 \xdotx{,} j_l}^{i_1 \xdotx{,} i_k} &= F_{j_1 \xdotx{,} j_l, m}^{i_1 \xdotx{,} i_k, m}.
\end{align*} In other words, just \emph{\textbf{set the last upper and lower indices} \textbf{equal}} and \textbf{\emph{sum}}.
\end{definition}

\item \begin{remark}
We consider a $(1,1)$-tensor $F = v \otimes \omega$. Under standard basis, $v = v^i E_i$ and $\omega = \omega_j\, \epsilon^j$, $F$ has representation
\begin{align*}
 F  &= v \otimes \omega \\
 &= (v^i E_i) \otimes  (\omega_j\, \epsilon^j) \\
 &= (\omega_j\,v^i) E_i \otimes \epsilon^j := F_{j}^{i} \;E_i \otimes \epsilon^j
\end{align*} There is an isomorphism $T^{(1,1)}V \rightarrow L(V;V)$ as $F \mapsto [F_{j}^{i}]_{j,i}$.
Then the \emph{\textbf{trace}} of $F$ is 
\begin{align*}
\text{tr }(v \otimes \omega) &= \omega(v) \\
&= \omega_i \, v^i\\
&= \text{tr }\paren{\brac{\begin{array}{ccc}
\omega_1 \, v^1 & \ldots & \omega_1 \, v^n\\
\vdots & \ddots & \vdots \\
\omega_n \, v^1 & \ldots & \omega_n \, v^n
\end{array}
}} = \text{tr }[F_{j}^{i}]_{j,i}
\end{align*}
\end{remark}

\item \begin{remark}
We have the formula for a $(k,l)$-tensor field $F$
\begin{align}
F(\omega^1 \xdotx{,} \omega^k, V_1 \xdotx{,} V_{l}) &= \underbrace{\text{tr}\xdotx{\circ} \text{tr}}_{k+l}\paren{F \otimes \omega^1 \xdotx{\otimes} \omega^k \otimes V_1 \xdotx{\otimes} V_l}, \label{eqn: tensor_trace_computation}
\end{align} where each trace operator acts on an upper index of $F$ and the lower index of the corresponding $1$-form, or a lower index of $F$ and the upper index of the corresponding vector field.

For instance, for covariant $2$-tensor field $g =\omega^1 \otimes \omega^2$:
\begin{align*}
g(X, Y) &= \text{tr}\paren{\text{tr}(\omega^1 \otimes \omega^2 \otimes X \otimes Y )} \\
&= \text{tr}\paren{\text{tr}(\omega^2 \otimes Y)\, \omega^1 \otimes X} \\
&= \text{tr}\paren{(\omega^2(Y))\, \omega^1 \otimes X} \\
&= (\omega^2(Y))\,\text{tr}\paren{\omega^1 \otimes X} \\
&= (\omega^2(Y))\,(\omega^1(X))\,
\end{align*}
\end{remark}
\end{itemize}

\section{Differential Forms}
\subsection{Elementary $k$-covectors}
\begin{itemize}
\item \begin{definition}
Given a positive integer $k$, \emph{\textbf{an ordered $k$-tuple}} $I=(i_1,\ldots, i_k)$ of \emph{positive integers} is called \emph{a \underline{\textbf{multi-index}} of length $k$}. If $I$ is such a multi-index and  $\sigma \in S_k$ is a permutation of $\set{1,\ldots,k}$, we write $I$ for the following multi-index:
\begin{align*}
I_{\sigma} &= \paren{i_{\sigma(1)},\ldots, i_{\sigma(k)}}.
\end{align*} Note that $I_{\sigma\tau}= (I_{\sigma})_{\tau}$ for $\sigma, \tau \in S_k$.
\end{definition}

\item \begin{definition}
Let $V$ be an $n$-dimensional vector space, and suppose $(\epsilon^1,\ldots, \epsilon^n)$ is any \emph{basis} for $V^{*}$. We now define a collection of \emph{$k$-covectors} on $V$ that generalize \underline{\emph{\textbf{the determinant function}}} on $\bR^n$. 

For each multi-index $I=(i_1,\ldots, i_k)$ of length $k$ such that $1\le i_1\le \ldots \le i_k \le n$, define \emph{\textbf{a covariant $k$-tensor}} $\epsilon^I = \epsilon^{i_1,\ldots, i_k}$ by
\begin{align}
\epsilon^I(v_1, \ldots, v_k) &= \det{\brac{\begin{array}{ccc}
\epsilon^{i_1}(v_1) & \ldots & \epsilon^{i_1}(v_k)\\
\vdots & \ddots & \vdots \\
\epsilon^{i_k}(v_1) & \ldots & \epsilon^{i_k}(v_k)
\end{array}}} = \det{\brac{ \begin{array}{ccc}
v_1^{i_1} & \ldots & v_k^{i_1}\\
\vdots & \ddots & \vdots \\
v_1^{i_k} & \ldots & v_k^{i_k}
\end{array}  }}.   \label{eqn: elementary_alt_tensor}
\end{align} 

In other words, if $\mb{V}$ denotes the $n \times k$ matrix whose columns are the components of the vectors $v_1,\ldots,v_k$ with respect to the basis $(E_i)$ dual to $(\epsilon^i)$, then $\epsilon^I(v_1, \ldots, v_k)$ is the \emph{\textbf{determinant of the $k\times k$ submatrix}} consisting of rows $i_1,\ldots, i_k$ of $\mb{V}$. Because the determinant changes sign whenever two columns are interchanged, it is clear that $\epsilon^I$ is \emph{an alternating $k$-tensor}. We call $\epsilon^I$ \underline{\emph{\textbf{an elementary alternating tensor}}} or \underline{\emph{\textbf{elementary $k$-covector}}}.
\end{definition}

\item \begin{definition}
If $I$ and $J$ are multiindices of length $k$, we define the Kronecker delta function:
\begin{align*}
\delta_{J}^{I} &= \det{\brac{ \begin{array}{ccc}
v_{j_1}^{i_1} & \ldots & v_{j_k}^{i_1}\\
\vdots & \ddots & \vdots \\
v_{j_1}^{i_k} & \ldots & v_{j_k}^{i_k}
\end{array}  }}
\end{align*} ($I$ represent the row number, $J$ represent the column number.)
\end{definition}

\item \begin{remark} The following is the property of Kronecker delta
\begin{equation*}
  \delta_{J}^{I} =
    \begin{cases}
      \sign{\sigma} &\text{if neither $I$ nor $J$ has a repeated index},\; J = I_{\sigma}, \; \sigma \in S_k\\
      0 & \text{if $I$ or $J$ has a repeated index or $J$ is not a permutation of $I$}
    \end{cases}       
\end{equation*}
\end{remark}

\item \begin{lemma} (\textbf{Properties of Elementary $k$-Covectors}). \\
Let $(E_i)$ be a basis for $V$, let $(\epsilon^i)$ be the dual basis for $V^{*}$, and let $\epsilon^I$ be as defined above.
\begin{enumerate}
\item If $I$ has a repeated index, then $\epsilon^I = 0$.
\item If $J = I_{\sigma}$ for some $\sigma \in S_k$, then $\epsilon^I = \sign{\sigma}\epsilon^{J}$.
\item The result of evaluating $\epsilon^I$ on a sequence of basis vectors is
\begin{align*}
\epsilon^I\paren{E_{j_1},\ldots, E_{j_{k}}} = \delta_{J}^{I}.
\end{align*}
\end{enumerate}
\end{lemma}

\item \begin{definition}
A multi-index $I=(i_1,\ldots, i_k)$ is said to be \emph{\textbf{increasing}} if $i_1 < \ldots < i_k$. It is useful
to use a primed summation sign to denote a sum over \emph{only increasing multi-indices}
\begin{align*}
\sum_{I}'a_{I}\epsilon^{I} &= \sum_{\set{I: i_1 < \ldots < i_k}}a_{I}\epsilon^{I}.
\end{align*}
\end{definition}

\item \begin{proposition}\label{prop: basis_alternating_tensor} (\textbf{A Basis for $\Lambda^k(V^{*})$})\\
Let $V$ be an $n$-dimensional vector space. If $(\epsilon^i)$ is any basis for $V^{*}$, then for each positive integer $k \le n$, the collection of $k$-covectors
\begin{align*}
\underline{\mathcal{E} = \set{\epsilon^I: \text{$I$ is an increasing multi-index of length $k$}}}
\end{align*} is \textbf{a basis for $\Lambda^k(V^{*})$}. Therefore,
\begin{align*}
\text{dim }\Lambda^k(V^{*}) = {n  \choose  k} = \frac{n!}{k! (n-k)!}
\end{align*} If $k > n$, then $\text{dim }\Lambda^k(V^{*}) = 0$.
\end{proposition}

\item \begin{remark}
In particular, for an $n$-dimensional vector space $V$, this proposition implies that $\Lambda^n(V^{*})$ is \emph{\textbf{$1$-dimensional}} and is spanned by $\epsilon^{1,\ldots,n}$.
\end{remark}

\item \begin{proposition} Suppose $V$ is an $n$-dimensional vector space and $\omega \in \Lambda^n(V^{*})$. If $T: V \rightarrow V$ is any \textbf{linear map} and $v_1, \ldots, v_n$ are arbitrary vectors in $V$, then
\begin{align}
\omega\paren{Tv_1, \ldots, Tv_n} &= \paren{\det{T}}\omega\paren{v_1, \ldots, v_n}. \label{eqn: k_covector_linear_map}
\end{align}
\end{proposition}
\end{itemize}

\subsection{Wedge Product}
\begin{itemize}
\item \begin{definition}
Let $V$ be a finite-dimensional real vector space. Given $\omega \in \Lambda^k(V^{*})$ and $\eta \in \Lambda^l(V^{*})$, we define their \underline{\emph{\textbf{wedge product}}} or \underline{\emph{\textbf{exterior product}}} to be the following \emph{$(k+ l)$-covector}:
\begin{align}
\omega \wedge \eta &= \frac{(k+l)!}{k! \,l!}\text{Alt}\paren{\omega \otimes \eta } =   \frac{1}{k! \,l!}\sum_{\sigma \in S_{k+l}} \paren{\sign{\sigma}}\paren{^{\sigma}\paren{\omega \otimes \eta }}  \label{eqn: wedge_product}
\end{align}
\end{definition}


\item The coefficients come from the following lemma:
\begin{lemma}
Let $V$ be an $n$-dimensional vector space and let $(\epsilon^1,\ldots, \epsilon^n)$ be a basis for $V^{*}$. For any multi-indices $I=(i_1,\ldots, i_k)$ and $J=(j_1,\ldots, j_l)$,
\begin{align}
\epsilon^{I} \wedge \epsilon^{J} &= \epsilon^{IJ} \label{eqn: wedge_product_of_basis}
\end{align}
where $IJ= (i_1,\ldots, i_k, j_1,\ldots, j_l)$ is obtained by \textbf{concatenating} $I$ and $J$.
\end{lemma}

\item \begin{proposition}\label{prop: wedge_product_property} (\textbf{Properties of the Wedge Product}). \\
Suppose $\omega, \omega', \eta, \eta'$ and $\xi$ are \textbf{multicovectors} on a finite-dimensional vector space $V$.
\begin{enumerate}
\item (\textbf{Bilinearity}): For $a,a; \in \bR$,
\begin{align*}
(a \omega + a' \omega')\wedge \eta &=  a (\omega \wedge \eta)+a' (\omega' \wedge \eta),\\
\eta \wedge  (a \omega + a' \omega')&=  a ( \eta \wedge \omega)+a' ( \eta  \wedge \omega').
\end{align*}
\item (\textbf{Associativity}):
\begin{align*}
\omega \wedge (\eta  \wedge \xi) &= (\omega \wedge \eta ) \wedge \xi 
\end{align*}
\item (\textbf{Anticommutativity}): For $\omega \in \Lambda^k(V^{*})$ and $\eta \in \Lambda^l(V^{*})$,
\begin{align}
\omega \wedge \eta &= \paren{-1}^{kl} \eta \wedge \omega \label{eqn: wedge_product_anticommutative}
\end{align}
\item  If $(\epsilon^i)$ is any basis for $V^{*}$ and $I=(i_1,\ldots, i_k)$ is any multi-index, then
\begin{align}
\epsilon^{i_1} \wedge \ldots \wedge \epsilon^{i_k} = \epsilon^{I} \label{eqn: wedge_product_basis}
\end{align}
\item For any covectors $\omega^1,\ldots, \omega^k$ and vectors $v_1,\ldots,v_k$,
\begin{align}
(\omega^1 \wedge \ldots \wedge \omega^k)(v_1,\ldots,v_k) &= \det\paren{\omega^{j}(v_i)} \label{eqn: wedge_product_determinant}
\end{align}
\end{enumerate}
\end{proposition}

\item \begin{remark}
Because of part (4) of this lemma, henceforth we generally use the notations $\epsilon^I$ and $\epsilon^{i_1}  \wedge \ldots \wedge \epsilon^{i_k}$ \emph{\textbf{interchangeably}}
\end{remark}

\item \begin{definition}
A $k$-covector $\eta$ is said to be \emph{\textbf{decomposable}} if it can be expressed in the form $\eta = \omega^1 \wedge \ldots \wedge \omega^k$, where $\omega^1,\ldots, \omega^k$ are \emph{covectors}. 
\end{definition}

\item \begin{remark}
It is important to be aware that not every $k$-covector is decomposable when $k > 1$; however, it follows from Proposition \ref{prop: basis_alternating_tensor} and above Proposition \ref{prop: wedge_product_property} (4) that \emph{\textbf{every $k$-covector} can be written as a \textbf{linear combination} of \textbf{decomposable ones}}.
\end{remark}

\item \begin{definition}
For any $n$-dimensional vector space $V$, define \textbf{a vector space} $\Lambda(V^{*})$ by
\begin{align*}
\Lambda(V^{*}) &= \bigoplus_{k=0}^{n} \Lambda^k(V^{*}).
\end{align*}
It follows from Proposition \ref{prop: basis_alternating_tensor} that $\text{dim }\Lambda(V^{*}) = 2^n$. The wedge product turns $\Lambda(V^{*})$ into an \emph{\textbf{associative algebra}}, called \underline{\emph{\textbf{the exterior algebra}}} (or \emph{\textbf{Grassmann algebra}}) of $V$. 
\end{definition}

\item \begin{remark}
 For any covectors $\omega^1,\ldots, \omega^k$ and vectors $v_1,\ldots,v_k$, \emph{\textbf{the exterior product}} is considered as the \emph{\textbf{determinant function} of a $k \times k$ submatrix }
\begin{align*}
(\omega^1 \wedge \ldots \wedge \omega^k)(v_1,\ldots,v_k) &= \det\brac{ \begin{array}{ccc}
\omega^{1}(v_1) & \ldots & \omega^{1}(v_k)\\
\vdots & \ddots & \vdots\\
\omega^{k}(v_1) & \ldots & \omega^{k}(v_k)
\end{array}} 
\end{align*} where \emph{\textbf{vectors} $v_1,\ldots,v_k$ forms \textbf{column vector}}, and \emph{\textbf{covectors} $\omega^1,\ldots, \omega^k$ form the \textbf{row vector}}.  

In other words, we can think of \emph{\textbf{exterior product of covectors}} as \emph{\textbf{an \underline{abstraction} of \underline{determinant operation}}}.
\end{remark}
\end{itemize}
\subsection{Interior Product}
\begin{itemize}
\item \begin{definition}
Let $V$ be a finite-dimensional vector space. For each $v \in V$, we define a \emph{linear map} $\iota_v: \Lambda^k(V^{*}) \rightarrow \Lambda^{k-1}(V^{*})$, called \underline{\emph{\textbf{interior multiplication (interior product)}}} by $v$, as follows:
\begin{align*}
(\iota_v\omega)(w_1,\ldots, w_{k-1}) &= \omega\paren{v, w_1,\ldots, w_{k-1}}.
\end{align*} In other words, $(\iota_v\omega)$ is obtained from $\omega$ by \emph{\textbf{inserting $v$ into the first slot}}.  By convention, we interpret $(\iota_v\omega)$ to be \emph{\textbf{zero}} when $\omega$ is a \emph{\textbf{$0$-covector}} (i.e., a \emph{\textbf{number}}). Another common notation is
\begin{align*}
v \mathbin{\lrcorner } \omega &= (\iota_v\omega).
\end{align*}
This is often read ``\underline{\emph{\textbf{$v$ into $\omega$}}}."
\end{definition}

\item \begin{proposition}
Let $V$ be a finite-dimensional vector space and $v \in V$.
\begin{enumerate}
\item $ \iota_v \circ \iota_v = 0$.
\item If $\omega \in \Lambda^k(V^{*})$ and $\eta \in \Lambda^l(V^{*})$,
\begin{align}
\iota_v(\omega \wedge \eta) &= \iota_{v}(\omega) \wedge \eta + (-1)^{k}\omega \wedge \iota_v(\eta)  \label{eqn: interior_product_property}
\end{align}
\end{enumerate}
\end{proposition}

\item \begin{remark} It is easy to verify the following form
\begin{align}
\iota_v\paren{\omega^1 \wedge \ldots \wedge \omega^{k}} = v\iprod{\paren{\omega^1 \wedge \ldots \wedge \omega^{k}}}= \sum_{i=1}^{k}(-1)^{i-1}\omega^{i}(v)\,\paren{\omega^1 \wedge \ldots \wedge \widehat{\omega}^{i} \wedge \ldots \wedge \omega^{k}} \label{eqn: interior_product_expansion}\\
\Leftrightarrow \paren{\omega^1 \wedge \ldots \wedge \omega^{k}}(v, v_2, \ldots, v_k)= \sum_{i=1}^{k}(-1)^{i-1}\omega^{i}(v)\,\paren{\omega^1 \wedge \ldots \wedge \widehat{\omega}^{i} \wedge \ldots \wedge \omega^{k}}(v_2, \ldots, v_k) \nonumber
\end{align} where the hat indicates that $\omega^i$ is \emph{\textbf{omitted}}. In \emph{determiant form}, it can be written as
\begin{align}
\det{\mb{V}} &=  \sum_{i=1}^{k}(-1)^{i-1}\omega^{i}(v)\,\det\mb{V}_{1}^{i}  \label{eqn: interior_product_determinant_expansion}
\end{align} where $\mb{V}_{j}^{i}$ denote the $(k -1) \times (k -1)$ submatrix of $\mb{V}$ obtained by \emph{\textbf{deleting}} the $i$-th row and $j$-th column.  This is just \emph{\textbf{the expansion of $\det \mb{V}$ by minors}} along the first column, and therefore is equal to $\det \mb{v}$. 
\end{remark}

\item \begin{remark} 
The \emph{exterior product} \emph{\textbf{increase}} the \emph{rank} of tensor, while the \emph{interior product} \emph{\textbf{decrease}} the \emph{rank} of tensor by $1$.
\end{remark}

\end{itemize}

\subsection{Differential Forms on Manifolds}
\begin{itemize}
\item \begin{definition} 
Let $T^kT^{*}M$ be the \emph{bundle} of all covariant $k$-tensors on $M$. The subset of $T^kT^{*}M$ consisting of \emph{\textbf{alternating tensors}} is denoted by $\Lambda^k(T^{*}M)$:
\begin{align*}
\Lambda^k(T^{*}M) &= \bigsqcup_{p\in M}\Lambda^k(T_{p}^{*}M).
\end{align*}  $\Lambda^k(T^{*}M)$ is a \emph{smooth subbundle} of $T^{k}T^{*}M$, so it is a \emph{smooth vector bundle of rank $n \choose k$}.
\end{definition}

\item \begin{remark}
$\Lambda^k(T^{*}M)$ is \emph{\textbf{the bundle of all alternating covariant $k$-tensors (exterior forms, $k$-covectors)}} on $M$.  
\end{remark}

\item \begin{definition}
A \emph{\textbf{section}} of $\Lambda^k(T^{*}M)$ is called \underline{\emph{\textbf{a differential $k$-form}}}, or just a \underline{\emph{\textbf{$k$-form}}}; this is
a \emph{(continuous) tensor field} whose value at each point is an \emph{alternating tensor}. The integer $k$ is called \emph{the \textbf{degree} of the form}. We denote the \emph{vector space} of \emph{\textbf{smooth $k$-forms}} by
\begin{align*}
\Omega^{k}(M) &= \Gamma\paren{\Lambda^k(T^{*}M)}.
\end{align*}
\end{definition}

\item \begin{remark}
\textit{A $k$-form is just an alternating covariant $k$-tensor fields}. 
\end{remark}

\item \begin{remark}
The \emph{\textbf{wedge product}} of \emph{two differential forms} is defined \emph{\textbf{pointwise}}: $(\omega \wedge \eta)_p = \omega_p \wedge \eta_p$. Thus, \underline{\emph{\textbf{the wedge product of a $k$-form with an $l$-form is a $(k +l)$-form}}}. If $f$ is a $0$-form (i.e. a smooth function) and $\omega$ is a $k$-form, we interpret the wedge product  $f\wedge \omega$ to mean the ordinary product $f\omega$.
\end{remark}

\item \begin{remark}
The direct sum of all vector spaces of smooth $k$-forms for $k\le n$ is 
\begin{align}
\Omega^{*}(M) &= \bigoplus_{k=0}^{n}\Omega^{k}(M). \label{eqn: grassman_algebra_k_form}
\end{align} Then the wedge product turns $\Omega^{*}(M) $ into \emph{an \textbf{associative}, \textbf{anticommutative} \textbf{graded algebra}}.
\end{remark}

\item \begin{remark}(\emph{\textbf{Duality of Basis}})\\
The basis of differential $k$-forms $(dx^{i_1} \wedge \ldots \wedge dx^{i_k})$ in $\Gamma\paren{\Lambda^{k}(T^{*}M)}$ acts on the local coordinate frames $(\partial / \partial x^i)$ in $TM$
\begin{align*}
\paren{dx^{i_1} \wedge \ldots \wedge dx^{i_k}}\paren{\partdiff{}{x^{j_1}}, \ldots, \partdiff{}{x^{j_k}}} = \delta_{J}^{I}
\end{align*}  
\end{remark}

\item \begin{remark}(\emph{\textbf{Coordinate Representation of $k$-Forms}})\\
In any smooth chart, a $k$-form $\omega$ can be written locally as
\begin{align*}
\omega &= \sum_{I}'\omega_{I}dx^{I} := \sum_{I}'\omega_{I}\,dx^{i_1} \wedge \ldots \wedge dx^{i_k}
\end{align*} where the coefficients $\omega^{I}$ are \emph{\textbf{continuous functions}} defined on the coordinate domain, and we use $dx^I$ as an abbreviation for $dx^{i_1} \wedge \ldots \wedge dx^{i_k}$ (not to be mistaken for the differential of a real-valued function $x^I$). Also $\sum_{I}'\epsilon^{I}$ means that sum with increasing multi-indices.  \emph{\textbf{The component function}} $\omega_I$ is computed as
\begin{align*}
\omega_{I} &= \omega\paren{\partdiff{}{x^{j_1}}, \ldots, \partdiff{}{x^{j_k}}}.
\end{align*} Note that $\omega_I$ is \underline{\emph{\textbf{the determinant of a $k \times k$ principal sub-matrix}}} \emph{\textbf{(i.e. principal minors)}} whose \emph{rows and columns} are indexed by increasing multi-index $I$.
\end{remark}

\item \begin{example} The followings are some basic differential $k$-forms:
\begin{enumerate}
\item Any smooth function $f \in \cC^{\infty}(M)$ is a \emph{\textbf{$0$-form}};
\item \emph{\textbf{A differential $1$-form}} is \emph{\textbf{the covariant vector field}} $df$
\begin{align*}
df &= \sum_{i}\partdiff{f}{x^{i}}dx^{i}
\end{align*}
\item \emph{\textbf{A differential $2$-form}}  is written as
\begin{align*}
\omega &= \sum_{i < j}\omega_{i,j}\; dx^{i} \wedge dx^j
\end{align*}
\end{enumerate}
\end{example}

\item \begin{definition}
If $F: M \rightarrow N$ is a smooth map and $\omega$ is \emph{\textbf{a differential form}} on $N$, \emph{the \textbf{pullback} $F^{*}$ is a \textbf{differential form on $M$}}; defined as for any covariant tensor field:
\begin{align*}
(F^{*}\omega)_p(v_1, \ldots, v_k) &= \omega_p\paren{dF_p(v_1), \ldots, dF_p(v_k)}.
\end{align*}
\end{definition}

\item \begin{lemma}
Suppose $F: M \rightarrow N$ is smooth.
\begin{enumerate}
\item $F^{*}: \Omega^k(N) \rightarrow \Omega^k(M)$ is \emph{\textbf{linear}} over $\bR$.
\item $F^{*}(\omega \wedge \eta) = (F^{*}\omega) \wedge (F^{*}\eta)$.
\item  In any smooth chart,
\begin{align}
F^{*}\paren{\sum_{I}'\omega_{I}\,dy^{i_1} \wedge \ldots \wedge dy^{i_k}} &= \sum_{I}'\paren{\omega_I \circ F}\,d(y^{i_1} \circ F) \wedge \ldots \wedge d(y^{i_k} \circ F) \label{eqn: differential_form_pull_back_coordinate_0}
\end{align}
\end{enumerate}
\end{lemma}

\item \begin{proposition}(\textbf{Pullback Formula for Top-Degree Forms}). \\
Let $F: M \rightarrow N$ be a smooth map between $n$-manifolds with or without boundary. If $(x^i)$ and $(y^j)$ are smooth coordinates on open subsets $U \subseteq M$ and $V \subseteq N$, respectively, and $u$ is a continuous real-valued function on $V$, then the following holds on $U \cap F^{-1}(V)$:
\begin{align}
F^{*}\paren{u\,dy^{1} \wedge \ldots \wedge dy^{n}} &= (u \circ F)\,(\det(DF))\,dx^{1} \wedge \ldots dx^{n}  \label{eqn: differential_form_pull_back_coordinate_1}
\end{align}
where $DF$ represents \textbf{the Jacobian matrix of $F$} in these coordinates.
\end{proposition} Note that $d(y^i \circ F) = dF^{i} = \det(DF)_{j}^{i} dx^{j}$

\item \begin{corollary} (\textbf{Change of Coordinates for Differential Forms})\\
If $(U, (x^i))$ and $(\widetilde{U}, (\widetilde{x}^j))$ are overlapping smooth coordinate charts on $M$, then the following identity holds on $U \cap \widetilde{U}$:
\begin{align}
d\widetilde{x}^{1} \wedge \ldots \wedge d\widetilde{x}^{n} &=\det\paren{\dfrac{\partial \widetilde{x}^{j}}{\partial x^i}} dx^1 \wedge \ldots \wedge dx^n.  \label{eqn: differential_form_change_of_variables}
\end{align}
\end{corollary}

\item \begin{remark}
The equation \eqref{eqn: differential_form_pull_back_coordinate_1} provides a computational formula for pullback of differential forms under coordinate systems for domain and codomain. And the equation \eqref{eqn: differential_form_change_of_variables} provides the fomula for change of variables of differential forms.
\end{remark}

\item \begin{definition}
\emph{\textbf{Interior multiplication}} also extends naturally to \emph{\textbf{vector fields}} and \emph{\textbf{differential forms}}, simply by letting it act \emph{pointwise}: if $X\in \frX(M)$ and $\omega \in \Omega^k(M)$, define a $(k-1)$-form $X \iprod \omega = \iota_{X}\omega$ by
\begin{align*}
(X \iprod \omega)_p &= X_p \iprod \omega_p.
\end{align*}
\end{definition}

\item \begin{proposition}
Let $X$ be a smooth vector field on $M$.
\begin{enumerate}
\item If $\omega$ is a smooth differential form, then $\iota_X\omega$ is smooth.
\item $\iota_X: \Omega^k(M)  \rightarrow \Omega^{k-1}(M)$ is \emph{\textbf{linear}} over $\cC^{\infty}(M)$ and therefore corresponds to a \textbf{smooth bundle homomorphism} $\iota_X: \Lambda^k(T^{*}M) \rightarrow \Lambda^{k-1}(T^{*}M)$.
\end{enumerate}
\end{proposition}
\end{itemize}

\subsection{Exterior Derivatives of Differential Forms}
\begin{itemize}
\item \begin{remark}
For each smooth manifold $M$ with or without boundary, we will show that there is \emph{\textbf{a differential operator}} $d: \Omega^k(M) \rightarrow \Omega^{k+1}(M)$ satisfying $d(d\omega) = 0$ for all $\omega$.  Thus, it will follow that \emph{a necessary condition} for a smooth
$k$-form $\omega$ to be equal to $d\eta$ for some $(k-1)$-form $\eta$ is that $d\omega = 0$.
\end{remark}

\item \begin{definition}
If $\omega = \sum_{J}'\omega_J dx^J$ is a smooth $k$-form on an open subset $U\subseteq \bR^n$ or $\bH^n$, we define its \underline{\emph{\textbf{exterior derivative}}} $d\omega$ to be the following \underline{$(k+1)$-form}:
\begin{align}
d\omega := d\paren{\sum_{J}'\omega_J dx^J} =\sum_{J}' d\omega_J \wedge dx^{J},  \label{eqn: exterior_derivatives_def_0}
\end{align}
where $d\omega_J$ is the differential of the function $\omega_J$. In somewhat more detail, this is
\begin{align}
d\omega := d\paren{\sum_{J}'\omega_J dx^J} = \sum_{J}'\sum_{i}\partdiff{\omega_{J}}{x^i} dx^i \wedge dx^{j_1} \wedge \ldots \wedge dx^{j_k}. \label{eqn: exterior_derivatives_def_1}
\end{align}
\end{definition}

\item \begin{remark}
The \emph{exterior derivatives} of a $k$-form is \emph{\textbf{a linear combination}} of \emph{$(k+1)$-forms}. It component function is  \emph{\textbf{the principal minior} of \textbf{Jacobian matrix of component functions} $(\partdiff{\omega_j}{x^i})$}.
\end{remark}

\item \begin{remark}
When $\omega$ is a $1$-form, this becomes
\begin{align*}
d\omega = d\paren{\sum_{j}\omega_j dx^j} &= \sum_j d\omega_j \wedge dx^{j}\\
&= \sum_j \sum_{i}\partdiff{\omega_j}{x^i} dx^i \wedge dx^{j} \\
&= \sum_{i < j} \partdiff{\omega_j}{x^i} dx^i \wedge dx^{j} + \sum_{i > j}\partdiff{\omega_j}{x^i} dx^i \wedge dx^{j}\\
&= \sum_{i < j}\paren{\partdiff{\omega_j}{x^i}  - \partdiff{\omega_i}{x^j}}dx^i \wedge dx^{j}.
\end{align*} Note that the component is the determinant of a $2 \times 2$ sub-matrix of Jacobian $(\partdiff{\omega_j}{x^i})$.
\end{remark}

\item \begin{remark}
The \emph{\textbf{exterior differentiation}} defines the differential of $k$-form. It is an \textbf{\emph{extension}} of \underline{\emph{\textbf{differentiation}} to \emph{\textbf{determinant function}}}.
\end{remark}


\item \begin{definition}
If $A =  \oplus_{k} A^k$ is a \emph{graded algebra}, a \emph{linear map} $T: A \rightarrow A$ is said to be a map \emph{\textbf{of degree $m$}} if $T(A^k) \subseteq A^{k+m}$ for each $k$. It is said to be an \emph{\textbf{antiderivation}} if it satisfies $T(xy) = (Tx)y + (-1)^k\, x(Ty)$ whenever $x\in A^k$ and $y \in A^l$. 
\end{definition}

\item \begin{remark} (\emph{\textbf{The Exterior Differentiation vs. The Interior Multiplication}})
\begin{enumerate}
\item The \emph{\textbf{exterior differentiation}} $d: \Omega^k(M) \rightarrow \Omega^{k+1}(M)$ is an  \emph{\textbf{antiderivation}} of \emph{degree} $+1$ whose \emph{\textbf{square is zero}}.
\item On the other hand, the \emph{\textbf{interior multiplication}} $\iota_{X}: \Omega^k(M) \rightarrow \Omega^{k-1}(M)$ is an \emph{\textbf{antiderivation}} of \emph{degree} $-1$ whose \emph{\textbf{square is zero}}, where $X \in \frX(M).$
\end{enumerate}
\end{remark}

\item Another important feature of the exterior derivative is that \emph{it \textbf{commutes} with all pullbacks}.
\begin{proposition} (\textbf{Naturality of the Exterior Derivative}). \\
If $F: M \rightarrow N$ is a smooth map, then for each $k$ \textbf{the pullback map} $F^{*}: \Omega^k(N) \rightarrow \Omega^k(M)$ \textbf{commutes}
with $d$: for all $\omega \in \Omega^{k}(N)$,
\begin{align}
F^{*}(d\omega) &= d\paren{F^{*}\omega}.  \label{eqn: exterior_derivatives_pullback_commute_2}
\end{align}
\end{proposition}
\end{itemize}

\subsection{An Invariant Formula for the Exterior Derivative}
\begin{itemize}
\item \begin{proposition} (\textbf{Exterior Derivative of a $1$-Form}). \\
For any smooth $1$-form $\omega$ and smooth vector fields $X$ and $Y$,
\begin{align}
d\omega(X,Y) &= X(\omega(Y)) - Y(\omega(X)) - \omega([X, Y]).\label{eqn: exterior_derivatives_1_form_invariant}
\end{align}
\end{proposition}
\begin{proof}
Since any smooth $1$-form can be expressed locally as a sum of terms of the form $u\,dv$ for smooth functions $u$ and $v$, it suffices to consider that case.  Suppose
$\omega = u\,dv$, and $X, Y$ are smooth vector fields. The LHS of \eqref{eqn: exterior_derivatives_1_form_invariant}
\begin{align*}
d\paren{u\, dv}(X, Y) &= (du \wedge dv)(X, Y) = du(X)dv(Y) - du(Y)dv(X) \\
&= X(u) Y(v) - X(v) Y(u)
\end{align*}
The RHS is
\begin{align*}
&=X(u\, dv(Y)) - Y(u\, dv(X)) -u\, dv([X, Y]) \\
&= X(u\, Y(v)) - Y(u\,X(v)) - u\,[X,Y](v)\\
&= X(u) Y(v) + u\,XY(v) - Y(u)X(v) - u\,YX(v) - u\,\paren{(XY- YX)v}\\
&= X(u) Y(v) - Y(u)X(v) + u\,\paren{XY(v) - YX(v)} - u\,\paren{XY(v) - YX(v)}\\
&= X(u) Y(v) - Y(u)X(v).
\end{align*} Thus \eqref{eqn: exterior_derivatives_1_form_invariant} holds. \qed.
\end{proof}

\item \begin{proposition}
Let $M$ be a smooth $n$-manifold with or without boundary, let $(E_i)$ be a smooth local frame for $M$, and let $(\epsilon^i)$ be the dual coframe. For each $i$,
let $b^{i}_{j,k}$ denote the \textbf{component functions} of \textbf{the exterior derivative} of $\epsilon^i$ in this frame, and for each $j,k$, let $c^{i}_{j,k}$ be the \textbf{component functions} of the \textbf{Lie bracket} $[E_j, E_k]$:
\begin{align*}
d\epsilon^i =\sum_{j < k} b^{i}_{j,k}\epsilon^{j} \wedge \epsilon^{k}; \quad [E_j, E_k] = c^{i}_{j,k} E_i
\end{align*} Then $b^{i}_{j,k}= - c^{i}_{j,k}$. 
\end{proposition}

\item \begin{proposition} (\textbf{Invariant Formula for the Exterior Derivative}).\\
Let $M$ be a smooth manifold with or without boundary, and $\omega \in \Omega^k(M)$. For any smooth vector fields $X_1,\ldots, X_{k+1}$ on $M$,
\begin{align}
d\omega\paren{X_1,\ldots, X_{k+1}} &= \sum_{1\le i \le k+1}(-1)^{i}X_i\paren{\omega\paren{X_1,\ldots,  \widehat{X}_i  \ldots, X_{k+1}}}  \nonumber\\
&\, +   \sum_{1\le i < j \le k+1}(-1)^{i+j}\omega\paren{[X_i, X_j], X_1,\ldots,  \widehat{X}_i, \ldots,  \widehat{X}_j, \ldots, X_{k+1}},
 \label{eqn: exterior_derivatives_k_form_invariant}
\end{align}
where the hats indicate \textbf{omitted} arguments.
\end{proposition}

\item \begin{remark}
The proof of formula \eqref{eqn: exterior_derivatives_k_form_invariant} and \eqref{eqn: exterior_derivatives_1_form_invariant} is only based on the definition of $k$-form and vector fields, and it does not involve any specific coordinate system. Thus it can be used to give an \underline{\emph{\textbf{invariant definition}}} of \emph{\textbf{the exterior differentiation}} $d$.
\end{remark}
\end{itemize}


\section{Directional Derivatives of Vector Fields}
\subsection{Connections}
\begin{itemize}
\item \begin{remark}
There are \emph{two alternatives} for the definition of \emph{\textbf{geodesics}}:
\begin{itemize}
\item Geodesics is the \emph{``\textbf{shortest}" path} that connects two points on the surface; This definition is hard since the definition of manifold is abstract. 
\item Geodesics is the curve on the surface that has \emph{\textbf{zero tangential acceleration}}. This is the motivation to introduce the concept of \emph{connections}.
\end{itemize} 
\end{remark}

\item \begin{remark}
Although the \emph{\textbf{velocity}} of a curve $\gamma$ in a manifold $M$ is well defined, the \emph{\textbf{acceleration}} of the curve on $M$ is \textbf{not} since it requires comparison between tangent vectors in two different tangent spaces $T_{\gamma(t)}M$ and $T_{\gamma(t+\Delta)}M$.
\end{remark}

\item \begin{remark}
To do so, we need a way to \emph{\textbf{compare values of the vector field at different points}}, or intuitively, to ``\emph{\textbf{connect}}" \emph{\textbf{nearby tangent spaces}}. This is where a connection comes in: it will be an additional piece of data on a manifold, \emph{a \textbf{rule}} for \emph{computing directional derivatives of vector fields}.
\end{remark}

\item \begin{remark}
\emph{\textbf{A connection}} is a \underline{\emph{\textbf{coordinate-independent}}} set of rules for taking \emph{\textbf{directional derivatives of vector fields}}.
\end{remark}

\item \begin{definition}
Let $\pi: E \rightarrow M$ be a \emph{smooth vector bundle} over \emph{a smooth manifold} $M$ with or without boundary, and let $\Gamma(E)$ denote the space of \emph{smooth sections} of $E$. A \underline{\emph{\textbf{connection}}} in $E$ is a map 
\begin{align*}
\nabla: \frX(M) \times \Gamma(E) \rightarrow \Gamma(E),
\end{align*} written $(X, Y) \mapsto \conn{X}{Y}$, satisfying the following properties:
\begin{enumerate}
\item $\conn{X}{Y}$ is \emph{\textbf{linear} over $\cC^{\infty}(M)$ \textbf{in $X$}}: for $f_1, f_2 \in \cC^{\infty}(M)$ and $X_1, X_2  \in \frX(M)$,
\begin{align*}
\conn{\paren{f_1\,X_1 + f_2\,X_2}}{Y} &= f_1\,\conn{X_1}{Y} + f_2\,\conn{X_2}{Y}
\end{align*}
\item $\conn{X}{Y}$ is \emph{\textbf{linear} over $\bR$ \textbf{in}} $Y$: for $a_1, a_2 \in \bR$ and $Y_1, Y_2  \in \Gamma(E)$,
\begin{align*}
\conn{X}{\paren{a_1\,Y_1 + a_2\,Y_2}} &= a_1\,\conn{X}{Y_1} + a_2\,\conn{X}{Y_2}
\end{align*}
\item $\nabla$ satisfies the following \emph{\textbf{product rule}}: for $f \in \cC^{\infty}(M)$,
\begin{align*}
\conn{X}{(fY)} &= f\,\conn{X}{Y} + (Xf)\,Y
\end{align*}
\end{enumerate} 
The symbol $\nabla$ is read ``\emph{\textbf{del}}" or ``\emph{\textbf{nabla}}," and $\conn{X}{Y}$ is called \emph{\textbf{\underline{the covariant derivative} of $Y$ \underline{in the direction $X$}}}.
\end{definition}

\item \begin{remark}
There is \emph{a variety of types of connections} that are useful in different circumstances. The type of connection we have defined here is sometimes called \emph{\textbf{a Koszul connection}} to distinguish it from other types. 
\end{remark}

\item \begin{lemma} \label{lem: locality} (\textbf{Locality}). \citep{lee2018introduction}\\
Suppose $\nabla$ is a connection in a smooth vector bundle $E \rightarrow M$. For every $X \in \frX(M)$, $Y\in \Gamma(E)$, and $p \in M$, the covariant derivative $\conn{X}{Y}|_{p}$ depends \textbf{only} on the values of $X$ and $Y$ in an arbitrarily \textbf{small neighborhood} of $p$. More precisely, if $X = \widetilde{X}$ and $Y = \widetilde{Y}$ on a neighborhood of $p$, then $\conn{X}{Y}|_{p} = \conn{\widetilde{X}}{\widetilde{Y}}|_{p}$.
\end{lemma}

\item \begin{proposition} (\textbf{Restriction of a Connection}).\citep{lee2018introduction} \\
Suppose $\nabla$ is a connection in a smooth vector bundle  $E \rightarrow M$. For every open subset $U \subseteq M$, there is a \textbf{unique} \textbf{connection} $\nabla^{U}$ on the \textbf{restricted bundle} $E|_{U}$ that satisfies the following relation for every $X \in \frX(M)$ and $Y\in \Gamma(E)$:
\begin{align}
\nabla^U_{(X|_{U})}(Y|_{U}) &= (\conn{X}{Y})|_{U}.  \label{eqn: connection_restriction}
\end{align}
\end{proposition}

\item \begin{proposition}
Under the hypotheses of Lemma \ref{lem: locality}, $\conn{X}{Y}|_{p}$ depends \textbf{only} on the \textbf{values} of $Y$ in a \textbf{neighborhood} of $p$ and the \textbf{value} of $X$ \textbf{at} $p$.
\end{proposition}

\item \begin{remark}
In the situation of these two propositions, we typically just refer to the \emph{restricted connection} as $\nabla$ instead of $\nabla^U$; the proposition guarantees that there is no ambiguity in doing so. Thus if $X$ is a vector field defined in a neighorhood of $p$, 
\begin{align*}
\conn{v}{Y} &= \conn{X}{Y}|_{p}, \quad \text{ for }v = X_p.
\end{align*}
\end{remark}
\end{itemize}
\subsection{Connections in the Tangent Bundle}
\begin{itemize}
\item We focus on the connection in tangent bundle.
\begin{definition}
Suppose $M$ is a smooth manifold with or without boundary. By the definition we just gave, \emph{a connection in $TM$} is a map
\begin{align*}
\nabla: \frX(M) \times \frX(M) \rightarrow \frX(M),
\end{align*}
satisfying properties (1)-(3) above. \emph{A connection in the tangent bundle $TM$} is often called simply \underline{\emph{\textbf{a connection on M}}}. (The terms \underline{\emph{\textbf{affine connection}}} and \emph{\textbf{linear connection}} are also sometimes used in this context.)
\end{definition}

\item \begin{definition}
For computations, we need to examine how a connection appears in terms of \emph{a local frame}. Let $(E_i)$ be a \emph{smooth local frame} for $TM$ on an open subset $U\subseteq M$. For every choice of the indices $i$ and $j$, we can expand the vector field $\conn{E_i}{E_j}$ in terms of this same frame:
\begin{align}
\conn{E_i}{E_j} &= \Gamma_{i,j}^{k}\,E_k. \label{eqn: christoffel_symbol_basis}
\end{align} As $i$, $j$, and $k$ range from $1$ to $n = \text{dim }M$, this defines $n^3$ smooth functions $\Gamma_{i,j}^{k}: U \rightarrow \bR$, called \emph{\underline{\textbf{the connection coefficients}} of $\nabla$ \textbf{with respect to the given frame}}. 
\end{definition}

\item The following proposition shows that the connection is completely determined in $U$ by its connection coefficients. 
\begin{proposition} (\textbf{Coordinate Representation of Connection}) \citep{lee2018introduction}\\
Let $M$ be a smooth manifold with or without boundary, and let $\nabla$ be a connection in $TM$. Suppose $(E_i)$ is a smooth local frame over an open subset $U\subseteq M$, and let $\set{\Gamma_{i,j}^{k}}$ be the connection coefficients of $\nabla$ with respect to this frame. For smooth vector fields $X, Y  \in \frX(M)$, written in terms of the frame as $X = X^i\,E_i$, $Y = Y^j\,E_j$, one has
\begin{align}
\conn{X}{Y} &= \paren{X(Y^k) + X^iY^j\,\Gamma_{i,j}^{k}}\,E_k. \label{eqn: connection_represent_christoffel_symbol}
\end{align}
\end{proposition}

\item \begin{remark}
The $n^3$ functions $\{\Gamma_{i,j}^{k}\}$ are called \underline{\emph{\textbf{the Christoffel symbols}}} under \emph{the metric connections}. \citep{do1976differential}
\end{remark}

\item \begin{remark}
The smooth function $\Gamma_{i,j}^{k} \in \cC^{\infty}(M)$ has three indices: \emph{two lower indices} $(i,j)$ cooresponds to the index of \emph{\textbf{component $X^i$ for the directional vector field}} $X$, and the index of \emph{\textbf{component $Y^j$ for the differentiated vector field}} $Y$ in $\conn{X}{Y}$;  \emph{the one upper index} $k$ correponds to the index of the \emph{\textbf{basis}} vector field $\partial/ \partial x^k$ which spans the space of vector fields. 
\end{remark}

\item \begin{remark}
The \emph{first term} of \eqref{eqn: connection_represent_christoffel_symbol} accounts for \emph{the \textbf{change of position} \textbf{relative} to the local frame} when moving $Y$ from one tangent space to another along the direction of $X$. The second term accounts for the \textbf{\emph{additional}} \textbf{``\emph{rotation}" of \emph{frames}}. For Euclidean space, the basis is fixed when moving along the tangent direction (i.e. no \emph{rotation} just \emph{translation}).
\end{remark}

\item \begin{proposition} (\textbf{Transformation Law for Connection Coefficients}).  \citep{lee2018introduction} \\
Let $M$ be a smooth manifold with or without boundary, and let $\nabla$ be a connection in $TM$. Suppose we are given two smooth local frames $(E_i)$ and $(\widetilde{E}_j)$ for $TM$ on an open subset $U \subseteq M$, related by $\widetilde{E}_i = A_i^j E_j$ for some matrix of functions $(A_i^j)$. Let $\Gamma_{i,j}^{k}$ and $\widetilde{\Gamma}_{i,j}^{k}$ denote the connection coefficients of $\nabla$ with respect to these two frames.
Then
\begin{align}
\widetilde{\Gamma}_{i,j}^{k} &= (A^{-1})_{t}^{k}A_{i}^{r}A_{j}^{s}\Gamma_{r,s}^{t}+ (A^{-1})_{t}^{k}A_{i}^{s}E_{s}(A_{j}^{t})  \label{eqn: chistoffel_symbol_change_of_basis}
\end{align}
\end{proposition}

\item \begin{lemma}
Suppose $M$ is a smooth $n$-manifold with or without boundary, and $M$ admits \textbf{a global frame} $(E_i)$. Formula \eqref{eqn: connection_represent_christoffel_symbol} gives a \textbf{one-to-one correspondence} between connections in $TM$ and choices of $n^3$ smooth real-valued functions $\{\Gamma_{i,j}^{k}\}$ on $M$.
\end{lemma}

\item \begin{proposition}
The tangent bundle of every smooth manifold with or without boundary admits a connection.
\end{proposition}

\item 
\begin{proposition} (\textbf{The Difference Tensor}). \\
Let $M$ be a smooth manifold with or without boundary. For any two connections $\nabla^0$ and $\nabla^1$ in $TM$, define a map $D: \frX(M) \times \frX(M) \rightarrow \frX(M)$ by
\begin{align*}
D(X, Y) = \nabla^0_{X}{Y} -\nabla^1_{X}{Y}.
\end{align*} Then $D$ is \textbf{bilinear} over $\cC^{\infty}(M)$, and thus defines a \textbf{$(1,2)$-tensor field} called \textbf{the difference tensor between $\nabla^0$ and $\nabla^1$}.
\end{proposition}

\item \begin{theorem}
Let $M$ be a smooth manifold with or without boundary, and let $\nabla^0$ be any connection in $TM$. Then \textbf{the set $\cA(TM)$ of all connections} in $TM$ is equal
to the following \underline{\textbf{affine space}}:
\begin{align*}
\cA(TM) = \set{\nabla^0 + D:\, D \in \Gamma(T^{(1,2)}TM)},
\end{align*} 
where $D \in \Gamma(T^{(1,2)}TM)$ is interpreted as a map from $ \frX(M) \times \frX(M)$ to $\frX(M)$, and $\nabla^0 + D: \frX(M) \times \frX(M) \rightarrow \frX(M)$ is defined by
\begin{align*}
(\nabla^0 + D)(X, Y) &= \nabla^0_{X}{Y} + D(X, Y).
\end{align*}
\end{theorem}

\item \begin{remark}
Finally we can define \emph{\textbf{the covariant derivative of every $1$-form $\omega$}} based on connection on $TM$. In particular, \emph{\textbf{the connection on $1$-form}} $\nabla: \frX(M) \times \frX^{*}(M) \rightarrow \frX^{*}(M)$ can be defined as 
\begin{align} 
\inn{\conn{X}{\omega}}{Y} &= \conn{X}{\inn{\omega}{Y}} - \inn{\omega}{\conn{X}{Y}} \nonumber\\
\Rightarrow (\conn{X}{\omega})(Y)&= X(\omega(Y)) - \omega(\conn{X}{Y}). \label{eqn: connections_1_form}
\end{align} where $\inn{\omega}{Y} = \omega(Y)$ is \emph{a natural pairing}. The coordinate representation of connection on $1$-form is
\begin{align}
\conn{X}{\omega} &= \paren{X(\omega_k) - X^{j}\omega_{i}\Gamma_{j,k}^{i}}\epsilon^{k}  \label{eqn: connection_1_form_coordinate}
\end{align} where $(\epsilon^i)$ are coframes and $\omega = \omega_k\,\epsilon^k$, $X = X^i\,E_i$.
\end{remark}

\item \begin{remark}
For a covariant $2$-tensor field $g= g_{i,j} dx^i \otimes dx^j$, the covariant derivative of $g$ in direction of $Z$ is
\begin{align*}
(\conn{(Z)}{g})(X, Y) &= Z\paren{g(X, Y)} - g\paren{\conn{Z}{X}, Y} - g\paren{X, \conn{Z}{Y}}
\end{align*}
\end{remark}
\end{itemize}

\subsection{Total Covariant Derivatives}
\begin{itemize}
\item \begin{proposition} (\textbf{The Total Covariant Derivative}). \citep{lee2018introduction}\\
Let $M$ be a smooth manifold with or without boundary and let $\nabla$ be a connection in $TM$. For every $F\in \Gamma(T^{(k,l)}TM)$,  the map
\begin{align*}
\nabla F: \underbrace{\Omega^1(M) \xdotx{\times} \Omega^1(M)}_{k} \times \underbrace{\frX(M) \xdotx{\times} \frX(M)}_{l+1} \rightarrow \cC^{\infty}(M)
\end{align*} given by
\begin{align}
\conn{}{F}\paren{\omega_1 \xdotx{,} \omega_k, Y_1 \xdotx{,} Y_l, X}&= (\conn{X}{F})\paren{\omega_1 \xdotx{,} \omega_k, Y_1 \xdotx{,} Y_l}\label{eqn: total_covariant_derivatives_tensor}
\end{align} defines a \textbf{smooth $(k,l+1)$-tensor field} on $M$ called \underline{\textbf{the total covariant derivative of $F$}}.
\end{proposition}

\item \begin{remark}
The total covariant derivative of $Y \in \frX(M) := \Gamma(T^{(1,0)}TM)$ is a \emph{\textbf{$(1,1)$-tensor field}}
\begin{align*}
\nabla Y (\omega, X) &= (\conn{X}{Y})(\omega) = \omega\paren{\conn{X}{Y}}.
\end{align*} %$\nabla Y \in \Gamma(T^{(1,1)}TM)$. 

Similarly, the total covariant derivative of $\omega \in \frX^{*}(M) = \Omega^1(M) = \Gamma(T^{(0,1)}TM)$ is a \emph{\textbf{$(0,2)$-tensor field}}
\begin{align*}
\nabla \omega (Y, X) &= (\conn{X}{\omega})(Y) = X(\omega(Y)) -  \omega\paren{\conn{X}{Y}}
\end{align*}
\end{remark}

\item \begin{remark} It can be verified that the following formula for total covariant derivative holds
\begin{align}
\conn{Y}{F} &= \text{tr}\paren{\conn{}{F} \otimes Y} \label{eqn: total_covariant_trace}
\end{align}
\end{remark}

\item \begin{definition}
Given vector fields $X, Y \in \frX(M)$, let us introduce the notation $\nabla^2_{X, Y}F$ for the $(k,l)$-tensor field obtained by inserting $X,Y$ in the last two slots of $\nabla^2 F= \nabla(\nabla F)$:
\begin{align*}
\nabla^2_{X, Y}F(\xdotx{}) &= \nabla^2 F(\ldots, Y, X)
\end{align*}
\end{definition}

\item \begin{proposition}
Let $M$ be a smooth manifold with or without boundary and let $\nabla$ be a connection in $TM$. For every smooth vector field or tensor field $F$,
\begin{align}
\nabla^2_{X, Y}F &= \nabla_{X}\paren{\nabla_{Y}F} - \nabla_{(\conn{X}{Y})}F.  \label{eqn: second_covariant_derivation}
\end{align}
\end{proposition}

\item \begin{example}(\textbf{\emph{The Covariant Hessian}}).\\
Let $u$ be a smooth function on $M$.
\begin{itemize}
\item The \underline{\emph{\textbf{total covariant derivative of a smooth function is equal to its $1$-form}}} $\nabla u = du \in \Omega^1(M) = \Gamma(T^{(0,1)}TM)$ since 
\begin{align*}
\nabla u(X) &= \conn{X}{u} = Xu = du(X)
\end{align*}

\item The $2$-tensor $\nabla^2u = \nabla(du)$  is called \underline{\emph{\textbf{the covariant Hessian of $u$}}}. Its action
on smooth vector fields $X,Y$ can be computed by the following formula:
\begin{align}
\nabla^2u(Y, X) = \nabla_{X, Y}^2 u &= \conn{X}{\conn{Y}{u}} - \conn{(\conn{X}{Y})}{u} = X(Yu) - (\conn{X}{Y})(u)  \label{eqn: covariant_hessian}
\end{align} In any local coordinates, it is
\begin{align*}
\nabla^2u &= u_{\;;i,j}\,dx^i \otimes dx^j
\end{align*} where 
\begin{align*}
u_{\;;i,j} &= \partdiff{}{x^j}\partdiff{u}{x^i} - \Gamma_{j,i}^{k}\partdiff{u}{x^k}
\end{align*}
\end{itemize}
\end{example}
\end{itemize}

\subsection{Vector and Tensor Fields Along Curves}
\begin{itemize}
\item \begin{definition}
Let $M$ be a smooth manifold with or without boundary. Given a smooth curve $\gamma: I \rightarrow M$, \underline{\emph{\textbf{a vector field along $\gamma$}}} is a \emph{continuous} map $V: I \rightarrow TM$ such that $V(t) \in T_{\gamma(t)}M$ for every $t \in I$; it is \emph{\textbf{a smooth vector field along $\gamma$}} if it is \emph{smooth} as a map from $I$ to $TM$. 

We let $\frX(\gamma)$ denote \emph{\textbf{the set of all smooth vector fields along $\gamma$}}. It is a \emph{real vector space} under pointwise vector addition and multiplication by constants, and it is a module over $\cC^{\infty}(I)$ with multiplication defined pointwise:
\begin{align*}
(fX)(t) &= f(t)X(t).
\end{align*}
\end{definition}

\item \begin{remark} (\emph{\textbf{Construction of A Smooth Vector Field Along the Curve}})\\
Suppose $\gamma: I \rightarrow M$ is a smooth curve and $\widetilde{V} \in \frX(M)$ is a smooth vector field on an open subset of $M$ containing the image of $\gamma$. The smooth vector field along the curve $\gamma$, $V = \widetilde{V} \circ \gamma$:
\begin{align*}
V(t) &= \widetilde{V}_{\gamma(t)} \in T_{\gamma(t)}M.
\end{align*} A smooth vector field along $\gamma$ is said to be \emph{\textbf{extendible}} if there \emph{exists} a smooth vector field $\widetilde{V}$ on
a neighborhood of the image of $\gamma$ that is related to $V$ in this way. 

\emph{\textbf{Not every vector field along a curve need be extendible}}; for example, if $\gamma(t_1) = \gamma(t_2)$ but $\gamma'(t_1) \neq \gamma'(t_2)$, then $\gamma'$ is not extendible. 
\end{remark}

\item \begin{definition}
More generally, \underline{\emph{\textbf{a tensor field along $\gamma$}}} is a \emph{continuous} map $\sigma$ from $I$ to some tensor bundle $T^{(k,l)}TM$ such that $\sigma(t) \in T^{(k,l)}T_{\gamma(t)}M$ for each $t \in I$. 

It is a \emph{\textbf{smooth tensor field along $\gamma$}} if it is \emph{smooth} as a map from $I$ to $T^{(k,l)}TM$, and it is \emph{\textbf{extendible}} if there is a smooth tensor field $\widetilde{\sigma}$ on a neighborhood of $\gamma(I)$ such that $\widetilde{\sigma} = \sigma \circ \gamma$.
\end{definition}

\item \begin{theorem} (\textbf{Covariant Derivative Along a Curve}). \\
Let $M$ be a smooth manifold with or without boundary and let $\nabla$ be a connection in $TM$. For each smooth curve $\gamma: I \rightarrow M$, the \textbf{connection} determines \textbf{a unique operator} 
\begin{align*}
D_t: \frX(\gamma) \rightarrow \frX(\gamma)
\end{align*} called \underline{\textbf{the covariant derivative along $\gamma$}}, satisfying the following properties:
\begin{enumerate}
\item (\textbf{Linearity over $\bR$}):
\begin{align*}
D_t\paren{a\,V + b\,W} &= a\,D_t(V) + b\,D_t(W), \quad \text{ for }a,b \in \bR.
\end{align*}
\item (\textbf{Product Rule}):
\begin{align*}
D_t(f\,V) &= f'\,V + f D_t(V), \quad \text{ for }f \in \cC^{\infty}(I).
\end{align*}
\item If $V \in \frX(\gamma)$ is \textbf{extendible}, then for every extension $\widetilde{V}$ of $V$,
\begin{align*}
D_t(V(t)) &= \conn{\gamma'(t)}{\widetilde{V}}.
\end{align*}
\end{enumerate} There is \textbf{an analogous operator} on the space of \textbf{smooth tensor fields} of any type along $\gamma$.
\end{theorem}


\item \begin{remark} (\emph{\textbf{Coordinate Representation for Covariant Derivatives Along a Curve}})\\
Choose smooth coordinates $(x^i)$ for $M$ in a neighborhood of $\gamma(t_0)$, and write
\begin{align*}
V(t) &=  V^{i}(t) \partdiff{}{x^i}\Bigr|_{\gamma(t)}
\end{align*} for $t$ near $t_0$, where $V^1 \xdotx{,} V^n$ are \emph{smooth real-valued functions} defined on some neighborhood of $t_0$ in $I$. By the properties of $D_t$, since each $\partdiff{}{x^i}$ is extendible,
\begin{align}
D_t\paren{V_t} &= \dot{V}^i(t) \partdiff{}{x^i}\Bigr|_{\gamma(t)} + V^i(t)\, \conn{\gamma'(t)}{\partdiff{}{x^i}\Bigr|_{\gamma(t)}} \nonumber\\
&= \paren{\dot{V}^k(t)  + \dot{\gamma}^{i}(t)V^j(t)\,\Gamma_{i,j}^{k}(\gamma(t))}\partdiff{}{x^k}\Bigr|_{\gamma(t)} \label{eqn: covariant_derivative_along_curve_represent}
\end{align}
\end{remark}
\end{itemize}

\subsection{Geodesics}
\begin{itemize}
\item 
\begin{definition}
Let $M$ be a smooth manifold with or without boundary and let $\nabla$ be a connection in $TM$. For every smooth curve $\gamma: I \rightarrow M$, we define the \underline{\emph{\textbf{acceleration}}} of $\gamma$ to be \emph{\textbf{the vector field $D_t(\gamma')$ along $\gamma$}}.
\end{definition}

\item \begin{definition}
A smooth curve $\gamma$ is called a \underline{\emph{\textbf{geodesic}}} (\emph{\textbf{with respect to $\nabla$}}) if \emph{its acceleration is zero}: $D_t(\gamma'(t)) = 0$. 
\end{definition}

\item \begin{remark}
\emph{\textbf{Geodesic}} is the curve whose \emph{\textbf{tangential acceleration}} is \emph{\textbf{zero}}. From \emph{the connection $\nabla$ point of view}, it specify both the directional vector field and the target vector field equal to $\gamma'(t)$. That is, the \emph{tangential acceleration along a curve} $\gamma$ is 
\begin{align*}
\conn{\gamma'(t)}{\gamma'(t)}.
\end{align*}
\end{remark}

\item \begin{remark} (\emph{\textbf{The Ordinary Differential Equations for the Geodesic}})\\
In terms of smooth coordinates $(x^i)$, if we write the component functions of $\gamma$ as $\gamma(t) = (x^1(t) \xdotx{,} x^n(t))$. From \eqref{eqn: covariant_derivative_along_curve_represent} and  $D_t\paren{\gamma'(t)}$, we have a set of ordinary differential equations called \underline{\emph{\textbf{the geodesic equations}}}:
\begin{align}
\ddot{x}^k(t)  + \dot{x}^{i}(t)\dot{x}^j(t)\,\Gamma_{i,j}^{k}(x(t)) = 0, \quad k=1,\ldots, n \label{eqn: geodesic_equation}.
\end{align} where $x(t) := (x^1(t) \xdotx{,} x^n(t))$. A (parameterized) curve $\gamma$ is a geodesic \emph{\textbf{if and only if}} its component functions satiesfy \emph{the geodesic equations}. This is a set of \emph{\textbf{\underline{second-order nonlinear} ODEs}}.
\end{remark}

\item \begin{theorem}(\textbf{Existence and Uniqueness of Geodesics}). \citep{lee2018introduction} \\
Let $M$ be a smooth manifold and $\nabla$ a connection in $TM$. For every $p \in M$, $w \in T_{p}M$, and $t_0 \in \bR$, there \textbf{exist} an open interval $I \subseteq \bR$ containing $t_0$ and a \textbf{geodesic} $\gamma: I \rightarrow M$  satisfying $\gamma(t_0) = p$ and $\gamma'(t_0) = w$. Any two such geodesics \textbf{agree} on their common domain.
\end{theorem}

\item \begin{remark}
From the geodesic equation, we see that \emph{\textbf{the only parameters} of the ODE that determines the geodesic is \textbf{the cooefficients of the connection} $\{\Gamma_{i,j}^{k}\}$}. That is, \emph{the geodesic} is solely determined by \emph{the connection $\nabla$}. Thus we also call it a \emph{\textbf{$\nabla$-geodesic}}.
\end{remark}

\item \begin{remark} The \emph{\textbf{geodesic equation} under the initial boundary condition} can be written in the following form:
\begin{align}
\dot{x}^k(t) &= v^{k}(t) \\
\dot{v}^k(t) &= - v^{i}(t)v^j(t)\,\Gamma_{i,j}^{k}(x(t)) \label{eqn: geodesic_equation_first_order}
\end{align} Treating $(x^1 \xdotx{,} x^n, v^1 \xdotx{,} v^n)$ as coordinates on $U \times \bR^n$, we can recognize \eqref{eqn: geodesic_equation_first_order} as the equations for the \textbf{\emph{flow}} of \textbf{\emph{the vector field}} $G \in \frX(U \times \bR^n)$ given by
\begin{align}
G_{(x, v)} &= v^k\partdiff{}{x^k}\Bigr|_{(x, v)} - v^{i}v^j\,\Gamma_{i,j}^{k}(x)\partdiff{}{v^k}\Bigr|_{(x, v)}.
\end{align} The importance of $G$ stems from the fact that it actually defines \emph{\textbf{a global vector field on the total space of $TM$}}, called \underline{\emph{\textbf{the geodesic vector field}}}. It can be verified that the components of $G$ under a change of coordinates \emph{take the same form} in \emph{every coordinate chart}.

Note that $G$ acts on a function $f \in \cC^{\infty}(U \times \bR^n)$ as
\begin{align}
Gf(p, v) &= \frac{d}{dt}\Bigr|_{t=0}f(\gamma_v(t), \gamma_v'(t)). \label{eqn: geodesic_vector_field_act_function}
\end{align}
\end{remark}

\item \begin{definition}
A geodesic $\gamma: I \rightarrow M$  is said to be \emph{\textbf{maximal}} if it \emph{cannot be extended} to a geodesic on a \emph{larger interval}, that is, if there does not exist a geodesic $\widetilde{\gamma}: \widetilde{I} \rightarrow M$ defined on an interval $\widetilde{I}$ properly containing $I$ and satisfying $\widetilde{\gamma}|_{I} = \gamma$. 

\emph{A \textbf{geodesic segment}} is a geodesic whose domain is a \emph{\textbf{compact interval}}.
\end{definition}

\item \begin{corollary}
Let $M$ be a smooth manifold and let $\nabla$ be a connection in $TM$. For each $p \in M$ and $v \in T_{p}M$, there is a \textbf{unique maximal geodesic} $\gamma: I \rightarrow M$ with $\gamma(0) = p$ and $\gamma'(0) = v$, defined on some open interval $I$ containing $0$.
\end{corollary}

\item \begin{definition}
The \emph{\textbf{unique maximal geodesic}} $\gamma$ with $\gamma(0) = p$ and $\gamma'(0) = v$ is often called simply \emph{\textbf{the geodesic with initial point $p$ and initial velocity $v$}}, and is denoted by $\gamma_v$. (Note that we can always find $p = \pi(v)$ where $\pi: TM \rightarrow M$ is the natural projection.)
\end{definition}
\end{itemize}

\subsection{Parallel Transport}
\begin{itemize}
\item \begin{definition}
Let $M$ be a smooth manifold and let $\nabla$ be a connection in $TM$. \emph{A smooth vector or tensor field $V$ along a smooth curve} $\gamma$ is said to be \emph{\textbf{parallel along} $\gamma$ (with respect to $\nabla$)} if $D_t(V) \equiv 0$.
\end{definition}

\item \begin{remark}
A \emph{geodesic} can be characterized as a curve whose \emph{\textbf{velocity vector field is parallel along the curve}}.
\end{remark}

\item \begin{remark} (\emph{\textbf{Coordinate Representation of Vector Field Parallel Along a Curve}})\\
Given a smooth curve $\gamma$ with a local coordinate representation $\gamma(t) = (\gamma^1(t) \xdotx{,} \gamma^n(t))$, formula \eqref{eqn: covariant_derivative_along_curve_represent} shows that a vector field $V$ is parallel along $\gamma$ if and only if
\begin{align}
\dot{V}^k(t)  + \dot{\gamma}^{i}(t)V^j(t)\,\Gamma_{i,j}^{k}(\gamma(t)) = 0, \quad k=1,\xdotx{,} n \label{eqn: vector_field_parallel_along_curve_represent}
\end{align} This is a set of \emph{\textbf{linear ordinary differential equations}} with respect to $(V^1(t) \xdotx{,} V^n(t))$.
\end{remark}

\item \begin{theorem} (\textbf{Existence and Uniqueness of Parallel Transport}). \\
Suppose $M$ is a smooth manifold with or without boundary, and $\nabla$ is a connection in $TM$. Given a smooth curve $\gamma: I \rightarrow M$, $t_0 \in I$, and a vector $v \in T_{\gamma(t_0)}M$ or tensor $v \in T^{(k,l)}T_{\gamma(t_0)}M$, there exists a \textbf{unique parallel vector or tensor field $V$} along $\gamma$ such that $V(t_0) = v$.
\end{theorem}

\item \begin{remark}
\emph{The vector or tensor field} whose existence and uniqueness are proved in Theorem aboive is called \emph{\textbf{the parallel transport of $v$ along $\gamma$}}.
\end{remark}

\item \begin{definition}
For each $t_0, t_1 \in I$, we define a map 
\begin{align}
P^{\gamma}_{t_0, t_1}: T_{\gamma(t_0)}M \rightarrow T_{\gamma(t_1)}M,  \label{eqn: parallel_transport_map}
\end{align} called \underline{\emph{\textbf{the parallel transport map}}}, by setting 
\begin{align*}
P^{\gamma}_{t_0, t_1}(v) &= V(t_1), \quad \forall v \in T_{\gamma(t_0)}M
\end{align*} where \emph{$V$ is the parallel transport of $v$ along $\gamma$}. 

This map is \emph{\textbf{linear}}, because \emph{the equation of parallelism is linear}. It is in fact an \emph{\textbf{isomorphism}}, because $P^{\gamma}_{t_1, t_0}$ is an \emph{\textbf{inverse}} for it.
\end{definition}

\item \begin{remark} (\emph{\textbf{Parallel Frames Along a Curve}})\\
Given any basis $(b_1 \xdotx{,} b_n)$ for $T_{\gamma(t_0)}M$, we can \emph{\textbf{parallel transport the vectors $b_i$ along $\gamma$}}, thus obtaining an $n$-tuple of \emph{parallel vector fields} $(E_1 \xdotx{,} E_n)$  along $\gamma$. Because each parallel transport map is an \emph{isomorphism}, \emph{\textbf{the vectors $(E_i(t))$ form a basis for $T_{\gamma(t)}M$ at each point $\gamma(t)$}}. Such an $n$-tuple of vector fields along $\gamma$ is called \emph{\textbf{a
parallel frame along $\gamma$}}.

Every smooth (or piecewise smooth) vector field along $\gamma$ can be expressed in terms of such a frame as 
\begin{align*}
V(t) = V^i(t)\, E_i(t),
\end{align*} and then the properties of covariant derivatives along curves, together with the fact that the $E_i$'s are parallel,
imply
\begin{align}
D_t(V_t) &= \dot{V}^i(t)\, E_i(t)  \label{eqn: parallel_transport_basis_vector_fields}
\end{align} wherever $V$ and $\gamma$ are smooth. This means that \emph{a vector field is \textbf{parallel} along $\gamma$} \emph{if and only} if \emph{\textbf{its component functions with respect to the frame $(E_i)$ are constants}}.
\end{remark}

\item \begin{theorem} (\textbf{Parallel Transport Determines Covariant Differentiation}). \citep{lee2018introduction}\\
Let $M$ be a smooth manifold with or without boundary, and let $\nabla$ be a connection in $TM$. Suppose $\gamma: I \rightarrow M$ is a smooth curve and $V$ is a smooth vector field along $\gamma$. For each $t_0 \in I$,
\begin{align}
D_t V(t_0) &=\lim_{\Delta t \rightarrow 0}\frac{P^{\gamma}_{(t_0+\Delta t), t_0}(V(t_0+\Delta t))  - V(t_0)}{\Delta t} \label{eqn: covariant_derivatives_from_parallel_transport}
\end{align}
\end{theorem}

\item \begin{corollary} (\textbf{Parallel Transport Determines the Connection}). \citep{lee2018introduction} \\
Let $M$ be a smooth manifold with or without boundary, and let  $\nabla$ be a connection in $TM$. Suppose $X$ and $Y$ are smooth vector fields on $M$. For every $p \in M$,
\begin{align}
\conn{X}{Y}\bigr|_{p}&=\lim_{t \rightarrow 0}\frac{P^{\gamma}_{t, 0}(Y_{\gamma(t)})  - Y_p}{t}, \label{eqn: connection_from_parallel_transport}
\end{align} where $\gamma: I \rightarrow M$ is any smooth curve such that $\gamma(0) = p$ and $\gamma'(0) = X_p$.
\end{corollary}

\item \begin{remark}
See similarity between \eqref{eqn: connection_from_parallel_transport} and the definition of Lie derivatives:
\begin{align*}
\paren{\mathscr{L}_{X}\,Y}_{p}  &= \lim_{t\rightarrow 0}\frac{ d(\theta_{-t})_{\theta_{t}(p)}\paren{Y_{\theta_t(p)}}   - Y_{p}}{t},
\end{align*} where $\theta$ is the \emph{\textbf{flow of $X$}} in the neighborhood of $p$ such that $\theta_0(p) = p$, $(\theta^{(p)})'(0) = X_p$.
\end{remark}

\item \begin{remark}
A smooth vector or tensor field on $M$ is said to be \emph{\textbf{parallel}} \emph{(with respect to $\nabla$)} if it is \emph{parallel along \textbf{every smooth curve} in $M$}. 
\end{remark}

\item \begin{proposition}
Suppose $M$ is a smooth manifold with or without boundary, $\nabla$ is a connection in $TM$, and $A$ is a \textbf{smooth vector or tensor field} on $M$. Then $A$ is
parallel on $M$ if and only if $\nabla A \equiv 0$.
\end{proposition}

\item \begin{remark}
It is always possible to extend a vector at a point to a parallel vector field along any given curve. However, it may not be possible in general to extend it to a \emph{\textbf{parallel vector field}} on an open subset of the manifold. The impossibility of finding such extensions is intimately connected with the phenomenon of \emph{\textbf{curvature}}.
\end{remark}

\item \begin{remark}
We see that both the concept of \emph{\textbf{connections}} and the concept of \emph{\textbf{parallel transport along a curve}} can be derived from each other.
\begin{align*}
\nabla & \xrightleftharpoons{}{} P_{t_0, t_1}^{\gamma}
\end{align*} They both define a way that ``\emph{connects}" the tangent space $T_pM$ at $p=\gamma(t_0)$ and the tangent space $T_{q}M$ at $q = \gamma(t_0 + \Delta t)$ in $p$'s close neighborhood. The former begins with \emph{\textbf{a set of rules} for a mapping} and the latter begins with \emph{\textbf{covariant derivatives along a curve}}.
\end{remark}
\end{itemize}
\section{Lie Derivatives}
\begin{itemize}
\item \begin{definition}
Suppose $M$ is a smooth manifold, $V$ is a \emph{smooth vector field} on $M$; and $\theta$ is the \emph{\textbf{flow of $V$}}. For any
smooth vector field $W$ on $M$, define \emph{\textbf{a rough vector field}} on $M$, \emph{denoted by} $\mathscr{L}_{V}\,W$ and called the \underline{\emph{\textbf{Lie derivative} of $W$ with respect to $V$}}, by
\begin{align}
\paren{\mathscr{L}_{V}\,W}_{p} &= \lim_{t\rightarrow 0}\frac{ d(\theta_{-t})_{\theta_{t}(p)}\paren{W_{\theta_t(p)}}   - W_{p}}{t} \label{eqn: lie_derivative_formula}\\
&= \frac{d}{dt}\Bigr|_{t=0} d\paren{\theta_{-t}}_{\theta_{t}(p)}\paren{W_{\theta_t(p)}}  \nonumber,
\end{align}
provided the derivative exists. For small $t \neq 0$, at least the difference quotient makes sense: $\theta_t$ is defined in a neighborhood of $p$, and $\theta_{-t}$ is the inverse of $\theta_t$, so both $d(\theta_{-t})_{\theta_{t}(p)}\paren{W_{\theta_t(p)}}$ and $W_p$ are elements of $T_{p}M$.
\end{definition}

\item \begin{remark}
If $M$ has nonempty boundary, this definition of  $\mathscr{L}_{V}\,W$ makes sense as long as $V$ is tangent to $\partial M$ so that its flow exists.
\end{remark}

\item \begin{lemma}
Suppose $M$ is a smooth manifold with or without boundary, and $V, W \in \mathfrak{X}(M)$. If $\partial M \neq \emptyset$, assume in addition that $V$ is tangent to $\partial M$. Then $(\mathscr{L}_{V}\,W)_{p}$ exists for every $p \in M$, and $\mathscr{L}_{V} W$ is a \textbf{smooth vector field}.
\end{lemma}

\item The following theorem is critical to understand the \emph{\textbf{Lie derivatives}} and \emph{\textbf{Lie bracket}}.
\begin{theorem}
If $M$ is a smooth manifold and $V, W \in \mathfrak{X}(M)$, then $\mathscr{L}_{V}\,W = [V, W]$.
\end{theorem}

\item \begin{remark}
This theorem allows us to extend the definition of the \emph{\textbf{Lie derivative}} to arbitrary \emph{smooth vector fields} on a smooth manifold $M$ with boundary. Given $V, W \in \mathfrak{X}(M)$ we define $(\mathscr{L}_{V}\,W)_{p}$ for $p \in \partial M$ by embedding $M$ in a smooth manifold $\widetilde{M}$ without boundary (such as the double of $M$), extending $V$ and $W$ to smooth vector fields on $\widetilde{M}$, and computing the Lie derivative there. By virtue of the preceding theorem,
$(\mathscr{L}_{V}\,W)_{p} = [V, W]_p$ is independent of the choice of extension.
\end{remark}

\item \begin{remark} This thoerem also gives us a \emph{\textbf{geometric interpretation}} of \emph{the Lie bracket of two vector fields}: it is the \emph{\textbf{directional derivative} of the second vector field} along \emph{the \textbf{flow} of the first}. 
\end{remark}

\item \begin{corollary}
Suppose $M$ is a smooth manifold with or without boundary, and $V, W, X \in \mathfrak{X}(M)$.
\begin{enumerate}
\item (\textbf{Anti-symmetric}) $\mathscr{L}_{V}\,W = -\mathscr{L}_{W}\,V$.
\item (\textbf{Product Rule}) $\mathscr{L}_{V}\,[W,\, X] =  [\mathscr{L}_{V}\,W, \,X] + [W,\, \mathscr{L}_{V}\,X]$.
\item $\mathscr{L}_{[V, W]}\,X = \mathscr{L}_{V}\mathscr{L}_{W}\,X - \mathscr{L}_{W}\mathscr{L}_{V}\,X$.
\item If $g \in \cC^{\infty}(M)$, then $\mathscr{L}_{V}\,(gW) = (Vg)\,W + g\,\mathscr{L}_{V}\,W$.
\item (\textbf{Pushforward}) If $F: M \rightarrow N$ is a \textbf{diffeomorphism}, then $F_{*}(\mathscr{L}_{V}\,X) = \mathscr{L}_{F_{*}V}\,F_{*}X$.
\end{enumerate}
\end{corollary}

\item \begin{remark}
Note that the Lie derivative is \emph{\textbf{not linear over $\cC^{\infty}(M)$ in $V$}}, i.e.
\begin{align*}
\mathscr{L}_{fV}\,W &\neq f\,\mathscr{L}_{V}\,W
\end{align*}
\end{remark}

\item \begin{remark}
If $V$ and $W$ are vector fields on $M$ and $\theta$ is the flow of $V$, the Lie derivative $(\mathscr{L}_{V}\,W)_{p}$, by definition, expresses the $t$-derivative of \emph{the \textbf{time-dependent vector}} $d\paren{\theta_{-t}}_{\theta_{t}(p)}\paren{W_{\theta_t(p)}}  \in T_{p}M$ at $t = 0$. The next proposition shows how it can also
be used to compute the derivative of this expression at other times. 
\end{remark}

\item \begin{proposition}
Suppose $M$ is a smooth manifold with or without boundary and $V, W \in \mathfrak{X}(M)$. If $\partial M \neq \emptyset$, assume also that $V$ is tangent to $\partial M$. Let $\theta$ be the flow of $V$. For any $(t_0, p)$ in the domain of $\theta$,
\begin{align}
 \frac{d}{dt}\Bigr|_{t=t_0} d\paren{\theta_{-t}}_{\theta_{t}(p)}\paren{W_{\theta_t(p)}}&= d(\theta_{-t_0})\paren{(\mathscr{L}_{V}\,W)_{\theta_{t_0}(p)}}.  \label{eqn: lie_derivative_at_other_time}
\end{align}
\end{proposition}

\end{itemize}


\section{Lie Group and Lie Algebra}


\newpage
\bibliographystyle{plainnat}
\bibliography{book_reference.bib}
\end{document}