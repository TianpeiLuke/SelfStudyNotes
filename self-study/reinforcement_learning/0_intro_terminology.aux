\relax 
\citation{cesa2006prediction,shalev2012online}
\@writefile{toc}{\contentsline {section}{\numberline {1}Reinforcement Learning}{2}}
\citation{sutton2018reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Generalized Policy iteration}}\relax }}{5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: GPI}{{1}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Summary of methods learned}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}From Multi-armed Bandit to MDP}{5}}
\newlabel{eqn: increment_update}{{1}{5}}
\citation{puterman2014markov}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Tabular methods vs. Function approximation}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Tabular methods}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Tabular methods (a) and Function approximation methods (b) diagram}}\relax }}{7}}
\newlabel{fig: diagram_tabluar_function_approximation_methods}{{2}{7}}
\newlabel{eqn: policy_improv_theorem_1}{{2}{8}}
\newlabel{eqn: policy_improv_theorem_2}{{3}{8}}
\newlabel{eqn: bellman_eqn_value}{{4}{8}}
\newlabel{eqn: bellman_eqn_value3}{{5}{8}}
\newlabel{eqn: bellman_eqn_value_action}{{6}{8}}
\newlabel{eqn: bellman_eqn_value_action_3}{{7}{8}}
\newlabel{eqn: bellman_eqn_optimal_value3}{{8}{8}}
\newlabel{eqn: bellman_eqn_optimal_value2}{{9}{8}}
\newlabel{eqn: bellman_eqn_optimal_value_action3}{{10}{8}}
\newlabel{eqn: bellman_eqn_optimal_value_action2}{{11}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Function Approximation}{10}}
\newlabel{eqn: sgd_fun_approx}{{12}{10}}
\citation{franccois2018introduction}
\citation{franccois2018introduction}
\newlabel{eqn: td_fun_approx}{{13}{11}}
\newlabel{eqm: semi_grad_sarsa}{{14}{11}}
\newlabel{eqn: semi_grad_q_learning}{{15}{11}}
\newlabel{eqn: semi_grad_exp_sarsa}{{16}{11}}
\newlabel{eqn: generalized_linear_form}{{17}{11}}
\newlabel{eqn: policy_grad_theorem}{{18}{12}}
\newlabel{eqn: policy_grad_theorem2}{{19}{12}}
\newlabel{eqn: policy_grad_theorem3}{{20}{12}}
\newlabel{eqn: reinforce_update}{{21}{12}}
\newlabel{eqn: reinforce_update_w_baseline}{{22}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {The Actor-Critic methods}}\relax }}{13}}
\newlabel{fig: actor_critic}{{3}{13}}
\newlabel{eqn: actor_update}{{23}{13}}
\newlabel{eqn: critic_update}{{24}{13}}
\newlabel{eqn: soft_max_policy}{{25}{13}}
\newlabel{eqn: log_soft_max_policy}{{26}{13}}
\newlabel{eqn: grad_log_soft_max_policy}{{27}{13}}
\citation{koller2009probabilistic}
\newlabel{eqn: normal_policy}{{28}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Learning vs. Planning}{14}}
\citation{liu2001monte}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {The generic relationship between model learning, direct RL, planning in a planning agent. Note that the value/policy can be updated either by real experience via direct RL or by simulated experience via planning. Meanwhile, the real experience will be used for model update/learning. }}\relax }}{15}}
\newlabel{fig: Planning_learning_relationship}{{4}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {The Dyna-Q architecture. }}\relax }}{16}}
\newlabel{fig: Dyna_Q}{{5}{16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Reward and Average reward}{16}}
\newlabel{eqn: returns_episodic}{{29}{16}}
\newlabel{eqn: returns_discount_continuing}{{30}{16}}
\newlabel{eqn: avg_reward_discount_continuing}{{31}{17}}
\newlabel{eqn: avg_reward_discount_continuing2}{{32}{17}}
\newlabel{eqn: avg_reward_discount_continuing3}{{33}{17}}
\newlabel{eqn: stationary_dist_eqn}{{34}{17}}
\newlabel{eqn: returns_diff_continuing}{{35}{17}}
\newlabel{eqn: bellman_eqn_value_diff}{{36}{18}}
\newlabel{eqn: bellman_eqn_value_diff_2}{{37}{18}}
\newlabel{eqn: bellman_eqn_value_action_diff}{{38}{18}}
\newlabel{eqn: bellman_eqn_value_action_diff_2}{{39}{18}}
\newlabel{eqn: bellman_opt_eqn_value_diff}{{40}{18}}
\newlabel{eqn: bellman_opt_eqn_value_diff_2}{{41}{18}}
\newlabel{eqn: bellman_opt_eqn_value_action_diff}{{42}{18}}
\newlabel{eqn: bellman_opt_eqn_value_action_diff_2}{{43}{18}}
\bibstyle{plainnat}
\bibdata{reference.bib}
\bibcite{cesa2006prediction}{{1}{2006}{{Cesa-Bianchi and Lugosi}}{{}}}
\bibcite{franccois2018introduction}{{2}{2018}{{Fran{\c {c}}ois-Lavet et~al.}}{{Fran{\c {c}}ois-Lavet, Henderson, Islam, Bellemare, Pineau, et~al.}}}
\bibcite{koller2009probabilistic}{{3}{2009}{{Koller and Friedman}}{{}}}
\bibcite{liu2001monte}{{4}{2001}{{Liu and Liu}}{{}}}
\bibcite{puterman2014markov}{{5}{2014}{{Puterman}}{{}}}
\bibcite{shalev2012online}{{6}{2012}{{Shalev-Shwartz et~al.}}{{}}}
\bibcite{sutton2018reinforcement}{{7}{2018}{{Sutton and Barto}}{{}}}
