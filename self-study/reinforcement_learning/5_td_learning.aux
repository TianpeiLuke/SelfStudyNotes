\relax 
\citation{sutton2018reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Temporal-Difference Prediction}{2}}
\newlabel{eqn: incremental_update}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {TD($0$) algorithm}}\relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: TD_0_algo}{{1}{3}}
\newlabel{eqn: incremental_update_td}{{2}{3}}
\newlabel{eqn: DP_target}{{4}{3}}
\newlabel{eqn: MC_target}{{5}{3}}
\citation{sutton2018reinforcement}
\citation{sutton2018reinforcement}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Backup diagram for (a) Dynamic Programming (DP); (b) Monte Carlo (MC) methods; (c) Temporal-Difference (TD(0)) learning. Note that both DP and TD used boostrapping to compute state-value based on previous estimate of state-value function. MC only updates at the end of episode.}}\relax }}{4}}
\newlabel{fig: backup_diagram_dp_mc_td}{{2}{4}}
\newlabel{eqn: td_error}{{6}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Advantages of TD Prediction Methods}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {An example from \citep  {sutton2018reinforcement} based on random walk Markov decision process. It can be seen that TD converge faster than MC since it updates value estimate faster at each time step.}}\relax }}{5}}
\newlabel{fig: td_mc_example}{{3}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Sarsa, on-policy TD control}}\relax }}{6}}
\newlabel{fig: sarsa_control}{{4}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Temporal-Difference Control}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Sarsa: On-policy TD Control}{6}}
\newlabel{eqn: bellman_eqn_value_action2}{{7}{6}}
\newlabel{eqn: sarsa_predict}{{8}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Q-learning, off-policy TD control}}\relax }}{7}}
\newlabel{fig: q_learning}{{5}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Q-learning: Off-policy TD Control}{7}}
\newlabel{eqn: bellman_eqn_optimal_value_action3}{{9}{7}}
\newlabel{eqn: q_learning}{{10}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Backup diagram for (a) Sarsa; (b) Q-learning; (c) Expected Sarsa.}}\relax }}{8}}
\newlabel{fig: backup_diagram_td_control}{{6}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Expected Sarsa: Off-policy TD Control}{8}}
\newlabel{eqn: bellman_eqn_value_action3}{{11}{8}}
\newlabel{eqn: expected_sarsa_predict}{{12}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Experiment performance between Sarsa, Q-learning, Expected Sarsa. Note that Sarsa is impacted by the choice of $\alpha $ but Q-learning and Expected Sarsa are not.}}\relax }}{9}}
\newlabel{fig: q_learning}{{7}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Summary}{9}}
\bibstyle{plainnat}
\bibdata{reference.bib}
\bibcite{sutton2018reinforcement}{{1}{2018}{{Sutton and Barto}}{{}}}
