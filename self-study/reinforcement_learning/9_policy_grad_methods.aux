\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Value-based methods vs. Policy-based methods}{2}}
\newlabel{eqn: avg_reward_discount_continuing3}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Policy Gradient methods}{2}}
\newlabel{eqn: grad_ascent}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Policy approximation}{2}}
\newlabel{eqn: soft_max_policy}{{3}{3}}
\newlabel{eqn: log_soft_max_policy}{{4}{3}}
\newlabel{eqn: grad_log_soft_max_policy}{{5}{3}}
\newlabel{eqn: normal_policy}{{6}{4}}
\newlabel{eqn: log_normal_policy}{{7}{4}}
\newlabel{eqn: grad_log_normal_policy}{{8}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {3}The Policy Gradient Theorem}{4}}
\newlabel{eqn: obj_policy_grad_episodic}{{9}{4}}
\citation{sutton2018reinforcement}
\newlabel{eqn: obj_policy_grad_continuing}{{10}{5}}
\newlabel{eqn: policy_grad_theorem}{{11}{5}}
\newlabel{eqn: policy_grad_theorem2}{{12}{5}}
\newlabel{eqn: policy_grad_theorem3}{{13}{5}}
\newlabel{eqn: grad_avg_rewards}{{14}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {4}REINFORCE: Monte Carlo Policy Gradient}{7}}
\newlabel{eqn: policy_grad_sga}{{15}{7}}
\newlabel{eqn: reinforce_update}{{16}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {The REINFORCE: Monte Carlo policy gradient}}\relax }}{8}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: reinforce_algo}{{1}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {The REINFORCE with value function baseline: Monte Carlo policy gradient}}\relax }}{8}}
\newlabel{fig: reinforce_baseline_algo}{{2}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {The Actor-Critic methods}}\relax }}{9}}
\newlabel{fig: actor_critic}{{3}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}REINFORCE with baseline}{9}}
\newlabel{eqn: reinforce_update_w_baseline}{{17}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Actor-Critic Methods}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {The Actor-Critic methods}}\relax }}{10}}
\newlabel{fig: actor_critic_algo}{{4}{10}}
\newlabel{eqn: actor_update}{{18}{10}}
\newlabel{eqn: critic_update}{{19}{10}}
\newlabel{eqn: semi_grad_sarsa}{{20}{11}}
\newlabel{eqn: semi_grad_q_learning}{{21}{11}}
\newlabel{eqn: semi_grad_exp_sarsa}{{22}{11}}
\bibstyle{plainnat}
\bibdata{reference.bib}
\bibcite{sutton2018reinforcement}{{1}{2018}{{Sutton and Barto}}{{}}}
