\relax 
\citation{jurafsky2014speech}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Hidden Markov Model notations}.}\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: hmm}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Hidden Markov Model}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}HMM for POS tagging}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {Viterbi algorithm for decoding HMM}.}\relax }}{3}}
\newlabel{fig: hmm_decode}{{2}{3}}
\newlabel{eqn: bellman_eqn}{{1}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Conditional Random Fields (CRFs)}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Inference on CRF}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {The inference process for RNN on sequence labeling task}.}\relax }}{5}}
\newlabel{fig: rnn_inference}{{3}{5}}
\newlabel{eqn: CRF_inference}{{2}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Recurrent Neural Networks}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}LSTM}{6}}
\citation{vaswani2017attention}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {The Long Short Term Memory}.}\relax }}{7}}
\newlabel{fig: lstm}{{4}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Transformer}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {The encoder-decoder structure of transformer}.}\relax }}{8}}
\newlabel{fig: transformer_block}{{5}{8}}
\newlabel{eqn: attention_comp}{{3}{9}}
\citation{he2016deep}
\newlabel{eqn: self_attention_mat}{{4}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Transformer blocks}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {The multi-head attention}.}\relax }}{11}}
\newlabel{fig: multi-head-attn}{{6}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Multi-head attention}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {The simple way to implement positional embedding}.}\relax }}{12}}
\newlabel{fig: pos_embedding}{{7}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Modeling word order: positional embeddings}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Encoder-Decoder structure}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Transformer for Contextual Generation and Summarization}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {\textbf  {The autoregressive text generation using transformer}.}\relax }}{13}}
\newlabel{fig: auto_text}{{8}{13}}
\bibstyle{plainnat}
\bibdata{book_reference.bib}
\bibcite{he2016deep}{{1}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{jurafsky2014speech}{{2}{2014}{{Jurafsky and Martin}}{{}}}
\bibcite{vaswani2017attention}{{3}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
