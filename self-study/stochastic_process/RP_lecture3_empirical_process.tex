\documentclass[11pt]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent %\usepackage{graphicx}
\usepackage{amsmath,amssymb, mathrsfs,  mathtools, dsfont}
\usepackage{tabularx}
\usepackage{tikz-cd}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{xcolor}
%\usepackage[linkbordercolor ={1 1 1} ]{hyperref}
%\usepackage[sf]{titlesec}
\usepackage{natbib}
\usepackage{../../Tianpei_Report}

%\usepackage{appendix}
%\usepackage{algorithm}
%\usepackage{algorithmic}

%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}



\begin{document}
\title{Lecture 3: Empirical Processes}
\author{ Tianpei Xie}
\date{Feb. 1st., 2023}
\maketitle
\tableofcontents
\newpage
\section{Uniform Law of Large Numbers}
\subsection{Motivations}
\begin{itemize}
\item \begin{remark}(\textbf{\emph{Unbiased Estimator of Cumulative Distribution Function}})\\
The law of any scalar random variable $X$ can be fully specified by its \emph{\textbf{cumulative distribution function (CDF)}}, whose value at any point $t \in \bR$ is given by $F(t) := \cP[X \le t]$. Now suppose that we are given a collection $\set{X_i}^{n}_{i=1}$ of $n$ i.i.d. samples, each drawn according to the law specified by $F$. A natural \emph{estimate} of $F$ is \emph{\textbf{the empirical CDF}} given by
\begin{align}
\widehat{F}_n(t) &:=\frac{1}{n}\sum^{n}_{i=1}\mathds{1}_{(-\infty, t]}(X_i),\label{def: empirical_cdf}
\end{align} where $\mathds{1}_{(-\infty, t]}(x)$ is a $\set{0, 1}$-valued indicator function for the event $\set{x \le t}$. Since \emph{\textbf{the population CDF}} can be written as $F(t) = \E{}{\mathds{1}_{(-\infty, t]}(X)}$, the empirical CDF is an \emph{\textbf{unbiased estimate}}.

For each $t \in \bR$, \emph{\textbf{the strong law of large numbers}} suggests that 
\begin{align*}
\widehat{F}_n(t) \rightarrow F(t), \quad \text{ a.s.}
\end{align*} A natural goal is to strengthen\emph{ this pointwise convergence} to a form of \emph{\textbf{uniform convergence}}. The reason why uniform convergence of $\widehat{F}_n(t)$ to $F(t)$ is important is that it can be used to prove the \emph{\textbf{consistency}} of \emph{\textbf{plug-in estimator}} for \emph{functionals of distribution function}.
\end{remark}

\item \begin{example} (\textbf{\emph{Expectation Functionals}}) \\
Given some integrable function $g$, we may define \underline{\textbf{\emph{the expectation functional}}} $\gamma_g$ via
\begin{align}
\gamma_{g}(F) &:= \int g(x) dF(x). \label{def: exp_functional}
\end{align} For any g, \emph{the plug-in estimate} is given by $\gamma_g(\widehat{F}_n) = \frac{1}{n}\sum^{n}_{i=1}g(X_i)$, corresponding to \emph{\textbf{the sample mean}} of $g(X)$. 
\end{example}

\item \begin{example} (\textbf{\emph{Quantile Functionals}}) \\
For any $\alpha \in [0, 1]$, \underline{\textbf{\emph{the quantile functional}}} $Q_{\alpha}$ is given by
\begin{align}
Q_{\alpha}(F) &:= \inf\set{t \in \bR: F(t) \ge \alpha}. \label{def: quantile_functional}
\end{align} The \emph{\textbf{median}} corresponds to the special case  $\alpha= 0.5$. \emph{The plug-in estimate} is given by
\begin{align}
Q_{\alpha}(\widehat{F}_n) &:= \inf\set{t \in \bR: \frac{1}{n}\sum^{n}_{i=1}\mathds{1}_{(-\infty, t]}(X_i) \ge \alpha}  \label{def: quantile_functional_estimate}
\end{align} and corresponds to estimating the $\alpha$-th quantile of the distribution by \emph{the $\alpha$-th \textbf{sample quantile}}. In the special case $\alpha= 0.5$, this estimate corresponds to \emph{the sample median}.  In this case, $Q_{\alpha}(\widehat{F}_n)$ is a fairly complicated, \emph{nonlinear function of all the
variables}, so that this convergence does not follow immediately by a classical result such as the law of large numbers.
\end{example}

\item \begin{example}(\emph{\textbf{Goodness-of-fit Functionals}})\\
It is frequently of interest to test the hypothesis of whether or not a given set of data has been drawn from a known distribution $F_0$. Such tests can
be performed using \emph{functionals that \textbf{measure the distance} between $F$ and the target CDF $F_0$}, including \emph{the sup-norm distance} $\norm{F - F_0}{\infty}$, or other distances such as \emph{\textbf{the Cramer-von Mises criterion}} based on the functional 
\begin{align*}
\gamma_{g}(F) &:= \int_{-\infty}^{+\infty} \paren{F(x) - F_0(x)}^2 dF_0(x)
\end{align*}
\end{example}

\item \begin{remark} (\emph{\textbf{Consistency of Plug-In Estimate}})\\
For any \emph{\textbf{plug-in estimator}} $\gamma_g(\widehat{F}_n)$, an important question is to understand when it is \emph{\textbf{consistent}} -- that is, when does $\gamma_g(\widehat{F}_n)$ converge to $\gamma_{g}(F)$ in \emph{probability} (or \emph{almost surely})?

We can define \emph{the \textbf{continuity} of a \textbf{functional}} $\gamma$ with respect to \emph{the supremum norm}: more precisely, we say that the functional $\gamma$ is \emph{\textbf{continuous} at $F$ in the \textbf{sup-norm}} if, for all $\epsilon > 0$, there exists a $\delta > 0$ such that 
\begin{align*}
\norm{G - F}{\infty} := \sup_{t\in \bR}\abs{G(t) - F(t)} \le \delta  \quad \text{ implies that }\quad \abs{\gamma(G) - \gamma(F)} \le \epsilon.
\end{align*} 
Thus for any \emph{\textbf{continuous functional}}, it reduces the \emph{\textbf{consistency}} question for \emph{the plug-in estimator} $\gamma_g(\widehat{F}_n)$ to the issue of whether or not the random variable $\|\widehat{F}_n - F\|_{\infty}$ \emph{\textbf{converges to zero}}. 
\end{remark}
\end{itemize}
\subsection{Glivenko-Cantelli Theorem}
\begin{itemize}
\item \begin{theorem} (\textbf{Glivenko-Cantelli Theorem})  \citep{wellner2013weak, wainwright2019high, gine2021mathematical} \\ 
For any distribution, the empirical CDF 
\begin{align*}
\widehat{F}_n(t) &:=\frac{1}{n}\sum^{n}_{i=1}\mathds{1}_{(-\infty, t]}(X_i)
\end{align*}  is a \textbf{strongly consistent estimator} of the population CDF in \textbf{the uniform norm}, meaning that
\begin{align}
\norm{\widehat{F}_n - F}{\infty} := \sup_{t\in \bR}\abs{\widehat{F}_n(t) - F(t)} \rightarrow 0, \text{ a.s. } \label{eqn: gilvenko_cantelli}
\end{align}
\end{theorem}
\end{itemize}


\section{Empirical Processes}
\subsection{Definitions}
\begin{itemize}
\item \begin{definition}(\textbf{\emph{Empirical Measure}})  \citep{wellner2013weak, gine2021mathematical} \\
Let $(\cX, \srF, \cP)$ be a \emph{probability space}, and let $X_i, i \in \bN$, be the \emph{coordinate functions} of \emph{the \textbf{infinite product probability space}} $(\Omega, \srB,  \bP) := (\cX^{\infty}, \srF^{\infty}, \cP^{\infty})$,  $X_i :  \cX^{\infty} \to \cX$, which are \emph{\textbf{independent} identically distributed} $\cX$-valued random variables with law $\cP$. %In fact, we will always take \emph{independent variables} (equally distributed or not) to be \emph{the coordinate functions} on \emph{product probability spaces}.

\underline{\emph{\textbf{The empirical measure}}} corresponding to the `\emph{observations}' $X_1 \xdotx{,} X_n$, for any $n \in \bN$, is defined as \emph{\textbf{the \underline{random} discrete probability measure}}
\begin{align}
\cP_n &:= \frac{1}{n}\sum_{i=1}^{n}\delta_{X_i} \label{def: empirical_measure}
\end{align} where $\delta_x$ is \emph{Dirac measure} at $x$, that is, unit mass at the point $x$.  In other words, for each event $A$,  $\cP_n(A)$ is \emph{the \textbf{proportion} of \textbf{observations} $X_i$,  $i = 1 \xdotx{,} n$, that fall in $A$}; that is,
\begin{align*}
\cP(A) &= \frac{1}{n}\sum_{i=1}^{n}\delta_{X_i}(A) = \frac{1}{n}\sum_{i=1}^{n}\ind{X_i \in A}, \quad A \in \srF.
\end{align*}
\end{definition}

\item \begin{remark}(\textbf{\emph{Probability Measure with Operator Notation}}) \citep{wellner2013weak, gine2021mathematical} \\
For any measure $\mu$ and $\mu$-integrable function $f$, we will use the following \underline{\textbf{\emph{operator notation}}} for the integral of $f$ with respect to $\mu$:
\begin{align*}
\mu f \equiv \mu(f) =  \int_{\Omega} f d\mu.
\end{align*} This is valid since there exists an isomorphism between \emph{the space of probability measure} and \emph{the space of bounded linear functional} on $\cC_{0}(\Omega)$ by Riesz-Markov representation theorem (assuming $\Omega$ is \emph{locally compact}). By this notion the expectation $ \cP f = \E{\cP}{f}$.
\end{remark}

\item \begin{definition} (\textbf{\emph{Empirical Process}})  \citep{wellner2013weak, gine2021mathematical} \\
Let $\cF$ be a \emph{collection of $\cP$-integrable functions} $f: \cX \to \bR$, usually infinite. For any such class of functions $\cF$, \underline{\emph{\textbf{the empirical measure}}} defines a \emph{\textbf{stochastic process}}
\begin{align}
f \rightarrow \cP_{n}f, \quad f\in \cF \label{def: empirical_process}
\end{align} which we may call  \underline{\emph{\textbf{the empirical process indexed by $\cF$}}}, although we prefer to reserve the
notation `\emph{empirical process}' for \emph{the \textbf{centred} and \textbf{normalised} process}
\begin{align}
f \rightarrow \nu_{n}(f) :=\sqrt{n} \paren{\cP_n f - \cP f}, \quad f\in \cF. \label{def: centered_empirical_process}
\end{align}
\end{definition}

\item \begin{remark}
An explicit notion of \emph{(centered and normalized) empirical process} is
\begin{align*}
\sqrt{n} \paren{\cP_n f - \cP f} \equiv \frac{1}{\sqrt{n}}\sum_{i=1}^n\paren{f(X_i) - \E{\cP}{f(X)}}, \quad f\in \cF.
\end{align*} where $X_1 \xdotx{,} X_n \sim \cP$ are i.i.d random variables. Note that it is a stochastic process since \emph{the function $f$ is changing} in $\cF$, i.e. the process $\paren{\cP_n  - \cP}f$ is indexed by function $f \in \cF$ not finite dimensional variable.
\end{remark}

\item \begin{remark}(\textbf{\emph{Random Measure}})\\
Normally we assume that data are sampled from some distribution $\cP$ and the data itself is random. However, the empirical measure 
\begin{align*}
\cP_n &:= \frac{1}{n}\sum_{i=1}^{n}\delta_{X_i}
\end{align*} itself is considered as a \emph{\textbf{random}} probability measure. That is, \emph{the sampling mechanism itself contains randomness} and it is not sampling from one distribution but \emph{\textbf{a system of distributions depending on the choice of dataset $X_1 \xdotx{,} X_n$ }}, which in turn were sampled from some \emph{prior} $\cP$. Due to this randomness, $\cP_n f = \E{\cP_n}{f}$ is not a fixed expectation number but a random variable. In fact, this is the empirical mean (i.e. sample mean)
\begin{align*}
\cP_n f = \E{\cP_n}{f} = \frac{1}{n}\sum_{i=1}^n f(X_i).
\end{align*} \emph{\textbf{The critical difference}} between mean of empirical measure vs. sample mean is that we now assume that $f$ is \emph{\textbf{not fixed}}.
\end{remark}

\item \begin{remark} (\emph{\textbf{Object of Empirical Process Theory}}) \\
\emph{The \textbf{object} of empirical process theory} is to study \emph{the \textbf{properties} of the \textbf{approximation}} of $\cP f$ by $\cP_n f$, \emph{\textbf{uniformly in $\cF$}}, concretely, to obtain both \emph{\textbf{probability estimates} for the random quantities}
\begin{align*}
\norm{\cP_n  - \cP}{\cF} &:= \sup_{f \in \cF}\abs{\cP_n f - \cP f}
\end{align*}
and \emph{\textbf{probabilistic limit theorems}} for the processes $\set{ (\cP_n - \cP)(f) : f \in \cF}$.

Note that the quantity $\norm{\cP_n  - \cP}{\cF}$ is a \emph{\textbf{random variable}} since $\cP_n$ is a \emph{\textbf{random measure}}.
\end{remark}

\item \begin{remark} (\textbf{\emph{Measurability Problem}})\\
There may be a \emph{\textbf{measurability problem}} for 
\begin{align*}
\norm{\cP_n  - \cP}{\cF} &:= \sup_{f \in \cF}\abs{\cP_n f - \cP f}
\end{align*}  since \emph{the \textbf{uncountable} suprema} of measurable functions \emph{may not be measurable}. 

However, there are many situations where this is actually a \emph{\textbf{countable supremum}}. For instance, for probability distribution on $\bR$
\begin{align*}
\norm{\cP_n  - \cP}{\infty} &:= \sup_{t \in \bR}\abs{\paren{\cP_n - \cP}(-\infty, t)} =  \sup_{t \in \bQ}\abs{F_n(t) - F(t)} =  \sup_{t \in \bQ}\abs{\paren{\cP_n - \cP}(-\infty, t)}
\end{align*} where $F(t) = \cP\paren{-\infty, t}$ is the cumulative distribution function. If $\cF$ is \emph{countable} or if there exists $\cF_0$ \emph{countable} such that
\begin{align*}
\norm{\cP_n  - \cP}{\cF}  = \norm{\cP_n  - \cP}{\cF_0}, \quad \text{a.s.} 
\end{align*} then the measurability problem disappears.

For the next few sections we will simply assume that the class $\cF$ is \emph{countable}.
\end{remark}

\item \begin{remark} (\textbf{\emph{Bounded Assumption}})\\
If we assume that
\begin{align}
\sup_{f \in \cF}\abs{f(x) - \cP f} < \infty, \quad \forall x \in \cX, \label{def: empirical_process_bounded}
\end{align}
then the maps from $\cF$ to $\bR$,
\begin{align*}
f \rightarrow f(x) - \cP f, \quad x\in \cX,
\end{align*} are \emph{\textbf{bounded functionals}} over $\cF$, and therefore, so is $f \to  (\cP_n - \cP)(f)$. That is,
\begin{align*}
\cP_n - \cP \in \ell_{\infty}(\cF),
\end{align*}
where  $\ell_{\infty}(\cF)$ is \emph{\textbf{the space of bounded real functionals} on $\cF$}, a \emph{Banach space} if we equip it with the supremum norm $\norm{\cdot}{\cF}$. 

A large literature is available on \emph{probability in \textbf{separable Banach spaces}}, but unfortunately, $\ell_{\infty}(\cF)$ is \emph{\textbf{only}} \emph{\textbf{separable}} when the class $\cF$ is \emph{\textbf{finite}}, and \emph{\textbf{measurability problems}} arise because \emph{the probability law} of the process $\set{ (\cP_n - \cP)(f) : f \in \cF}$ \emph{\textbf{does not extend} to the Borel $\sigma$-algebra of $\ell_{\infty}(\cF)$} even in simple situations.
\end{remark}

\item \begin{remark}
This chapter addresses \emph{\textbf{three main questions}} about the empirical process:
\begin{enumerate}
\item The first question has to do with \underline{\emph{\textbf{concentration}} of $\norm{\cP_n  - \cP}{\cF}$ about \emph{its \textbf{mean}}} when \underline{$\cF$ is \emph{\textbf{uniformly bounded}}}. Recall that $\norm{\cP_n  - \cP}{\cF}$ is a random variable itself, due to randomness of the empirical measure. We mainly use the \emph{non-asymptotic analysis} to obtain \emph{the exponential bound for concentration}.

\item The second question is do \underline{\emph{\textbf{good estimates}} for \emph{\textbf{mean}} $\E{}{\norm{\cP_n  - \cP}{\cF}}$} exist? We will examine two main techniques that give answers to this question, both related to \emph{\textbf{metric entropy}} and \emph{\textbf{chaining}}. One of them, called \emph{\textbf{bracketing}}, uses \emph{chaining} in combination with \emph{truncation} and \emph{Bernstein's inequality}. The other one applies to \emph{\textbf{Vapnik-Cervonenkis (VC) classes of functions}}.

\item Finally, the last question about the empirical process refers to \underline{\emph{\textbf{limit theorems}}}, mainly \underline{\emph{\textbf{the uniform law of large numbers}}} and the \underline{\emph{\textbf{central limit theorem}}}, in fact, the analogues of \emph{\textbf{the classical Glivenko-Cantelli}} and \emph{Donsker theorems} for the empirical distribution function.

Formulation of \emph{the central limit theorem} will require some more \emph{measurability} because we will be considering \emph{\textbf{convergence in law}} of random elements in \emph{\textbf{not necessarily separable} \textbf{Banach spaces}}.
\end{enumerate}
\end{remark}
\end{itemize}
\subsection{Glivenko-Cantelli Class}
\begin{itemize}
\item \begin{definition} (\textbf{\emph{Glivenko-Cantelli Class}}) \citep{wellner2013weak, wainwright2019high, gine2021mathematical}\\
We say that $\cF$ is a \underline{\emph{\textbf{Glivenko-Cantelli class}}} for $\cP$ if 
\begin{align*}
\norm{\cP_n  - \cP}{\cF} &:= \sup_{f \in \cF}\abs{\cP_n f - \cP f} \to 0
\end{align*}  \emph{\textbf{in probability}} as $n \to \infty$. 

This notion can also be defined in a \emph{stronger} sense, requiring \emph{\textbf{almost sure convergence}} of $\norm{\cP_n  - \cP}{\cF}$, in which case we say that $\cF$ satisfies a \underline{\emph{\textbf{strong Glivenko-Cantelli law}}}.
\end{definition}

\item \begin{example} (\textbf{\emph{Empirical CDFs and Indicator Functions}}) \\
Consider the function class
\begin{align}
\cF := \set{\mathds{1}_{(-\infty, t]}(\cdot), t \in \bR}  \label{def: emp_cdf_class}
\end{align}
where $\mathds{1}_{(-\infty, t]}$ is the \emph{$\set{0, 1}$-valued indicator function} of the interval $(-\infty, t]$. For each fixed $t \in \bR$, we have the equality $ \E{}{\mathds{1}_{(-\infty, t]}(X)} = \cP[X \le t] = F(t)$, so that the classical \emph{Glivenko-Cantelli theorem} is equivalent to a \emph{\textbf{strong uniform law} for the class} \eqref{def: emp_cdf_class},
\end{example}
\end{itemize}

\subsection{Tail bounds for Empirical Processes}
\begin{itemize}
\item \begin{remark}
Consider \underline{\emph{the \textbf{suprema} of \textbf{empirical process}}}:
\begin{align}
Z := \sup_{f \in \cF}\set{\cP_n f} =  \sup_{f \in \cF}\set{ \frac{1}{n}\sum_{i=1}^n f(X_i)} \label{def: suprema_emp_process}
\end{align} where $(X_1 \xdotx{,} X_n)$ are independent random variables drawn from $\cP := \otimes_{i=1}^{n}\cP_i$,  each $\cP_i$ is supported on some set $\cX_i \subseteq \cX$.  $\cF$ is a family of real-valued functions $f: \cX \to \bR$. The primary goal of this section is to derive a number of \emph{upper bounds} on \emph{the tail event} $\set{Z \ge \E{}{Z} + t}$.
\end{remark}

\item \begin{theorem}  (\textbf{Functional Hoeffding Inequality}) \citep{wainwright2019high, boucheron2013concentration}\\
For each $f \in \cF$ and $i = 1 \xdotx{,} n$, assume that there are real numbers $a_{i,f} \le b_{i,f}$ such that $f(x) \in [a_{i,f},  b_{i,f}]$ for all $x \in \cX_i$.
Then for all $t \ge 0$, we have
\begin{align}
\cP\set{Z \ge \E{}{Z} + t} \le \exp\paren{-\frac{n t^2}{4 L^2}} \label{ineqn: hoeffding_inequality_functional}
\end{align} where $Z :=\sup_{f \in \cF}\set{ \frac{1}{n}\sum_{i=1}^n f(X_i)}$, and $L^2 := \sup_{f \in \cF}\set{\frac{1}{n}\sum_{i=1}^{n}\paren{a_{i,f} - b_{i,f}}^2}$.
\end{theorem}

\item \begin{theorem}(\textbf{Functional Bernstein Inequality, Talagrand Concentration for Empirical Processes})  \citep{wainwright2019high, boucheron2013concentration}\\
Consider a \textbf{countable} class of functions $\cF$ \textbf{uniformly bounded} by $b$. Then for all  $t > 0$, the suprema of empirical process $Z$ as defined in \eqref{def: suprema_emp_process} satisfies the upper tail bound
\begin{align}
\cP\set{Z \ge \E{}{Z} + t} \le \exp\paren{-\frac{n t^2}{8e\Sigma^2 + 4bt}} \label{ineqn: bernstein_inequality_functional}
\end{align} where $\Sigma^2 := \E{}{\sup_{f \in \cF}\set{\frac{1}{n}\sum_{i=1}^{n}f^2(X_i)}}$ is \textbf{the weak variance}.
\end{theorem}

\item \begin{remark}
As opposed to control only in terms of \emph{\textbf{bounds} on the \textbf{function values}}, the inequality \eqref{ineqn: bernstein_inequality_functional} \textbf{\emph{also}} brings a notion of \emph{\textbf{variance}} into play. 
\end{remark}

\item \begin{remark}
We will prove the bound in next section:
\begin{align*}
\Sigma^2 \le \sigma^2 + 2b\E{}{Z}
\end{align*} where $\sigma^2 := \sup_{f\in \cF}\E{}{f^2(X)}$. Then, the functional Bernstein inequality \eqref{ineqn: bernstein_inequality_functional} can be formulated as
\begin{align}
\cP\set{Z \ge \E{}{Z} +c_0 \gamma \sqrt{t} + c_1 bt} \le  e^{- n t}  \label{ineqn: bernstein_inequality_functional_2}
\end{align} for some constant $c_0, c_1$ and $\gamma^2 := \sigma^2 + 2b\E{}{Z}$. We can have an alternative form of this bound \eqref{ineqn: bernstein_inequality_functional_2} for any $\epsilon > 0$, 
\begin{align}
\cP\set{Z \ge (1+\epsilon)\E{}{Z} +c_0 \sigma \sqrt{t} + (c_1 + c_0^2/\epsilon)bt} \le  e^{- n t}.  \label{ineqn: bernstein_inequality_functional_3}
\end{align}
\end{remark}

\item \begin{theorem} (\textbf{Bousquet's Inequality, Functional Bennet Inequality})  \citep{boucheron2013concentration}\\
Let $X_1 \xdotx{,} X_n$ be \textbf{independent} \textbf{identically distributed} random vectors. Assume that $\E{}{f(X_i)} = 0$, and that $f(X_i) \le 1$ for all $f \in \cF$.
Let
\begin{align*}
\gamma^2= \sigma^2 + 2\E{}{Z},
\end{align*} where $\sigma^2 := \sup_{f\in \cF}\set{\frac{1}{n}\sum_{i=1}^n\E{}{f^2(X_i)}}$ is \textbf{the wimpy variance}. Let $\phi(u) = e^u - u - 1$ and $h(u) = (1 + u) \log(1 + u) - u$, for $u \ge -1$. Then for all $\lambda \ge 0$,
\begin{align*}
\log \E{}{e^{\lambda\paren{Z - \E{}{Z}}}} \le n\gamma^2 \phi(\lambda).
\end{align*}
Also, for all $t \ge 0$,
\begin{align}
\cP\set{Z \ge \E{}{Z} + t} \le \exp\paren{- n\gamma^2 \, h\paren{\frac{t}{\gamma^2}}}. \label{ineqn: bousquet_inequality_bennet_functional}
\end{align}
\end{theorem}
\end{itemize}
\subsection{Maximal Inequalities}



\section{Variance of Suprema of Empirical Process}
\subsection{General Upper Bounds for the Variance}
\begin{itemize}
\item \begin{definition}(\textbf{\emph{Variances of Empirical Process}})\\
Let $X_1 \xdotx{,} X_n$ be independent random variables taking values in $\cX$.  Depending on \emph{\textbf{ordering}} of the \emph{\textbf{expectation}}, \emph{\textbf{suprema}} and \emph{\textbf{summation}} operator, we define \emph{three different types of \textbf{variance}} associated with empirical process
\begin{align*}
\cP_n f = \E{\cP_n}{f} = \frac{1}{n}\sum_{i=1}^n f(X_i).
\end{align*}
\begin{enumerate}
\item \emph{\textbf{The strong variance}} is defined as
\begin{align}
V &:= \frac{1}{n}\sum_{i=1}^n \E{}{\sup_{f\in \cF}f^2(X_i)}  \label{def: strong_variance_emp_process}
\end{align}
\item  \emph{\textbf{The weak variance}} is defined as
\begin{align}
\Sigma^2 &:= \E{}{\sup_{f \in \cF}\set{\frac{1}{n}\sum_{i=1}^{n}f^2(X_i)}}  \label{def: weak_variance_emp_process}
\end{align}
\item  \emph{\textbf{The wimpy variance}} is defined as
\begin{align}
\sigma^2 &:= \sup_{f\in \cF}\set{\frac{1}{n}\sum_{i=1}^n \E{}{f^2(X_i)}} \label{def: wimpy_variance_emp_process}
\end{align}
\end{enumerate} 
By Jensen's inequality, 
\begin{align*}
\sigma^2 \le \Sigma^2 \le V
\end{align*} In general, there may be \emph{significant gaps between any two of these quantities}. A notable difference is the case of \emph{\textbf{Rademacher averages}} when $\sigma^2 = \Sigma^2$.
\end{definition}
\end{itemize}
\subsection{Symmetrization and Contraction Principle}
%\subsection{Uniform Convergence via Rademacher Complexity}
\subsection{Bounding the Weak Variance via Wimpy Variance}
\subsection{Rademacher Complexity and Gaussian Complexity}

\section{Expected Value of Suprema of Empirical Process}
\subsection{Covering Number, Packing Number and Metric Entropy}
\subsection{Chaining and Dudley's Entropy Integral}
\subsection{Vapnik-Chervonenkis Class}
\subsection{Comparison Theorems}




\newpage
\bibliographystyle{plainnat}
\bibliography{reference.bib}
\end{document}