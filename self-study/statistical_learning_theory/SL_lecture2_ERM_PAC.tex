\documentclass[11pt]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent %\usepackage{graphicx}
\usepackage{amsmath,amssymb, mathrsfs, dsfont}
\usepackage{tabularx}
\usepackage[all,cmtip]{xy}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{xcolor}
%\usepackage[linkbordercolor ={1 1 1} ]{hyperref}
%\usepackage[sf]{titlesec}
\usepackage{natbib}
%\usepackage{tikz-cd}

\usepackage{../../Tianpei_Report}

%\usepackage{appendix}
%\usepackage{algorithm}
%\usepackage{algorithmic}

%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}



\begin{document}
\title{Lecture 2: Probably Approximately Correct  Learning}
\author{ Tianpei Xie}
\date{ Jul. 30th., 2015 }
\maketitle
\tableofcontents
\newpage
\section{PAC Learning in Deterministic Setting}
\subsection{Definitions}
\begin{itemize}
\item \begin{remark}
\emph{In deterministic scenario}, denote a collection of $n$ \emph{independent identically distributed (i.i.d.) \textbf{random samples}} generated by $P_X$ as $\cD_n$, i.e.
\begin{align*}
\cD_n:= \set{X_i: 1\le i\le n}.
\end{align*} Denote $\cC$ as \emph{the set of all concepts} we wish to learn as the \underline{\emph{\textbf{concept class}}}:
\begin{align*}
\cC := \set{c: \cX \rightarrow \cY } =\cY^{\cX}.
\end{align*} A \underline{\emph{\textbf{learner}}}  considers a \emph{\textbf{fixed} subset of concepts}  $\cH\subset \cC$, which is referred as a \underline{\emph{\textbf{hypothesis class}}}, and provides a \emph{\textbf{hypothesis}} or a \underline{\emph{\textbf{classifier}}} or a \emph{\textbf{decision function}} $g\in \cH \subset \cY^{\cX}$ based on $\cD$. The task of \underline{\emph{\textbf{supervised learning}}} is to minimize \emph{the generalization error}:
\begin{align}
L(g) &= \cP_{X}\set{g(X) \neq c(X)} \equiv \E{X}{\mathds{1}_{g(X)\neq c(X)}}\label{expr: gen_err_determin}
\end{align} where $\mathds{1}_{\omega}$ is the indicator function of the event $\omega$.

The generalization error of a hypothesis is not directly accessible to the learner since both the distribution $\cP$ and the target concept $c$ are unknown. However, the
learner can measure \emph{\textbf{the empirical error}} of a hypothesis on the labeled sample $\cD_n$:
\begin{align*}
 \widehat{L}_{n}(g) &=\frac{1}{n} \sum_{i=1}^{n}\ind{g(X_i)\neq c(X_i)}, &&  (\text{deterministic setting}).
\end{align*} Recall that the expectation of empirical error under $\cP_{X}$ is the generalization error
\begin{align*}
\E{X}{ \widehat{L}_{n}(g)} &= L(g).
\end{align*}
\end{remark}

\item We denote by $\cO(d)$ an \emph{upper bound} on \emph{the \textbf{cost} of the \textbf{computational representation}} of any element $x \in \cX \subset \bR^d$ and by $\text{size}(c)$ \emph{\textbf{the maximal cost} of the computational representation of $c \in \cC$}. For example, $x$ may be a vector in $\bR^d$,
for which the cost of an array-based representation would be in $\cO(n)$.
\begin{definition} (\emph{\textbf{Probably Approximately Correct (PAC) Learning}})\\
A \textit{\textbf{concept class}} $\cC$ is said to be \underline{\emph{\textbf{Probably Approximately Correct-learnable}}} if there exists an \emph{algorithm} $\cA$ and
a \emph{\textbf{polynomial function}} $\text{poly}(\cdot, \cdot, \cdot, \cdot)$ such that for any $\epsilon > 0$ and $\delta > 0$, for \emph{\textbf{all distributions}} $\cP$ on $X$ and \emph{for \textbf{any target concept}} $c \in \cC$, the following holds for \emph{\textbf{any sample size}} 
$n \ge \text{poly}(1/\epsilon, 1/\delta, d, \text{size}(c)):$
\begin{align}
\cP_{\cD_{n}}\set{ L(g_n(\cdot|\cD_{n})) \le \epsilon} &\ge  1 - \delta  \label{eqn: pac_definition}\\
\Leftrightarrow \cP_{\cD_{n}}\set{ L(g_n(\cdot|\cD_{n})) \ge \epsilon} &\le   \delta  \nonumber
\end{align}
If $\cA$ further runs in $\text{poly}(1/\epsilon, 1/\delta, d, \text{size}(c))$, then $\cC$ is said to be \underline{\emph{\textbf{efficiently PAC-learnable}}}. When such an algorithm $\cA$ exists, it is called a \emph{\textbf{PAC-learning algorithm}} for $\cC$.
\end{definition}

\item \begin{remark}
\emph{A concept class} $\cC$ is thus \emph{PAC-learnable} if the hypothesis returned by the algorithm after observing a number of points \emph{polynomial} in $1/\epsilon$ and $1/\delta$ is
\begin{enumerate}
\item \emph{\textbf{approximately correct}} (\emph{error at most $\epsilon$}) 
\item \emph{\textbf{with high probability}} (at least $1 - \delta$), 
\end{enumerate}
which justifies \emph{the PAC terminology}. 
\begin{enumerate}
\item $\delta > 0$ is used to define \emph{\textbf{the confidence}} $1 - \delta$
\item and $\epsilon > 0$ \emph{\textbf{the accuracy}} $1-\epsilon$. 
\end{enumerate}
Note that if the running time of the algorithm is \emph{\textbf{polynomial}} in $1/\epsilon$ and $1/\delta$, then \emph{\textbf{the sample size}} $n$ must also be polynomial if the full sample is received by the algorithm.
\end{remark}

\item \begin{remark}
Several key points of the PAC definition are worth emphasizing. 
\begin{enumerate}
\item First, \emph{the PAC framework is a \textbf{distribution-free model}}: \emph{no particular assumption} is made about \emph{the distribution $\cP_{X}$} from which examples are drawn. 

\item Second, \emph{the training sample} and \emph{the test examples} used to define the error are drawn \emph{according to \textbf{the same distribution} $\cP_{X}$}. This is a necessary assumption for generalization to be possible in most cases.

\item Finally, \emph{the PAC framework deals with the question of \textbf{learnability for a concept class} $\cC$ and not a particular concept}. Note that \emph{\textbf{the concept class $\cC$ is known to the algorithm}}, but of course target concept $c \in \cC$ is unknown.
\end{enumerate}
We may omit the polynomial dependency on $n$ and $\text{size}(c)$ in the PAC definition and \emph{focus only on the \textbf{sample complexity}}.
\end{remark}
\end{itemize}

\subsection{PAC-Learnable Guarantees for Finite Hypothesis Sets}
\begin{itemize}
\item \begin{remark} (\emph{\textbf{Finite Hypothesis Sets}})\\
We will see if the size of hypothesis class $\cH$ is finite, i.e. $\abs{\cH} < \infty$, we can achieve \emph{PAC learnability} if the target $c \in \cH$:
\end{remark}

\item \begin{proposition} (\textbf{Learning bounds, Finite $\cH$, Consistent Case}) \citep{mohri2018foundations}\\
Let $\cH$ be a \textbf{finite} set of functions mapping from $\cX$ to $\cY$. Let $\cA$ be an algorithm that for \textbf{any target concept $c \in \cH$} and i.i.d. sample $\cD_m$ returns a \underline{\textbf{consistent hypothesis} $g_m$}, i.e. the training error of $g_m$ is zero:
\begin{align*}
\widehat{L}_{m}(g_m) = 0
\end{align*} 
Then, for any $\epsilon, \delta > 0$, the inequality 
\begin{align*}
\cP_{\cD_m}\set{ L(g_m(\cdot|\cD_{m})) \le \epsilon} &\ge  1 - \delta
\end{align*} holds if
\begin{align}
m &\ge \frac{1}{\epsilon}\paren{\log \abs{\cH} + \log\frac{1}{\delta}} \label{eqn: pac_sample_complexity_finite_case}
\end{align}
This sample complexity result admits the following \textbf{equivalent statement} as a generalization bound: for any $\epsilon, \delta > 0$, with \textbf{probability} at least $1 - \delta$,
\begin{align}
 L(g_m(\cdot|\cD_{m})) :=   \cP_{\cD_{m}}\set{g_m(X|\cD_{m}) \neq c(X)}&\le \frac{1}{m}\paren{\log\abs{\cH} + \log\frac{1}{\delta} }. \label{eqn: pac_sample_complexity_finite_case_consistency}
\end{align}
\end{proposition}

\item To prove the general case, we use a bound from \emph{Hoeffding's inequality}:
\begin{proposition}
Fix $\epsilon > 0$ and let $\cD_{m}$ denote an i.i.d. sample of size $m$. Then, for any hypothesis $g: \cX \to \set{0, 1}$, the following inequalities hold:
\begin{align}
\cP_{\cD_m}\set{\widehat{L}(g)  - L(g) \ge \epsilon} \le \exp\paren{-2m\epsilon^2} \label{eqn: hoeffding_binary_bound_1}\\
\cP_{\cD_m}\set{\widehat{L}(g)  - L(g) \le -\epsilon} \le \exp\paren{-2m\epsilon^2} \label{eqn: hoeffding_binary_bound_2}
\end{align}
By the union bound, this implies the following two-sided inequality:
\begin{align}
\cP_{\cD_m}\set{\abs{\widehat{L}(g)  - L(g)} \ge \epsilon} \le 2\exp\paren{-2m\epsilon^2}. \label{eqn: hoeffding_binary_bound_union}
\end{align}
\end{proposition}
Setting the right-hand side of \eqref{eqn: hoeffding_binary_bound_union} to be equal to $\delta$ and solving for $\epsilon$ yields immediately the following bound for \emph{a single hypothesis}

\begin{corollary} (\textbf{Generalization bound for Single Hypothesis})  \citep{mohri2018foundations}\\
Fix a hypothesis $g: \cX \to \set{0, 1}$. Then, for any $\delta > 0$, the following inequality holds with probability at least $1 - \delta$:
\begin{align}
 L(g) \le  \widehat{L}(g) + \sqrt{\frac{\log\frac{2}{\delta}}{2m}}  \label{eqn: pac_one_hypothesis}
\end{align}
\end{corollary} This error bound can be seen as coming from \emph{the \textbf{randomness} of \textbf{coin tossing}} when approximate the generalization error $L(g)$ by training error. Thus it will always exist for any generalization error bound. 

\item \begin{proposition} (\textbf{Learning bounds, Finite $\cH$, Inconsistent Case}) \citep{mohri2018foundations}\\
Let $\cH$ be a \textbf{finite} set of functions mapping from $\cX$ to $\cY$.  Then, for any $\delta > 0$ with probability at least $1 - \delta$, the following inequality holds: for all $g \in \cH$
\begin{align}
 L(g) \le  \widehat{L}(g) + \sqrt{\frac{\log\abs{\cH} + \log\frac{2}{\delta}}{2m}}  \label{eqn: pac_finite_case_consistency}
\end{align} Thus for a finite hypothesis set $\cH$, 
\begin{align}
 L(g) \le  \widehat{L}(g) + O\paren{\sqrt{\frac{\log\abs{\cH}}{m}}}  \label{eqn: pac_finite_case_consistency_bigO}
\end{align}
\end{proposition}

\item \begin{remark} (\emph{\textbf{Sample Efficiency $\Leftarrow$ Representation Efficiency of Hypothesis Class}})\\
As already pointed out, $\log\abs{\cH}$ can be interpreted as \emph{\textbf{the number of bits} needed to \textbf{represent} $\cH$}. Several other remarks similar to those made on the generalization bound in the consistent case can be made here: 

\emph{a larger sample size $m$ guarantees \textbf{better generalization}, and the bound \textbf{increases} with $\abs{\cH}$, but only \textbf{logarithmically}}.

But, here, the bound is a less favorable function of $\log\abs{\cH}/m$; it varies as \emph{\textbf{the square root} of this term}. This is not a minor price to pay: for a \emph{fixed} $\abs{\cH}$, to attain the \emph{same guarantee} as \emph{in the \textbf{consistent case}}, \emph{a \textbf{quadratically larger} \textbf{labeled} sample is needed}.
\end{remark}

\item \begin{example}(\emph{\textbf{Conjunction of Boolean Literals}}) \citep{mohri2018foundations}\\
\end{example}

\item \begin{example}(\emph{\textbf{k-term DNF Formulae}})  \citep{mohri2018foundations}\\
\end{example}

\item \begin{example}(\emph{\textbf{k-CNF Formulae}})  \citep{mohri2018foundations}\\
\end{example}
\end{itemize}



\subsection{PAC-Learnable Examples for Infinite Hypothesis Sets}
\begin{itemize}
\item \begin{example} (\emph{\textbf{Learning Axis-Aligned Rectangles}}) \citep{mohri2018foundations}\\
Consider the case where the set of instances are \emph{points} in the plane, $\cX = \bR^2$, and the concept class $\cC$ is the set of \emph{\textbf{all axis-aligned rectangles}} lying in $\bR^2$. Thus, each concept $c$ is \emph{the set of points inside a particular axis-aligned rectangle}. The learning
problem consists of determining with small error a target axis-aligned rectangle using the labeled training sample. We will show that \emph{the concept class of axis-aligned rectangles} is \emph{\textbf{PAC-learnable}}.
\end{example}



\item \begin{example} (\emph{\textbf{Threshold Function Class is Learnable}}) \citep{shalev2014understanding, mohri2018foundations}\\
Let $\cH$ be the set of threshold functions over the \emph{real line}, namely,
\begin{align*}
\cH &= \set{h_{\alpha}: \alpha \in \bR}, \quad\text{where } h_{\alpha}(x) = \mathds{1}_{\set{x<\alpha}}.
\end{align*}
Clearly, $\cH$ is of infinite size.

Nevertheless, the following lemma shows that $\cH$ is \emph{learnable} in \emph{the PAC model} using \emph{the ERM algorithm}.

\begin{lemma}\citep{shalev2014understanding}\\
Let $\cH$ be the class of \textbf{thresholds} as defined earlier. Then, $\cH$ is \textbf{PAC learnable}, using the ERM rule, with \textbf{sample complexity} of
\begin{align*}
m_{\cH}(\epsilon, \delta) \le \ceil{\log\paren{\frac{2/\delta}{\epsilon}}}.
\end{align*}
\end{lemma}

\end{example}
\end{itemize}



\section{PAC Learning in Stochastic Setting}
\begin{itemize}
\item \begin{definition} (\emph{\textbf{Agnostic PAC-Learning}})\\
Let $\cH$ be a hypothesis set. $\cA$ is an \underline{\emph{\textbf{agnostic PAC-learning algorithm}}} if there
exists a \emph{\textbf{polynomial function}} $\text{poly}(\cdot, \cdot, \cdot, \cdot)$  such that for any $\epsilon > 0$ and $\delta > 0$,
for all distributions $\cP$ over $\cX \times \cY$, the following holds for any sample size $m \ge \text{poly}(1/\epsilon, 1/\delta, d, \text{size}(c))$:
\begin{align}
\cP_{\cD_m}\set{L(g_m(\cdot| \cD_{m})) - \inf_{g \in \cH}L(g)  \le \epsilon } \ge 1 - \delta. \label{eqn: agnostic_pac_definition}
\end{align} If $\cA$ further runs in $\text{poly}(1/\epsilon, 1/\delta, d, \text{size}(c))$, then $\cC$ is said to be \emph{\textbf{\underline{efficiently agnostic} \underline{PAC-learnable}}}. 
\end{definition}

\item  \begin{definition} (\emph{\textbf{Bayes Error in Stochastic Scenario}})\\
Under a given distribution $\cP_{X, Y}$,  the \underline{\emph{\textbf{Bayes error}}} $L^{*}$ or \underline{\emph{\textbf{Bayes risk}}} $R^{*}$ is defined as 
\begin{align}
 L^{*} &= \inf_{h \in \cY^{\cX} \text{ measurable}}\set{L(h)}, \label{expr: Bayes_err}
\end{align} where the infimum is with respect to \emph{all measureable function} $g: \cX \rightarrow \cY$. And the \emph{hypothesis} $g^{*}$ such that $L(g^{*})= L^{*}$ is called the \underline{\emph{\textbf{Bayes classifier}}}.
\end{definition}

\item \begin{remark} (\emph{\textbf{Estimation Error vs. Approximation Error}})\\
The difference between \emph{the error of a hypothesis} $g \in \cH$ and \emph{the Bayes error} can be decomposed as:
\begin{align*}
L(g) - L^{*} &=\underbrace{\paren{L(g) - \inf_{g \in \cH}L(g)}}_{\text{\emph{estimation error}}} + \underbrace{\paren{\inf_{g \in \cH}L(g) - L^{*}}}_{\text{\emph{approximation error}}}.
\end{align*} 
where the first term is called \emph{\textbf{estimation error}} and the second term is called \emph{\textbf{approximation error}}.  

When $\cH$ and $\cP_{X, Y}$ is \emph{fixed}, \emph{the approximation error is fixed}. The definition of \emph{\textbf{PAC learnability}} requires that \emph{\textbf{the estimation error}} would be \emph{bounded} \emph{\textbf{uniformly} over \textbf{all distributions}}.
\begin{align*}
L(g_m) - \inf_{g \in \cH}L(g) &= L(g_m) - \widehat{L}(g_m) + \widehat{L}(g_m) - L(g^{*})\\
&\le  L(g_m) - \widehat{L}(g_m) +\widehat{L}(g^{*}) - L(g^{*})\\
&\le  2 \sup_{g\in \cH}\abs{L(g) - \widehat{L}(g)}
\end{align*} where $g^{*} = \argmin_{g\in \cH}L(g)$ and $\widehat{L}(g)$ is the training error of $g$. Thus \emph{the estimation error} can be bounded uniformly by \emph{the generalization error bound} $|L(g) - \widehat{L}(g)|$ for any $g \in \cH$.
\end{remark} 
\end{itemize}

\section{PAC Learning vs. Universal Consistency}
\begin{itemize}
\item \begin{remark} (\emph{\textbf{Universal Consistency is Not Enough}}) \citep{shalev2014understanding}\\
We compare the \emph{universal consistency} and \emph{PAC learnability} as \emph{\textbf{performance guarantee}} for a classification rule:
\begin{enumerate}
\item \emph{\textbf{The universal consistency}} of a classification rule provides \emph{a performance guarantee} in terms of \underline{\emph{\textbf{asymptotic analysis}}}. It concerns that if \emph{the generalization error} of classfication rule $\set{g_n}$ will reach to \emph{infimum} within given class $\cH$ when the sample size is \emph{\textbf{infinity}} $n \to \infty$.  \underline{\emph{\textbf{The universal consistency}} is a \emph{meaurable of \textbf{correctness}}}, i.e. the classification rule can reach to \emph{correct} solution \emph{eventually} regardless of the underlying distribution of data.

On the other hand, it does not anwser \emph{how many examples are required to be as good as the best hypothesis in $\cH$}. The answer to this question depends on the underlying distribution $\cP_{X,Y}$ in consistency statement.  %whether or not an algorithm can reach to that optimality  within \emph{polynomial time} given sample size $n$. 
\emph{The consistency statement} is more related to \emph{\textbf{large sample statistical behavior}}.

\item \emph{\textbf{The PAC learnability}} of a classification rule provides \emph{a performance guarantee} in terms of \underline{\emph{\textbf{non-asymptotic analysis}}}. It provide \underline{a \emph{measure of \textbf{efficiency}}}. \emph{An \textbf{algorithm} is efficient} if it obtain a solution \emph{close} to \emph{correct} solution (\emph{within an error rate $\epsilon$}) \emph{with high probability} $(1 - \delta)$ \emph{\textbf{given a finite set  of $n \ge n(\epsilon, \delta)$ samples}}. 

\emph{The PAC learnability} is a measure for the \emph{\textbf{algorithm}} as well as \emph{\textbf{the statistical nature}} of the problem. Its formulation is closer to \emph{computer science} than to \emph{statistics}. 

Moreover, \emph{the definition of PAC learning} yields \emph{\textbf{the limitation} of learning (via the \textbf{No-Free-Lunch theorem})} and \emph{the \textbf{necessity} of \textbf{prior knowledge}}. It gives us a crisp way to encode prior knowledge by choosing \emph{a hypothesis class}, and once this choice is made, we have a generic learning rule - \emph{ERM}. Compared to PAC learning, the definition of \emph{\textbf{consistency}} \emph{does \textbf{not} yield} a natural learning paradigm or a way to \emph{\textbf{encode prior knowledge}}. In fact, in many cases there is \emph{no need for prior knowledge at all} since the consistency only cares about the \emph{asymptotic behavior}.
\end{enumerate}


Consider the classification prediction algorithm \emph{\textbf{Memorize}} defined as follows. The algorithm \emph{memorizes} the training examples, and, given a test point $x$, it predicts the majority label among all labeled instances of $x$ that exist in the training sample (and some fixed default label if no instance of $x$ appears in the training set). It is possible to show (see Exercise below) that the \emph{\textbf{Memorize}} algorithm is \emph{\textbf{universally consistent}} for \emph{\textbf{every countable domain}} $\cX$ and a \emph{\textbf{finite label}} set $\cY$ (w.r.t. the zero-one loss).

Intuitively, it is \emph{not obvious} that the \emph{Memorize} algorithm should be viewed as a \emph{learner}, since it \emph{\textbf{lacks}} the aspect of \emph{\textbf{generalization}}, namely, of using observed data to \emph{predict} the labels of unseen examples. The fact that \emph{Memorize} is a \emph{consistent algorithm} for the class of \emph{all functions over any countable domain set} therefore raises \emph{doubt} about \emph{\textbf{the usefulness of consistency guarantees}}. Furthermore, the sharp-eyed reader may notice that the ``\emph{bad learner}" we introduced in Chapter 2, which led to \emph{overfitting}, is in fact the \emph{Memorize} algorithm.
\end{remark}

\item \begin{exercise} (\textbf{Memorize Algorithm})\\
In this example we wish to show that the algorithm \textbf{Memorize} is a \textbf{consistent learner} for \textbf{every class of (binary-valued) functions} over \textbf{any countable domain}. 

Let $\cX$ be a \textbf{countable} domain and let $\cP$ be a probability distribution over $\cX$.
\begin{enumerate}
\item Let $\set{x_i : i \in \bN}$ be an enumeration of the elements of $\cX$ so that for all $i \le j$, $\cP(\set{x_i}) \le \cP({x_j})$. Prove that
\begin{align*}
\lim\limits_{n\rightarrow \infty}\sum_{i \ge n}\cP(\set{x_i}) = 0.
\end{align*} Note that $\sum_{i = 0}^{\infty}\cP(\set{x_i}) = 1$ by definition of probability measure.

\item Given any $\epsilon > 0$, prove that there exists $\epsilon_{\cP} > 0$ such that
\begin{align*}
\cP\set{x \in \cX: \cP(\set{x}) < \epsilon_{\cP}} < \epsilon.
\end{align*}

\item Prove that for every $\eta > 0$, if $n$ is such that $\cP(\set{x_i}) < \eta$ for all $i > n$, then for every $m \in \bN$, let $\cD_m$ be the sample of  size $m$ generated according to $\cP$
\begin{align*}
\cP_{\cD_{m}}\set{\exists x_i: \cP(\set{x_i}) > \eta \text{ and }x_i \not\in \cD_{m} } &\le n e^{- \eta m}.
\end{align*}

\item Conclude that if $\cX$ is \textbf{countable} then for \textbf{every probability distribution} $\cP$ over $\cX$ there exists a function $m_{\cP}: (0,1) \times (0,1) \to \bN$ such that for every $\epsilon, \delta > 0$ if $m > m_{\cP}(\epsilon, \delta)$ then
\begin{align*}
\cP_{\cD_{m}}\set{\cP(\set{x: x\not\in \cD_{m}}) > \epsilon } < \delta.
\end{align*}

\item Prove that \textbf{Memorize} is a consistent learner for every class of (binary-valued) functions over any countable domain.
\end{enumerate}
\end{exercise}

\item \begin{remark} (\emph{\textbf{Universal Consistency May Not be Good Preference}}) \citep{shalev2014understanding}\\
One may argue that even though \emph{consistency is a weak requirement}, it is desirable that \emph{a learning algorithm will be \textbf{consistent} with respect to the set of \textbf{all functions} from $\cX$ to $\cY$}, which gives us a \emph{guarantee} that for enough training examples, we will always be as good as \emph{the Bayes optimal predictor}. Therefore, if we have two algorithms, where one is \emph{consistent} and the other one is \emph{not consistent}, we should \emph{prefer} the consistent algorithm. \emph{\textbf{However}}, \emph{this argument is \textbf{problematic}} for two reasons.
\begin{enumerate}
\item First, maybe it is the case that for most ``\emph{natural}" distributions we will observe in practice that \emph{\textbf{the sample complexity} of the \textbf{consistent} algorithm} will be \emph{so \textbf{large}} so that in every practical situation we will \emph{not obtain enough examples} to enjoy this guarantee. 

\item Second, it is \emph{\textbf{not very hard to make any PAC} or \textbf{nonuniform learner} \textbf{consistent} with respect to the class of all functions from $\cX$ to $\cY$}. 

Concretely, consider a countable domain, $\cX$ , a finite label set $\cY$, and a hypothesis class, $\cH$, of functions from $\cX$ to $\cY$. We can make any nonuniform learner for H be consistent with respect to the class of all classifiers from $\cX$ to $\cY$ using the following simple trick: Upon receiving a training set, we will first run the nonuniform learner over the training set, and then we will obtain a \emph{bound} on the true risk of the learned predictor. If this bound is \emph{small enough} we are done. Otherwise, we revert to the \emph{Memorize} algorithm. This simple modification makes the algorithm consistent with respect to all functions from $\cX$ to $\cY$. 

Since \emph{\textbf{it is easy to make any algorithm consistent}}, it may \emph{\textbf{not be wise}} to \emph{\textbf{prefer}} one algorithm over the other just because of consistency considerations.
\end{enumerate}
\end{remark}
\end{itemize}
\newpage
\bibliographystyle{plainnat}
\bibliography{reference.bib}
\end{document}