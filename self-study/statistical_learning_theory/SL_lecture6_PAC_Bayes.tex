\documentclass[11pt]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent %\usepackage{graphicx}
\usepackage{amsmath,amssymb, mathrsfs, dsfont}
\usepackage{tabularx}
\usepackage[all,cmtip]{xy}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{xcolor}
%\usepackage[linkbordercolor ={1 1 1} ]{hyperref}
%\usepackage[sf]{titlesec}
\usepackage{natbib}
%\usepackage{tikz-cd}

\usepackage{../../Tianpei_Report}

%\usepackage{appendix}
%\usepackage{algorithm}
%\usepackage{algorithmic}

%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}



\begin{document}
\title{Lecture 6: PAC Bayesian Theory}
\author{ Tianpei Xie}
\date{Feb. 10th., 2023}
\maketitle
\tableofcontents
\newpage
\section{Bayesian Learning}
\subsection{Bayesian Predictor}
\begin{itemize}
\item \begin{remark} (\emph{\textbf{Data}})\\
Define an \emph{\textbf{observation}} as a $d$-dimensional vector $x$. The \emph{unknown} nature of the observation is called a \emph{\textbf{class}}, denoted as $y$. The domain of observation is called an \emph{\textbf{input space} or \textbf{feature space}}, denoted as $\cX\subset \bR^{d}$, whereas the domain of class is called the \emph{\textbf{target space}}, denoted as $\cY$. For \emph{\textbf{classification task}}, $\cY= \set{1,\ldots, M}$; and for \emph{\textbf{regression task}}, $\cY = \bR$. 
Denote a collection of $n$ \emph{\textbf{samples}} as 
\begin{align*}
\cD \equiv \cD_n = \paren{(X_1, Y_1) \xdotx{,} (X_n, Y_n)}.
\end{align*} Note that $\cD_n$ is a finite \emph{\textbf{sub-sequence}} in $(\cX \times \cY)^n$.
\end{remark}


\item \begin{definition} (\emph{\textbf{Concept Class as a Function Class}})\\
 A \underline{\emph{\textbf{concept}}} $c: \cX \rightarrow \cY$ is the \emph{input-output association} from the nature and is \emph{to be learned} by \emph{\textbf{a learning algorithm}}.  Denote $\cC$ as \emph{the set of all concepts} we wish to learn as the \underline{\emph{\textbf{concept class}}}. That is, $\cC \subseteq \set{c: \cX \rightarrow \cY } =\cY^{\cX}.$ Concept class $\cC$ is a \emph{function class}. 
\end{definition}

\item \begin{definition} (\emph{\textbf{Hypothesis and Hypothesis Class}})\\
The learner is requested to output a \emph{prediction rule}, $h : \cX \to \cY$. This function is also called a \emph{\textbf{predictor}}, a \emph{\textbf{hypothesis}}, or a \emph{\textbf{classifier}}. The predictor can be used to predict the label of new domain points. 

Note that $\cH$ and $\cC$ may not overlap, since the concept class is unknown to learner. 
\end{definition}

\item  \begin{definition} (\emph{\textbf{Bayesian Hypothesis}})\\
Assume instead that the hypothesis $h$ is \emph{random}. That is, let $(\cH, \srH, \bP)$ be a probability space with probability measure $\bP$. We refer $\bP$ as \emph{the \textbf{\underline{prior distribution} of hypothesis}} $h \in \cH$. The corresponding \emph{\textbf{randomized hypothesis}} $h$ is called \underline{\emph{\textbf{Bayesian hypothesis}}}.
\end{definition}



\item \begin{definition}(\textbf{\emph{Bayesian Learning and Generalization Error}})\\
Following the Bayesian reasoning approach, \emph{the output of the learning algorithm} is \emph{not necessarily a single hypothesis}. Instead, the learning
process defines \emph{a \textbf{\underline{posterior probability}} over $\cH$}, which we denote by $\bQ$. Note that the posterior distribution is absolutely continuous with respect to prior $\bP$, i.e. $\bQ \ll \bP$. 

In the context of a \emph{supervised learning problem}, where $\cH$ contains functions from $\cX$ to $\cY$, one can think of $\bQ$ as defining a randomized prediction rule as follows.  Whenever we get a new instance $x$, we \emph{\textbf{randomly}} pick a hypothesis $h \in \cH$ according to $\bQ$ and predict $h(x)$. We define \emph{the \textbf{loss} of} $\bQ$ on an example $z$ to be
\begin{align}
L(\bQ, z) := \E{h \sim \bQ}{\ell(h, z)} \label{def: bayesian_hypothesis_loss}
\end{align} where $\ell: \cH \times \cZ \to \bR_{+}$ is a loss function. \underline{\emph{\textbf{The generalization loss}}} and \underline{\emph{\textbf{training loss of $\bQ$}}} can be written as
\begin{align}
L_{\cP}(\bQ)& := \E{h \sim \bQ}{L_{\cP}(h)} =  \E{h \sim \bQ}{\E{Z \sim \cP}{\ell(h, Z)}}  = \E{Z \sim \cP}{L(\bQ, Z)}\label{def: bayesian_hypothesis_generalization_error}\\
L_{\cD}(\bQ)& :=\E{h \sim \bQ}{L_{\cD}(h)} =  \E{h \sim \bQ}{\frac{1}{m}\sum_{i=1}^{m}\ell(h, Z_i)} =  \frac{1}{m}\sum_{i=1}^{m}L(\bQ, Z_i) \label{def: bayesian_hypothesis_training_error}
\end{align}
\end{definition}

\item \begin{remark}
In Bayesian statistics, \emph{\textbf{posterior distribution}} can be formulated given the prior distribution $\bP(h)$ and likelihood function $\cL(\cD_m | h)$ as
\begin{align}
\bQ(h) := \bP(h | \cD_m) \propto \cL(\cD_m | h) \times \bP(h) \label{eqn: posterior_dist}
\end{align} Several inference techniques could then be derived from the posterior. For instance, \emph{the \textbf{mean} of the posterior}
\begin{align*}
\widehat{h}_{mean} := \int_{\cH} h  \bQ(dh),
\end{align*} \emph{\textbf{the maximum a posteriori (MAP)}}
\begin{align*}
\widehat{h}_{map} \in \arg\max_{h\in \cH}\bQ(h)
\end{align*} and a single draw
\begin{align*}
\widehat{h}_{draw} \sim \bQ(h)
\end{align*} are all popular choices.
\end{remark}
\end{itemize}
\subsection{Generalized Bayesian Learning}
\begin{itemize}
\item \begin{remark}(\textbf{\emph{Tempered Posterior}}) \citep{guedj2019primer}\\
\emph{\textbf{A first strategy}} consists in modulating \emph{the influence of the likelihood term}, by considering a \emph{\underline{\textbf{tempered}} version} of it: from \eqref{eqn: posterior_dist}, the posterior now becomes \emph{\underline{\textbf{the tempered posterior $\bQ_{\lambda}$}}}:
\begin{align}
\bQ_{\lambda}(h) := \bP_{\lambda}(h | \cD_m) \propto \cL(\cD_m | h)^{\lambda} \times \bP(h) \label{eqn: tempered_posterior_dist}
\end{align} where $\lambda \ge 0$. The former Bayesian model is now a particular case ($\lambda = 1$) of a continuum of distributions. Different values for $\lambda$ will achieve different \emph{\textbf{tradeoffs}} between the \emph{\textbf{prior}} $\bP$ and the \underline{\emph{\textbf{tempered likelihood}} $\cL^{\lambda}$}. Let us stress here that $\cL_{\lambda}$ may no longer explicitly refer to a canonical probabilistic model.

This notion of tempered likelihood has been investigated, among others, by a striking series of paper (Grünwald, 2011, 2012, 2018; Grünwald and Van Ommen, 2017)
which develop a ``\emph{\textbf{safe Bayesian}}" framework. These papers prove that \emph{the tempered posterior} \emph{\textbf{concentrates}} to the best \emph{approximation of the truth} in the set of predictors $\cH$, while this might not be the case for the non-tempered posterior: as such, tempering provides \emph{\textbf{robustness guarantees}} when the chosen predictor, while being wrong, still captures some aspects of the truth.
\end{remark}

\item \begin{remark}(\textbf{\emph{Generalized Posterior}}) \citep{guedj2019primer}\\
\emph{\textbf{A second strategy}} within generalised Bayes is an information-theoretic framework (see Csiszár and Shields, 2004, for an introduction) in which the ``likelihood" of a predictor $h$ is no longer assessed by the probability mass from some specified model, but rather by \emph{\textbf{the loss}} encountered when
predicting $h(X)$ instead of $Y$, the actual output value we wish to predict.

In other words, the posterior from \eqref{eqn: posterior_dist} or the tempered posterior from \eqref{eqn: tempered_posterior_dist} are replaced with \underline{\emph{\textbf{the generalised posterior}}}
\begin{align}
\bQ_{\lambda}(h) := \bP_{\lambda}(h | \cD_m) \propto \ell_{\lambda, m}(h)  \times \bP(h) \label{eqn: generalized_posterior_dist}
\end{align} where $\lambda \ge 0$ and $\ell_{\lambda, m}(h)$ is a \emph{\textbf{loss term} measuring the quality of the predictor $h$ on the collected
data $\cD_{m}$} (the training data, on which $h$ is built upon). To set ideas, one could think of $\ell_{\lambda, m}(\cdot)$ as a \emph{\textbf{functional} of the empirical risk $L_{\cD}(h)$.}
\begin{align*}
\ell_{\lambda, m}(h) = F_{\lambda}(L_{\cD_{m}}(h)).
\end{align*}  
As \emph{the loss term is merely an \textbf{instrument}} to guide oneself towards better performing algorithms but is \emph{\textbf{no longer explicitly motivated by statistical modelling}}, the \underline{\emph{\textbf{generalised}}} \underline{\emph{\textbf{Bayesian framework}}} may be described as \emph{model-free}, as no such assumption is required. Other terms appear in the statistical and machine learning literature, with  occurrences of ``\emph{generalised posterior}", ``\emph{pseudo-posterior}" or ``\emph{quasi-posterior}" succeeding one another. Similarly, the terms ``\emph{prior}" and ``\emph{posterior}" have been consistently used as they ``\emph{surcharge}" the existing terms in \emph{Bayesian statistics}, however the distributions in \eqref{eqn: generalized_posterior_dist} are now \textbf{\emph{different objects}}.

Consider for example the \emph{\textbf{prior}} $\bP$: rather than \emph{incorporating prior knowledge} (which might not be available), $\bP$ serves as a way to \emph{\textbf{structure the set of predictors} $\cH$}, by putting more mass towards predictors enjoying any other \emph{desirable propert}y (suggested by the context, CPU / storage resources, etc.) such as \emph{sparsity}.
\end{remark}
\end{itemize}
\subsection{Gibbs Posterior}
\begin{itemize}
\item 
Among all possible loss functions $\ell_{\lambda, m}(\cdot)$, a most typical choice is the so-called \emph{Gibbs posterior (or measure)}:
\begin{definition}\emph{\textbf{(Gibbs posterior (or measure))}}
\begin{align}
\bQ_{\lambda}(h) :=\bP_{\lambda}(h | \cD_m) \propto e^{-\lambda L_{\cD_{m}}(h)}  \times \bP(h) \label{eqn: gibbs_posterior_dist}
\end{align} In Gibbs posterior, the \emph{loss} term \emph{\textbf{exponentially penalises} the performance of a predictor} $h$ on the training data, and the parameter $\lambda \ge 0$ (often referred to as an \emph{\textbf{inverse temperature}}, by analogy with \emph{the Boltzmann distribution} in \emph{statistical mechanics}) controls the \emph{\textbf{tradeoff}} between \emph{the prior term} and \emph{the loss term}.
\end{definition}

\item \begin{remark}
Let us examine both extremes cases:
\begin{enumerate}
\item  when $\lambda = 0$, the loss term \emph{vanishes} and \emph{the generalised posterior amounts to the prior}: the
predictor is blind to data. 
\item When $\lambda \to \infty$, \emph{\textbf{the influence of data} becomes \textbf{overwhelming}} and the probability mass accumulates around the predictor  which achieves \emph{the best empirical error}, i.e., \emph{the generalised Bayesian predictor} reduces to \emph{the celebrated \textbf{empirical risk minimiser (ERM)}}.
\end{enumerate}
\end{remark}

\item \begin{remark}(\emph{\textbf{Gibbs Measure} as Solution of \textbf{Maximum Entropy Optimization}})\\
Let $(\cH, \srH)$ denote a \emph{measurable space} and consider $\mu, \nu$ two probability measures on $(\cH, \srH)$. We note $\bQ \ll \bP$ when $\bQ$ is absolutely continuous with respect to $\bP$, and we let $\cM_{\bP}(\cH, \srH)$ denote the space of probability measures on $(\cH, \srH)$ which are absolutely continuous with respect to $\bP$:
\begin{align*}
\cM_{\bP}(\cH, \srH)= \set{\bQ: \bQ \ll \bP}.
\end{align*} \emph{The Kullback-Leibler divergence} is defined as
\begin{align*}
\kl{\bQ}{\bP} &= \left\{\begin{array}{cc}
\int_{\cH}\log\paren{\frac{d\bQ}{d\bP}}d\bQ & \text{ when }\bQ \ll \bP\\
\infty &\text{o.w.}
\end{array}
\right.
\end{align*} Let us consider the maximum entropy optimization problem
\begin{align}
\inf_{\bQ \in \cM_{\bP}(\cH, \srH)} & \frac{1}{\lambda}\kl{\bQ}{\bP} + \int_{\cH}L_{\cD_{m}}(h) \bQ(dh)  \label{eqn: max_ent}
\end{align} This problem amounts to minimising the integrated (with respect to any measure $\bQ$) \emph{empirical risk} plus a \emph{divergence term} between the \emph{generalised posterior} and the \emph{prior}. In other words, minimising a criterion of performance plus a divergence from the initial distribution, which is the analogous of \emph{penalised regression} (such as Lasso).

The optimization problem has an \emph{\textbf{unique solution}} if $\cM_{\bP}(\cH, \srH)$ is non-empty. The solution is attained when $\bQ$ is a \emph{\textbf{Gibbs measure}}:
\begin{align}
\frac{d\bQ}{d\bP}(h) &= \frac{1}{Z_{\lambda, m}}\exp\paren{- \lambda L_{\cD_{m}}(h)} \label{eqn: gibbs_max_ent}
\end{align} where 
\begin{align*}
Z_{\lambda, m} = \int_{\cH}\exp\paren{- \lambda L_{\cD_{m}}(h)} d\bP(h)
\end{align*} And the optimial value 
\begin{align}
\inf_{\bQ \in \cM_{\bP}(\cH, \srH)}\set{\frac{1}{\lambda}\kl{\bQ}{\bP} + \int_{\cH}L_{\cD_{m}}(h) \bQ(dh) } = -\frac{1}{\lambda}\log \int_{\cH}\exp\paren{- \lambda L_{\cD_{m}}(h)} d\bP(h). \label{eqn: gibbs_max_ent_opt_value}
\end{align}
\end{remark}
\end{itemize}

\section{PAC Bayesian Theory}
\subsection{PAC Bayesian Inequalities}
\begin{itemize}
\item \begin{theorem} (\textbf{Catoni's PAC Bayesian Inequality})\citep{catoni2003pac, alquier2021user}\\
Let $\cP$ be an arbitrary distribution over an example domain $\cZ$. Let $\cH$ be a hypothesis class and let $\ell: \cH \times \cZ \to [0,1]$ be a loss function. Let $\bP$ be a prior distribution over $\cH$ and let $\delta \in (0,1)$. Then, with probability of at least $1 - \delta$ over
the choice of an i.i.d. training set $\cD = \set{z_1 \xdotx{,} z_m}$ sampled according to $\cP$, for all distributions $\bQ$ over $\cH$ (even such that depend on $\cD$) and for all $\lambda >0$, we have
\begin{align}
L_{\cP}(\bQ) \le L_{\cD}(\bQ) + \frac{\kl{\bQ}{\bP}  + \log(1/\delta) }{\lambda} +  \frac{\lambda }{8m}  \label{ineqn: catoni_pac_bayes_kl}
\end{align} where $\kl{\bQ}{\bP} = \E{\bQ}{\log \paren{\bQ / \bP}}$ is the Kullback-Leibler divergence.
\end{theorem}
\begin{proof}
\begin{enumerate}
\item Recall \textit{\textbf{the duality formulation}} of \emph{logarithmic moment generating function} for random variable $M$:
\begin{align*}
\log \E{\bP}{e^{\lambda M}} &= \sup_{\bQ \ll \bP}\set{ \lambda \E{\bQ}{M} - \kl{\bQ}{\bP} }
\end{align*} Let $M := \Delta(h)$ where $\Delta(h) := \paren{L_{\cP}(h) - L_{\cD}(h)}$. For all $\bQ \ll \bP$, we have
\begin{align}
\log \E{\bP}{e^{\lambda \Delta(h)}} &\ge \set{ \lambda \E{\bQ}{\Delta(h)} - \kl{\bQ}{\bP} }. \label{pf: catoni_pac_bayes_kl_1}
\end{align} It follows that $\Delta(h) := \paren{L_{\cP}(h) - L_{\cD}(h)} \equiv \Delta(h, \cD)$. Taking exponential and expectation with respect to sample $\cD$ on both sides of inequality yields
\begin{align}
\E{\cD}{e^{\sup_{\bQ \ll \bP}\set{\lambda\E{\bQ}{\Delta(h)} - \kl{\bQ}{\bP} }}} &\le \E{\cD}{\E{\bP}{e^{\lambda \Delta(h)}}} \label{pf: catoni_pac_bayes_kl_2}
\end{align}
The advantage of the expression on the right-hand side stems from the fact that we can switch the order of expectations (because $\bP$ is a prior that \emph{\textbf{does not depend on sample}} $\cD$), which yields
\begin{align}
\E{\cD}{e^{\sup_{\bQ \ll \bP}\set{\lambda\E{\bQ}{\Delta(h)} - \kl{\bQ}{\bP} }}} &\le \E{\bP}{\E{\cD}{e^{\lambda \Delta(h)}}} \label{pf: catoni_pac_bayes_kl_3}
\end{align}

\item Next, \emph{\textbf{for any hypothesis}} $h\in \cH$, we bound the expecation term $\E{\cD}{e^{\lambda \Delta(h)}}$. Since $L_{\cD}(h) = \frac{1}{m}\sum_{i=1}^{m}\ell(h, z_i) \in [0,1], a.s.$,  from \emph{Hoeffding's lemma}  
\begin{align}
\E{\cD}{e^{\lambda m \Delta(h)}} &\le \exp\paren{\frac{ m \lambda^2 }{8}} \nonumber\\
\Rightarrow \E{\cD}{e^{\lambda  \Delta(h)}} &\le \exp\paren{\frac{\lambda^2}{8m}} \label{pf: catoni_pac_bayes_kl_4}
\end{align} Combining \eqref{pf: catoni_pac_bayes_kl_4} with Equation \eqref{pf: catoni_pac_bayes_kl_3}, we have
\begin{align}
\E{\cD}{e^{\sup_{\bQ \ll \bP}\set{\lambda\E{\bQ}{\Delta(h)} - \kl{\bQ}{\bP}  }}}  \le  \exp\paren{\frac{\lambda^2 }{8m}} \nonumber\\
\Rightarrow \E{\cD}{\exp\paren{\sup_{\bQ \ll \bP}\set{\lambda\E{\bQ}{\Delta(h)} - \kl{\bQ}{\bP} - \frac{\lambda^2 }{8m}  }}} \le 1  \label{pf: catoni_pac_bayes_kl_5}
\end{align}

\item  Finally, we  obtain the result by applying \emph{Chernoff's method}. Specifically, by \emph{Markov's inequality}, 
\begin{align}
&\cP_{\cD}\set{\sup_{\bQ \ll \bP}\set{\lambda  \E{\bQ}{\Delta(h)}  - \kl{\bQ}{\bP} - \frac{\lambda^2 }{8m}  } \ge \epsilon} \nonumber\\
&\le e^{-\epsilon}\E{\cD}{\exp\paren{\sup_{\bQ \ll \bP}\set{\lambda\E{\bQ}{\Delta(h)} - \kl{\bQ}{\bP} - \frac{\lambda^2 }{8m}  }}} \nonumber\\
&\le e^{-\epsilon}  \label{pf: catoni_pac_bayes_kl_6}
\end{align} Denote the right-hand side of the above $\delta$, thus $\epsilon = \log(1/\delta)$. After rearranging the term, we therefore obtain that with probability of at least $1 - \delta$ we have that for all $\bQ \ll \bP$, and for all $\lambda$
\begin{align*}
 \E{\bQ}{\Delta(h)} \le \frac{\kl{\bQ}{\bP}  + \log(1/\delta) }{\lambda} +  \frac{\lambda }{8m}. \qed
\end{align*} 
\end{enumerate}
\end{proof}

\item \begin{remark} (\textbf{\emph{Catoni's Oracle PAC Bayesian Inequality}})\citep{catoni2003pac, alquier2021user}\\
\emph{An \textbf{oracle inequality}}  can be reformulated in similar form as \eqref{ineqn: catoni_pac_bayes_kl}:
\begin{align}
\cP_{\cD}\brac{ \E{h \sim \bP_{\lambda}(h|\cD)}{L_{\cP}(h)}  \le \inf_{\bQ \in \cM_{\bP}(\cH, \srH)}\set{\E{h \sim \bQ}{L_{\cP}(h)} + \frac{\kl{\bQ}{\bP}  + \log(1/\delta) }{\lambda} +  \frac{\lambda }{8m} }} \ge 1 - \delta. \label{ineqn: catoni_pac_bayes_kl_version_2}\\
\Rightarrow \cP_{\cD}\brac{L_{\cP}(\bP_{\lambda}(h|\cD)) \le \inf_{\bQ \in \cM_{\bP}(\cH, \srH)}\set{L_{\cP}(\bQ) + \sqrt{\frac{\kl{\bQ}{\bP}  + \log(1/\delta) }{2m}} }} \ge 1 - \delta. \nonumber
\end{align}
\end{remark}

\item The first PAC-Bayesian Inequality is from McAllester \citep{mcallester2003pac}.
\begin{theorem} (\textbf{McAllester's PAC Bayesian Inequality})\citep{mcallester2003pac, shalev2014understanding, rasmussen2006gaussian, alquier2021user}\\
Under the same condition as in \eqref{ineqn: catoni_pac_bayes_kl}, then, with probability of at least $1 - \delta$, for all distributions $\bQ \ll \bP$ over $\cH$, we have
\begin{align}
\E{h \sim \bQ}{L_{\cP}(h)} &\le \E{h \sim \bQ}{L_{\cD}(h)} + \sqrt{\frac{ \kl{\bQ}{\bP}  + \log(1/\delta) + \log(m) + 2}{2m - 1}} \label{ineqn: mcallester_pac_bayes_kl}
\end{align} 
\end{theorem}
\begin{proof}
The proof is similar to above. In this time, we want to show that 
\begin{align}
\E{\cD}{e^{(2m-1) \Delta(h)^2}} &\le 4m, \label{pf: mcallester_pac_bayes_kl_1}
\end{align} where $\Delta(h) := \abs{L_{\cP}(h) - L_{\cD}(h)}$. Since the loss function is bounded within $[0,1]$ almost surely, by \emph{Hoedffing's inequality}
\begin{align*}
\cP_{\cD}\set{\Delta(h) \ge x} &\le 2\exp\paren{-2m x^2}.
\end{align*} Note that $\cP_{\cD}\set{\Delta \ge x} = \int_{x}^{\infty}f(\Delta) d\Delta$ where $f(\Delta) \equiv \frac{d\cP_{\cD}(\Delta)}{d\Delta}$ is the density function. Since the tail is dominated by Gaussian tail, the density function is also dominated by Gaussian density
\begin{align*}
 \int_{x}^{\infty}f(\Delta)  d\Delta &\le 2e^{-2m x^2}\\
 \Rightarrow f(\Delta) &\le 8m \Delta e^{-2m \Delta^2}.
\end{align*} Therefore, the expectation
\begin{align*}
\E{\cD}{e^{(2m-1) \Delta(h)^2}} &= \int_{0}^{\infty}e^{(2m-1) \Delta^2} f(\Delta) d\Delta\\
&\le  \int_{0}^{\infty}e^{(2m-1) \Delta^2} 8m \Delta e^{-2m \Delta^2}d\Delta \\
&= 8m  \int_{0}^{\infty}e^{- \Delta^2}  \Delta d\Delta\\
&= 4m.
\end{align*} With inequality \eqref{pf: mcallester_pac_bayes_kl_1}, we use the dual formulation of log-MGF, 
\begin{align*}
\log \E{\bP}{e^{\lambda M}} &= \sup_{\bQ \ll \bP}\set{\E{\bQ}{ \lambda M} - \kl{\bQ}{\bP} }
\end{align*} and let $M:= \Delta^2$ and $\lambda := (2m-1)$, so that  we have
\begin{align}
\E{\cD}{e^{ \sup_{\bQ \ll \bP}\set{\E{\bQ}{ (2m-1) \Delta^2} - \kl{\bQ}{\bP}}}}  &\le \E{\cD}{\E{\bP}{e^{(2m-1) \Delta(h)^2}}} \nonumber \\
&\le \E{\bP}{\E{\cD}{e^{(2m-1) \Delta(h)^2}}} \nonumber\\
&\le 4m  \quad \text{(by bound \eqref{pf: mcallester_pac_bayes_kl_1})}.  \label{pf: mcallester_pac_bayes_kl_2}
\end{align} By Markov's inequality
\begin{align*}
&\cP\set{\sup_{\bQ \ll \bP}\set{\E{\bQ}{ (2m-1) \Delta^2} - \kl{\bQ}{\bP}} \ge \epsilon} \\
&\le e^{-\epsilon} \E{\cD}{e^{ \sup_{\bQ \ll \bP}\set{\E{\bQ}{ (2m-1) \Delta^2} - \kl{\bQ}{\bP}}}}\\
&\le 4m e^{-\epsilon}.
\end{align*} Denote the RHS as $\delta$, so $\epsilon = \log(4m/\delta)$. We have with probability as least $1-\delta$, for all $\bQ \ll \bP$,
\begin{align*}
(2m-1) \E{\bQ}{\Delta^2} - \kl{\bQ}{\bP} \le \log\frac{4m}{\delta}\\
\Rightarrow \paren{\E{\bQ}{\Delta}}^2 \le \E{\bQ}{\Delta^2} \le \frac{\kl{\bQ}{\bP} + \log(1/\delta) + \log(m) + 2}{2m-1}
\end{align*} The leftmost inequality is due to Jenson's inequality on $\phi(x) := x^2$. We have proved the result. \qed
\end{proof}

\item \begin{remark}
An alternative way to prove inequality \eqref{pf: mcallester_pac_bayes_kl_1} is by Hoeffding's lemma \eqref{pf: catoni_pac_bayes_kl_4}
\begin{align*}
\E{\cD}{e^{\lambda  \Delta(h)}} &\le \exp\paren{\frac{\lambda^2}{8m}}
\end{align*} Then multiplying both sides by $\exp\paren{-\frac{\lambda^2}{8m s}}$ where $s \in (0,1)$
\begin{align*}
\E{\cD}{e^{\lambda  \Delta(h) -\frac{\lambda^2}{8m s} }} &\le \exp\paren{\frac{\lambda^2(s-1)}{8ms}}, \forall \lambda.
\end{align*} This inequality holds for all $\lambda$. After integrating with respect to $\lambda$ and using Fubini's theorem, we have the LHS
\begin{align*}
\int_{-\infty}^{\infty}\exp\paren{\frac{\lambda^2(s-1)}{8ms}}d\lambda = \sqrt{\frac{8ms \pi}{1-s}}.
\end{align*} And the RHS, for each $x:=\Delta(h)$
\begin{align*}
\int_{-\infty}^{\infty}\exp\paren{\lambda x -\frac{\lambda^2}{8m s} }d\lambda = \sqrt{8ms \pi}\exp\paren{2m s x^2}
\end{align*} Taking expectation with respect to $X := \Delta(h)$,
\begin{align}
\int_{-\infty}^{\infty}\E{\cD}{e^{\lambda  \Delta(h) -\frac{\lambda^2}{8m s} }} d\lambda &= \sqrt{8ms \pi}\E{\cD}{\exp\paren{2m s\Delta^2}} \le \sqrt{\frac{8ms \pi}{1-s}} \\
\Rightarrow \E{\cD}{\exp\paren{2m s\Delta^2}} &\le \frac{1}{\sqrt{1 - s}} \label{ineqn: sub_gaussian_pac_bayes}
\end{align} Let $s = \frac{2m - 1}{2m}= 1 - \frac{1}{2m}$. We have
\begin{align*}
\E{\cD}{e^{(2m -1)\Delta^2}} \le \frac{1}{\sqrt{1 - s}} = \sqrt{2m} \le 4m. \qed
\end{align*} Note that \eqref{ineqn: sub_gaussian_pac_bayes} holds \emph{\textbf{for all sub-Gaussian loss}}.
\end{remark}

\item \begin{remark}
Note that this bound  \eqref{ineqn: mcallester_pac_bayes_kl} cannot be obtained from \eqref{ineqn: catoni_pac_bayes_kl} by minimizing $\lambda$ since the optimal $\lambda^{*}= \sqrt{(\kl{\bQ}{\bP}  + \log(1/\delta)) 8m}$ depends on $\bQ$, which is not allowed. 

A natural idea is to propose \emph{a finite grid} $\Lambda \subset (0, +\infty)$ and to minimize over this grid, which can be justified by a union bound argument. This way we pay the rise for an additional term $\log(m)$ in the boun, i.e. $\sqrt{\frac{ \kl{\bQ}{\bP}  + \log(1/\delta)}{2m }}  \to \sqrt{\frac{ \kl{\bQ}{\bP}  + \log(1/\delta) + \log(m)}{2m - 1}}$ .
\end{remark}


\item \begin{remark}(\textbf{\emph{Generalization Error Bound of Posterior by KL Divergence}})\\
\emph{The McAllester's PAC Bayesian theorem} tells us that \emph{the difference} between \emph{the generalization loss} and \emph{the empirical loss} of a \emph{\textbf{posterior}} $\bQ$ is bounded by an expression that depends on \emph{\textbf{the Kullback-Leibler divergence}} between $\bQ$ and the prior distribution $\bP$.
\end{remark}

\item \begin{remark}(\textbf{\emph{Agnostic PAC Bound vs. PAC Bayesian Bound}})\\
We can compare the PAC bound and PAC-Bayeisan bound. With probability at least $1-\delta$,
\begin{align*}
(\text{Agnostic PAC Bound}) && L_{\cP}(h) &\le L_{\cD}(h) + \sqrt{\frac{\log\abs{\cH} + \log(1/\delta)}{2m}} \\
(\text{PAC-Bayesian Bound}) &&\E{h \sim \bQ}{L_{\cP}(h)} &\le \E{h \sim \bQ}{L_{\cD}(h)} + \sqrt{\frac{ \kl{\bQ}{\bP}  + \log(m/\delta)}{2m -1}}
\end{align*}
\end{remark}


\item \begin{remark}(\textbf{\emph{Bayesian Learning as Minimum Description Length}})\\
As in \underline{\emph{\textbf{the MDL paradigm}}}, we define a \emph{\textbf{hierarchy}} over hypotheses in our class $\cH$. Now, \emph{the hierarchy takes the form of a prior distribution over $\cH$} so that the preferred hypothesis has higher chance being selected.  

The \emph{McAllester's PAC Bayesian bound} is like the MDL paradigm with the \underline{\emph{\textbf{complexity}}} of hypothesis encoded by \underline{\emph{\textbf{the KL-divergence}}}. 
\end{remark}

\item \begin{remark} (\textbf{\emph{Regularization}}). \\
The \emph{\textbf{PAC-Bayes bound}} leads to the following learning rule:

Given a prior $\bP$, return a posterior $\bQ$ that minimizes the function
\begin{align}
 \E{h \sim \bQ}{L_{\cD}(h)} + \sqrt{\frac{ \kl{\bQ}{\bP}  + \log(m/\delta)}{2m-1}} \label{eqn: pac_bayes_learning}
\end{align}
This rule is similar to \emph{\textbf{\underline{the regularized risk minimization}} principle}. That is, we \emph{jointly minimize the empirical loss} of $\bQ$ on the sample \emph{and the Kullback-Leibler ``distance"} between $\bQ$ and $\bP$.
\end{remark}


\item For the special case of $0$-$1$ loss, we can the following improved bound:
\begin{theorem}(\textbf{Seeger's PAC Bayesian Inequality})\citep{seeger2002pac, maurer2004note, rasmussen2006gaussian, alquier2021user}\\
Let $\cP$ be an arbitrary distribution over an example domain $\cZ$. Let $\cH$ be a hypothesis class and let $\ell: \cH \times \cZ \to \set{0,1}$ be a loss function. Let $\bP$ be a prior distribution over $\cH$ and let $\delta \in (0,1)$. Then, with probability of at least $1 - \delta$ over $\cD$, for all distributions $\bQ \ll \bP$ over $\cH$, we have
\begin{align}
\mathds{KL}_{Ber}\paren{L_{\cD}(\bQ) \;\|\; L_{\cP}(\bQ)} \le   \frac{\kl{\bQ}{\bP} + \log\paren{1/\delta} + \log\paren{2\sqrt{m}}}{m} \label{ineqn: seeger_pac_bayes_kl}
\end{align} where $\mathds{KL}_{Ber}\paren{p \;\|\; q}$ is  the Kullback-Leibler divergence for Bernoulli random variable
\begin{align*}
\mathds{KL}_{Ber}\paren{p \;\|\; q} = p \log\frac{p}{q} + (1-p)\log\frac{1-p}{1- q}.
\end{align*}
\end{theorem}

\item \begin{remark}
This bound is based on the following inequality (see \citep{maurer2004note}):
\begin{align}
\E{\cD}{e^{m\mathds{KL}_{Ber}\paren{\hat{\mu}_m \;\|\; \mu} }} &\le 2\sqrt{m}, \label{pf: seeger_pac_bayes_kl_1}
\end{align} where $\hat{\mu}_m = \frac{1}{m}\sum_{i=1}^{m}X_i$ where $X_i \in  [0,1]$ almost surely and $X_1 \xdotx{,} X_m$ are i.i.d. random variables with mean $\E{}{X_i} = \mu$. The inequality is sharp since the equality is attained by Bernoulli random variable. The original inequality in \citep{seeger2002pac} is 
\begin{align*}
\E{\cD}{e^{m\mathds{KL}_{Ber}\paren{\hat{\mu}_m \;\|\; \mu} }} &\le m + 1.
\end{align*}
\end{remark}

\item \begin{remark}
By \emph{Pinsker's inequality},
\begin{align*}
\paren{L_{\cP}(\bQ) - L_{\cD}(\bQ) }^2  \le  \mathds{KL}_{Ber}\paren{L_{\cD}(\bQ) \;\|\; L_{\cP}(\bQ)} 
\end{align*} which recovers the inequality \eqref{ineqn: mcallester_pac_bayes_kl}.
\end{remark}

\item \begin{remark}
We can rewrite \eqref{ineqn: seeger_pac_bayes_kl} explicitly as
\begin{align}
\cP_{\cD}\set{L_{\cP}(\bQ) \le \mathds{KL}_{Ber}^{-1}\paren{L_{\cD}(\bQ) \Bigr\|\; \frac{\kl{\bQ}{\bP} + \log\paren{2\sqrt{m}/\delta}}{m}}} \ge 1 - \delta  \label{ineqn: seeger_pac_bayes_kl_2}
\end{align} where 
\begin{align*}
 \mathds{KL}_{Ber}^{-1}(q \| b) = \sup\set{p \in [0, 1]: \mathds{KL}_{Ber}(p \| q) \le b}.
\end{align*}
\end{remark}

\item \begin{corollary}\citep{alquier2021user}\\
For any $\delta >0$, any $\lambda \in (0, 2)$, with probability at least $1- \delta$,
\begin{align}
L_{\cP}(\bQ) \le \paren{1 - \frac{\lambda}{2}}^{-1}L_{\cD}(\bQ) + \brac{\lambda\paren{1 - \frac{\lambda}{2}}}^{-1}\frac{\kl{\bQ}{\bP} + \log\paren{2\sqrt{m}/\delta}}{m} \label{ineqn: seeger_pac_bayes_kl_modified}
\end{align}
\end{corollary}
\end{itemize}
\subsection{PAC Bayesian Inequalities for Other Divergences}





\newpage
\bibliographystyle{plainnat}
\bibliography{reference.bib}
\end{document}