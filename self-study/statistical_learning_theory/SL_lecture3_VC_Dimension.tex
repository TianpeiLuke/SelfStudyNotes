\documentclass[11pt]{article}
\usepackage[scaled=0.92]{helvet}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
\usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent %\usepackage{graphicx}
\usepackage{amsmath,amssymb, mathrsfs,  mathtools, dsfont}
\usepackage{tabularx}
\usepackage{tikz-cd}
\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage{graphicx}
\usepackage{xcolor}
%\usepackage[linkbordercolor ={1 1 1} ]{hyperref}
%\usepackage[sf]{titlesec}
\usepackage{natbib}
%\usepackage{tikz-cd}

\usepackage{../../Tianpei_Report}

%\usepackage{appendix}
%\usepackage{algorithm}
%\usepackage{algorithmic}

%\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\renewcommand{\algorithmicensure}{\textbf{Output:}}



\begin{document}
\title{Lecture 3: Rademacher Complexity and VC-Dimension}
\author{ Tianpei Xie}
\date{ Dec. 18th., 2022 }
\maketitle
\tableofcontents
\newpage
\section{PAC Learnability for Infinite Hypothese Set}
\begin{itemize}
\item \begin{remark} (\emph{\textbf{Bounding Excess Risk via Uniform Deviation}})\\
\item The definition of \emph{\textbf{agnostic PAC learnability}} requires that \emph{\textbf{the excess risk}} would be \underline{\emph{bounded above} \emph{\textbf{uniformly} over \textbf{all distributions}}}.
\begin{align}
L(h_n) - \inf_{h \in \cH}L(h) &= L(h_n) - \widehat{L}(h_n) + \widehat{L}(h_n) - L(h^{*}) \nonumber\\
&\le  L(h_n) - \widehat{L}(h_n) +\widehat{L}(h^{*}) - L(h^{*}) \nonumber\\
&\le  2 \sup_{h\in \cH}\abs{L(h) - \widehat{L}(h)} \label{ineqn: excess_risk_bound}
\end{align} where $h^{*} = \argmin_{h\in \cH}L(h)$ and $\widehat{L}(h)$ is the training error of $g$. The second last inequality is due to the fac that $h_n$ minimizes the training error.  Thus \emph{the estimation error} can be bounded uniformly by \emph{the generalization error bound} $|L(h) - \widehat{L}(h)|$ for any $h \in \cH$.

In this chapter, we discuss various ways to bound the uniform deviation:
\begin{align*}
\sup_{h\in \cH}\abs{\widehat{L}(h) - L(h)} := \norm{\widehat{\cP}_n - \cP}{\cH}
\end{align*}
\end{remark} 

\item \begin{remark} (\emph{\textbf{Universal Consistency for Infinite Hypothesis Set}})\\
When $\abs{\cH} < \infty$, we can use sample complexity bounds that involve $\log \abs{\cH}$ for \emph{universal consistency} of ERM. Obviously, we cannod do so when $\abs{\cH} = \infty$. A general idea of analyzing infinite hypothesis set consists of \emph{\textbf{reducing the infinite case to the analysis of finite sets of hypotheses}} and then proceed as in the previous chapter. 

There are different techniques for that reduction, each relying on \emph{a different notion of \textbf{complexity}} for \emph{the family of hypotheses}. 
\end{remark}
\end{itemize}

\section{Rademacher complexity}
\begin{itemize}
\item \begin{remark} (Notations)\\
We will continue to use $\cH$ to denote a \emph{hypothesis set} as in the previous chapters, and $h \in \cH$ an \emph{element} of $\cH$. Many of the results of this section are general and hold for an arbitrary \emph{loss function} $L: \cY \times \cY \to \bR$. To each $h: \cX \to \cY$, we can associate a function $g$:
\begin{align*}
g: (x, y) \in \cX \times \cY \to L(h(x), y)
\end{align*} without explicitly describing the specific loss $L$ used. In what follows $\cG$ will generally be interpreted as \emph{\textbf{the family of loss functions} associated to $\cH$}. 
\end{remark}


\item \begin{definition} (\emph{\textbf{Empirical Rademacher Complexity}})\\
Let $\cG$ be a family of functions mapping from $\cZ := \cX \times \cY$ to $[a, b]$ and $\cD_{n} = (z_1 \xdotx{,} z_n)$ a fixed \emph{sample} of size $n$ with elements in $\cZ$. Then, \underline{\emph{\textbf{the empirical Rademacher complexity}}} of $\cG$ \emph{with respect to the sample $\cD$} is defined as:
\begin{align}
\widehat{\mathfrak{R}}_{\cD}(\cG)&= \E{\sigma}{\sup_{g \in \cG}\frac{1}{m}\sum_{i=1}^{m}\sigma_{i}g(z_i)}   \label{eqn: rademacher_complexity}
\end{align}
where $\sigma := (\sigma_1 \xdotx{,} \sigma_m)$, with $\sigma_i$s \textbf{\emph{independent uniform random variables}} taking values in $\set{-1, +1}$. The random variables $\sigma_i$ are called \underline{\emph{\textbf{Rademacher variables}}}.
\end{definition}

\item \begin{remark} (\emph{\textbf{How Well to Fit Random Noise}})\\
\emph{The Rademacher complexity} captures the richness of a family of functions by measuring \emph{\textbf{the degree} to which \textbf{a hypothesis set} can \textbf{fit random noise}}. The \emph{richer or more complex families} $\cH$ can generate \emph{more vectors} $h_{\cD}$ and thus \emph{better correlate with random noise}, on $\cD_n$.

The intuition is that if a hypothesis set can fit arbitrary noise, then it is too large to bound the performance of ERM, i.e. it is very likely to have overfitting (zero empirical error but arbitrary bad generalization error).
\end{remark}

\item \begin{remark}
\begin{align*}
\widehat{\mathfrak{R}}_{\cD}(\cG)&= \E{\sigma}{\sup_{g \in \cG}\frac{\inn{\sigma_n}{g_{\cD_n}}}{n}}
\end{align*} which measures the correlation between the random noise $\sigma_m := \set{\sigma_i}$ and $g_{\cD_n} := \set{g(X_i, Y_i)}_{i=1}^{n}$. The supremum $\sup_{g \in \cG} \inn{\sigma_n}{g_{\cD_n}}/n$ is a measure of how well the function class $\cH$ correlates with $\sigma$ over the sample $\cD_n$. Thus, \emph{\textbf{the empirical Rademacher complexity}} measures \emph{on average} how well \emph{\textbf{the function class}} $\cH$ \emph{\textbf{correlates with random noise}} on $\cD_n$. 
\end{remark}

\item \begin{remark} (\emph{\textbf{Rademacher Variables $=$ Symmetric Bernoulli Random Variable}})\\
\emph{A Rademacher variables} is a \emph{random variable} on $\set{-1, +1}$ with \emph{\textbf{symmetric probability}} $P(\sigma_i = +1) = P(\sigma_i = -1)= \frac{1}{2}$, i.e. a set of i.i.d. symmetric Bernoulli random variables.

\emph{A Rademacher proces}s is a stochastic process $\set{\sigma_i g(X_i)}$ conditioning on $\set{X_i}$, which is sub-gaussian process.
\end{remark}


\item \begin{definition} (\emph{\textbf{Rademacher Complexity}})\\
Let $\cP$ denote the distribution according to which samples are drawn. For any integer $n \ge 1$, \underline{\emph{\textbf{the Rademacher complexity}}} of $\cG$ is defined as the \emph{expectation} of \emph{the empirical
Rademacher complexity} over \emph{all samples of size $n$} drawn according to $\cP$:
\begin{align*}
\frR_{n}(\cG) &= \E{\cP^{n}}{\widehat{\mathfrak{R}}_{\cD_n}(\cG)}.
\end{align*}
\end{definition}

\item \begin{proposition} (\textbf{Consistency Bound with respect to Rademacher Complexity}) \citep{mohri2018foundations}\\
Let $\cG$ be a family of functions mapping from $\cZ$ to $[0, 1]$. Then, for any $\delta > 0$, \textbf{with probability at least} $1 - \delta$, each of the following holds for all $g \in \cG$:
\begin{align}
\E{}{g(Z)} &\le \frac{1}{n}\sum_{i=1}^{n}g(Z_i) + 2 \frR_{n}(\cG)  + \sqrt{\frac{\log(1 / \delta)}{2n}} \label{eqn: radmatcher_bound_1}
\end{align}
and
\begin{align}
\E{}{g(Z)} &\le \frac{1}{n}\sum_{i=1}^{n}g(Z_i) + 2 \widehat{\mathfrak{R}}_{n}(\cG) + 3\sqrt{\frac{\log(2 / \delta)}{2n}} \label{eqn: radmatcher_bound_2}
\end{align}
\end{proposition}

\item \begin{remark} 
Define a function $\Phi$ on $\cD_n$ by
\begin{align*}
\Phi(\cD_n) &:= \sup_{g \in \cG}\abs{\frac{1}{n}\sum_{i=1}^{n}g(Z_i) - \E{}{g(Z)}}.
\end{align*}This function has \emph{bounded difference} if \emph{only one sample makes changes}. The proof consists of three parts:
\begin{enumerate}
\item Bound \emph{the probability} of \emph{tail event} of $\Phi$
\begin{align*}
\Phi(\cD_n) - \E{\cP^{n}}{\Phi(\cD_n)}
\end{align*}
\item Bound the expectation $\E{\cP^{n}}{\Phi(\cD_n)}$ by \emph{Radmatcher complexity on $\cG$}
\begin{align*}
\E{\cP^{n}}{\Phi(\cD_n)} &\le 2   \frR_{n}(\cG).
\end{align*} This obtain the inequality \eqref{eqn: radmatcher_bound_1}.

\item Bound \emph{the probability} of \emph{tail events} for \emph{empircal Radmatcher complexity}:
\begin{align*}
\frR_{n}(\cG)  - \widehat{\mathfrak{R}}_{\cD_n}(\cG)
\end{align*}
\end{enumerate}
Combines the difference in (1) and (3) using union bounds. 
\end{remark}

\item \begin{lemma} (\textbf{Radmatcher Complexity for 0-1 Loss}) \citep{mohri2018foundations}\\
Let $\cH$ be a family of functions taking values in $\{-1, +1\}$ and let $\cG$ be the family of loss functions associated to $\cH$ for \textbf{the zero-one loss}: 
\begin{align*}
\cG := \set{(x,y) \mapsto \mathds{1}_{h(x) \neq y}:  h \in \cH}.
\end{align*}
For any sample $\cD = ((X_1, Y_1) \xdotx{,} (X_n, Y_n))$ of elements in $\cX \times \{-1, +1\}$, let $\cD_{\cX} := (X_1 \xdotx{,} X_n)$. Then, the following relation holds between \textbf{the empirical Rademacher complexities} of $\cG$ and $\cH$:
\begin{align*}
\widehat{\frR}_{\cD}(\cG) &\le \frac{1}{2}\widehat{\frR}_{\cD_{\cX}}(\cH)
\end{align*}
\end{lemma}

\item \begin{proposition} \label{prop: generalization_bound_radmatcher} (\textbf{Rademacher Complexity Generalization Bounds})\citep{mohri2018foundations}\\
Let $\cH$ be a family of functions taking values in $\{-1, +1\}$  and let $\cP_{X}$ be the distribution over the input space $\cX$. Then, for any $\delta > 0$, \textbf{with probability at least} $1 - \delta$ over a sample $\cD_{n}$ of size $n$ drawn according to $\cP_{X}$, each of the following holds for \textbf{any} $h \in \cH$:
\begin{align}
L(h) &\le \widehat{L}_{n}(h) + \frR_{n}(\cH) + \sqrt{\frac{\log(1 / \delta)}{2n}}  \label{eqn: radmatcher_binary_loss_bound_1}
\end{align} and
\begin{align}
L(h) &\le \widehat{L}_{n}(h) + \widehat{\frR}_{\cD_{n}}(\cH) +  3\sqrt{\frac{\log(2 / \delta)}{2n}}.  \label{eqn: radmatcher_binary_loss_bound_2}
\end{align}
\end{proposition}

\item \begin{remark} (\emph{\textbf{Compute Emipirical Radmatcher Complexity}})\\
Note that the Radmatcher variable $\sigma_i$ and $-\sigma_i$ have the same distribution. So
\begin{align*}
\widehat{\frR}_{\cD}(\cH)&= \E{\sigma}{\sup_{h \in \cH}\frac{1}{n}\sum_{i=1}^{n}(-\sigma_{i}h(X_i))} = -\E{\sigma}{\inf_{h \in \cH}\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}h(X_i)}
\end{align*} Now, for \emph{a fixed value of $\sigma$}, computing 
\begin{align*}
\inf_{h \in \cH}\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}h(X_i)
\end{align*} is equivalent to an \emph{\textbf{empirical risk minimization problem}}, which is known to be \emph{\textbf{computationally hard}} for some hypothesis sets. Thus, in some cases, computing $\widehat{\frR}_{\cD}(\cH)$ could be computationally hard. 
\end{remark}
\end{itemize}

\section{VC-Dimension}
\subsection{Definition of VC-Dimension}
\begin{itemize}
\item \begin{remark}
Recall the decomposition of generalization error:
\begin{align*}
L(g_n^{*}) - L^{*} &=\underbrace{\paren{L(g_n^{*}) - \inf_{g \in \cH}L(g)}}_{\text{\emph{estimation error}}} + \underbrace{\paren{\inf_{g \in \cH}L(g) - L^{*}}}_{\text{\emph{approximation error}}}.
\end{align*} 
where the first term is called \emph{estimation error} and the second term is called approximation error.  The definition of \emph{\textbf{PAC learnability}} requires that \emph{the estimation error} would be bounded \emph{\textbf{uniformly} over \textbf{all distributions}}.

Can \emph{infinite-size classes} be learnable, and, if so, what determines their \emph{sample complexity} ?
\end{remark}

\item \begin{remark} (\emph{\textbf{Restriction based on Behavior of Functions on Data $\cD$}})\\
Recall the No-Free-Lunch theorem and its proof \citep{shalev2014understanding}. There, we have shown that \emph{without restricting the hypothesis class}, for any learning algorithm, \emph{an adversary can construct a distribution} for which \emph{the learning algorithm will perform poorly}, while there is \emph{another learning algorithm that will succeed on the same distribution}. To do so, the adversary used a finite set $\cD \subset \cX$ and considered a family of distributions that are
\emph{\textbf{concentrated on elements of $\cD$}}. Each distribution was \emph{derived from a ``true" target function} from $\cD$ to $\set{0,1}$. To make any algorithm fail, the adversary used the power of choosing a target function from the set of \emph{all possible functions} from $\cD$ to $\set{0,1}$.

When considering \emph{PAC learnability of a hypothesis class $\cH$}, the adversary is restricted to \emph{constructing distributions} for which \emph{some hypothesis $h \in \cH$ achieves a zero risk}. Since we are considering \emph{\textbf{distributions that are concentrated on elements of $\cD$}}, we should study how $\cH$ behaves on $\cD$
\end{remark}

\item \begin{definition} (\emph{\textbf{Restriction of $\cH$ to $\cD$}}). \\
Let $\cH$ be a class of functions from $\cX$ to $\set{0,1}$ and let $\cD = \set{x_1 \xdotx{,} x_n} \subset \cX$. 

\underline{\emph{\textbf{The restriction of $\cH$ to $\cD$}}} is \emph{the set of functions} from $\cD$ to $\set{0,1}$ that can be \emph{derived from} $\cH$. That is,
\begin{align*}
\cH_{\cD} := \set{(h(x_1) \xdotx{,} h(x_n)): h \in \cH},
\end{align*}
where we \emph{\textbf{represent}} each function from $\cX$ to $\set{0,1}$ as a \emph{\textbf{vector}} in $\set{0,1}^{\abs{\cD}}$.
\end{definition}

\item \begin{remark} (\textbf{\emph{What You See Is All You Know}})\\
Using the output of functions on \emph{\textbf{a finite set of samples}}, we can define an \emph{\textbf{equivalence relationship}}: $f \sim g$ if and only if their \emph{\textbf{outputs}} vector on given finite set $\cD_m$ are \emph{\textbf{the same}}. Thus, unlike the original space $\cH$, \emph{\textbf{the quotient space}} $\cH/\sim$ is a much \emph{simpler function space} with \emph{\textbf{finite dimensional representation}}. 

In other word, \underline{\emph{\textbf{what you see is all you know}}}, i.e. there is no way to distinguish $f$ and $g$ beyond their answers to given limited set of questions in $\cD$.

%One can also think of this in functional analysis, where a set of $m$ evaluation functional $\delta_{x_i}(f) = f(x_i), 1\le i\le m$ are given and the function space is reparameterized using these $m$ functionals. 
\end{remark}

\item \begin{definition}(\emph{\textbf{Shattering}}). \\
A hypothesis class $\cH$ \underline{\emph{\textbf{shatters}} a finite set $\cD \subset \cX$} if \emph{\textbf{the restriction of $\cH$ to $\cD$}} is the set of \emph{\textbf{all functions}} from $\cD$ to $\set{0,1}$. That is, 
\begin{align*}
\abs{\cH_{\cD}} &= 2^{\abs{\cD}}.
\end{align*}
\end{definition}

\item \begin{remark}
Whenever some set $\cD$ is \emph{\textbf{shattered}} by $\cH$, the \emph{\textbf{adversary}} is \emph{\textbf{not restricted} by $\cH$}, as they can \emph{\textbf{construct a distribution over $\cD$}} based on \emph{\textbf{any}} target function from $\cD$ to $\set{0,1}$, while still maintaining the realizability assumption. 
\end{remark}

\item The following is the corollary of \emph{the No Free Lunch Theorem}:
\begin{corollary} \citep{shalev2014understanding}\\
Let $\cH$ be a hypothesis class of functions from $\cX$ to $\set{0,1}$. Let $n$ be a training set size. Assume that there exists a set $\cD \subset X$ of size $2n$ that is \textbf{shattered} by $\cH$. Then, for any learning algorithm, $\cA$, there exist a \textbf{distribution} $\cP$ over $\cX \times \set{0,1}$
and a predictor $h \in \cH$ such that 
\begin{align*}
L_{\cP}(h) = 0
\end{align*}
but \textbf{with probability of at least} $1/7$ over the choice of $\cD \sim \cP^n$ we have that 
\begin{align*}
L_{\cP}(\cA(\cD)) \ge 1/8.
\end{align*}
\end{corollary}

\item \begin{remark} (\emph{\textbf{A Model that can Explain Everything is Worthless}})\\
If $\cH$ \emph{\textbf{shatters}} some set $\cD$ of size $2m$ then we cannot learn $\cH$ using $m$ examples. 

Intuitively, if a set $\cD$ is \emph{\textbf{shattered}} by $\cH$, and we receive a sample containing half the instances of $\cD$, the labels of these instances give us \emph{\textbf{no information} about the labels of the \textbf{rest} of the instances} in $\cD$ - \emph{\textbf{every possible labeling of the rest of the instances can be explained by some hypothesis in $\cH$}}. 

Philosophically,

\underline{\emph{\textbf{If someone can explain every phenomenon, his explanations are worthless}}}.
\end{remark}

\item \begin{definition} (\emph{\textbf{VC-Dimension}}). \\
\underline{\emph{\textbf{The VC-dimension}}} of a hypothesis class $\cH$, denoted $VCdim(\cH)$ or simply $v(\cH)$, is \emph{\textbf{the \underline{maximal size}} of a set $\cD \subset \cX$ that can be \textbf{shattered} by $\cH$}.

If $\cH$ can shatter sets of \emph{\textbf{arbitrarily large size}} we say that $\cH$ has \underline{\emph{\textbf{infinite VC-dimension}}}.
\end{definition}

\item \begin{theorem} (\textbf{No Free Lunch, VC Dimension}) \citep{shalev2014understanding}\\
Let $\cH$ be a class of \textbf{infinite VC-dimension}. Then, $\cH$ is \textbf{not PAC learnable}.
\end{theorem}

\end{itemize}

\subsection{Growth Function}
\begin{itemize}
\item \begin{remark}
We defined the notion of \emph{\textbf{shattering}}, by considering \emph{\textbf{the restriction of $\cH$} to a finite set of instances}. The \emph{growth function} measures the \emph{maximal ``\textbf{effective}" size of $\cH$ on a set of $n$ examples}. Formally:
\end{remark}

\item \begin{definition} (\emph{\textbf{Growth Function}}). \\
Let $\cH$ be a hypothesis class. Then \underline{\emph{\textbf{the growth function of $\cH$}}}, denoted $\tau_{\cH}: \bN \to \cN$, is defined as
\begin{align*}
\tau_{\cH}(m) &:= \max_{\cD \subset \cX: \abs{\cD} = n}\abs{\cH_{\cD}}.
\end{align*}
In words, $\tau_{\cH}(m)$ is \textbf{\emph{the number of different functions}} from a set $\cD$ of \emph{\textbf{size $n$}} to $\set{0,1}$ that can be obtained by \emph{\textbf{restricting $\cH$ to $\cD$}}.
\end{definition}

\item \begin{remark}
if $VCdim(\cH) = d$ then for any $n \le d$ we have $\tau_{\cH}(n) = 2^n$. In such cases, $\cH$ induces \emph{all possible functions from} $\cD$ to $\set{0,1}$. 
\end{remark}

\item \begin{lemma} (\textbf{Sauer's Lemma}). \citep{shalev2014understanding, mohri2018foundations}\\
Let $\cH$ be a hypothesis class with $VCdim(\cH) \le d < \infty$. Then, for all $n$, 
\begin{align}
\tau_{\cH}(n) &\le \sum_{i=0}^{d}{{n}\choose{i}} \label{eqn: sauer_lemma_1}
\end{align}
In particular, if $n > d + 1$ then
\begin{align}
\tau_{\cH}(n) &\le \paren{\frac{e\,n}{d}}^{d}. \label{eqn: sauer_lemma_2}
\end{align}
\end{lemma}
\begin{proof}
To prove the lemma it suffices to prove the following \emph{stronger claim}: For any $\cD = \set{x_1 \xdotx{,} x_m}$ we have
\begin{align}
\forall\, \cH, \; \abs{\cH_{\cD}} \le  \abs{\set{\cB \subseteq \cD: \cH\text{ shatters }\cB}}\label{eqn: sauer_lemma_3}
\end{align} The reason why Equation \eqref{eqn: sauer_lemma_3} is sufficient to prove the lemma is that if
$VCdim(\cH) \le d$ then no set whose size is \emph{larger than} $d$ is \emph{shattered} by $\cH$ and therefore
\begin{align*}
\abs{\set{\cB \subseteq \cD: \cH\text{ shatters }\cB}} \le  \sum_{i=0}^{d}{{n}\choose{i}}.
\end{align*}
When $n > d + 1$ the right-hand side of the preceding is \emph{at most} $\paren{\frac{e\,n}{d}}^{d}$.

We are left with proving Equation \eqref{eqn: sauer_lemma_3} and we do it using an \emph{\textbf{inductive argument}}. 
\begin{enumerate}
\item For $n = 1$, no matter what $\cH$ is, either both sides of Equation \eqref{eqn: sauer_lemma_3} equal $1$ or both sides equal $2$ (\emph{the empty set} is always considered to be shattered by $\cH$).

\item  \emph{Assume} Equation \eqref{eqn: sauer_lemma_3} holds for sets of size $k < n$ and let us prove it for sets of size $m$. 

Fix $\cH$ and $\cD = \set{x_1 \xdotx{,} x_n}$. Denote $\cD_{-1} = \set{x_2 \xdotx{,} x_n}$ and in addition, define the
following two sets:
\begin{align*}
Y_{0} = \set{(y_2 \xdotx{,} y_n): (0, y_2 \xdotx{,} y_n) \in \cH_{\cD} \lor  (1, y_2 \xdotx{,} y_n) \in \cH_{\cD}},
\end{align*}
and
\begin{align*}
Y_{1} = \set{(y_2 \xdotx{,} y_n): (0, y_2 \xdotx{,} y_n) \in \cH_{\cD} \land  (1, y_2 \xdotx{,} y_n) \in \cH_{\cD}}.
\end{align*}
It is easy to verify that $\abs{\cH_{\cD}} = \abs{Y_0} + \abs{Y_1}$. Additionally, since $Y_0 = \cH_{\cD_{-1}}$, using \emph{the induction assumption} (applied on $\cH$ and $\cD_{-1}$) we have that
\begin{align*}
\abs{Y_0} = \abs{\cH_{\cD_{-1}}} &\le \abs{\set{\cB \subseteq \cD_{-1}: \cH\text{ shatters }\cB}} \\
&=  \abs{\set{\cB \subseteq \cD: x_1 \not\in \cB \land \cH\text{ shatters }\cB}}.
\end{align*}
Next, define $\cH' \subseteq \cH$ to be
\begin{align*}
\cH'  &= \set{h \in \cH : \exists\,h' \in \cH\text{ s.t. }(1 - h'(x_1), h'(x_2) \xdotx{,} h'(x_n)) = (h(x_1), h(x_2)  \xdotx{,} h(x_n)) },
\end{align*}
namely, $\cH'$ contains pairs of hypotheses $(h, h')$ that \emph{agree on $\cD_{-1}$ and differ on $x_1$}. Using this definition, it is clear that if $\cH'$ \emph{shatters} a set $\cB \subseteq \cD$ then it \emph{also shatters} the set $\cB \cup \set{x_1}$ and \emph{vice versa}. 

Combining this with the fact that $Y_1 = \cH'_{\cD_{-1}}$ and using the inductive assumption (now applied on $\cH'$ and $\cD_{-1}$ ) we obtain that
\begin{align*}
\abs{Y_1} = \abs{ \cH'_{\cD_{-1}}} &\le \abs{\set{\cB \subseteq \cD_{-1}:  \cH'\text{ shatters }\cB }} \\
&= \abs{\set{\cB \subseteq \cD_{-1}:  \cH'\text{ shatters }\cB\cup\set{x_1} }} \\
&= \abs{\set{\cB \subseteq \cD:  x_1 \in \cB \land \cH'\text{ shatters }\cB }} \\
&= \abs{\set{\cB \subseteq \cD:  x_1 \in \cB \land \cH \text{ shatters }\cB }}.
\end{align*}
Overall, we have shown that
\begin{align*}
\abs{\cH_{\cD}} &= \abs{Y_0} + \abs{Y_1} \\
&\le \abs{\set{\cB \subseteq \cD: x_1 \not\in \cB \land \cH\text{ shatters }\cB}} +  \abs{\set{\cB \subseteq \cD:  x_1 \in \cB \land \cH \text{ shatters }\cB }}\\
&= \abs{\set{\cB \subseteq \cD: \cH\text{ shatters }\cB}}
\end{align*}
which concludes our proof. \qed
\end{enumerate}
\end{proof}
\end{itemize}
\subsection{Relate Growth Function to Radmatcher Complexity}
\begin{itemize}
\item \begin{lemma}(\textbf{Massart's Lemma}) \citep{mohri2018foundations}\\
Let $A \subseteq \bR^n$ be \textbf{a finite set}, with $r = \max_{x \in A}\norm{x}{2}$, then the following holds:
\begin{align}
\E{\sigma}{\frac{1}{n}\sup_{x \in A}\sum_{i=1}^{n}\sigma_i\,x_i} \le \frac{r\sqrt{2 \log\abs{A}}}{n} \label{eqn: massart_lemma}
\end{align}
where $\sigma_i$'s are \textbf{independent uniform random variables} taking values in $\set{-1, +1}$ and $x_1 \xdotx{,} x_n$ are the components of vector $x$.
\end{lemma}

\item \begin{corollary} \label{cor: radmatcher_growth} (\textbf{Radmatcher Complexity Bounds by Growth Number}) \citep{mohri2018foundations}\\
Let $\cH$ be a family of functions taking values in $\set{-1, +1}$. Then the following holds:
\begin{align}
\frR_{n}(\cH) &\le \sqrt{\frac{2 \log \tau_{\cH}(n)}{n}} \label{eqn: growth_number_radmatcher_comp}
\end{align}
\end{corollary}
\begin{proof}
For a fixed sample $\cD = (x_1 \xdotx{,} x_n)$, we denote by $\cH_{\cD} \subset \{-1, +1\}^{m}$ the set of vectors of function values $(h(x_1) \xdotx{,} h(x_n))$ where $h$ is in $\cH$. Since $h\in \cH$ takes values in $\set{-1, +1}$, the \emph{norm} of these vectors is bounded by $\sqrt{n}$. We can then apply
\emph{Massart's lemma} as follows:
\begin{align*}
\frR_{n}(\cH) &= \E{\cD}{\E{\sigma}{\sup_{u \in \cH_{\cD}}\frac{1}{n}\sum_{i=1}^{n}\sigma_i u_i \Big| \cD}} \le  \E{\cD}{\frac{\sqrt{n}\,\sqrt{2 \log\abs{\cH_{\cD}}}}{n}}
\end{align*}
By definition, $\abs{\cH_{\cD}}$ is bounded by \emph{the growth function}, thus,
\begin{align*}
\frR_{n}(\cH)  &\le  \E{\cD}{\frac{\sqrt{n}\,\sqrt{2 \log \tau_{\cH}(n)}}{n}} = \sqrt{\frac{2 \log \tau_{\cH}(n)}{n}},
\end{align*}
which concludes the proof. \qed
\end{proof}
\end{itemize}
\subsection{Generalization Bounds via Growth Function and VC-Dimension}
\begin{itemize}
\item Combining Proposition \ref{prop: generalization_bound_radmatcher} to Corollary \ref{cor: radmatcher_growth}, we have:
\begin{corollary} \label{cor: generalization_bound_growth}   (\textbf{Growth Function Generalization Bound})  \citep{mohri2018foundations}\\
Let $\cH$ be a family of functions taking values in $\set{-1, +1}$.  Then, for any $\delta > 0$, with probability at least $1 - \delta$, for any $h \in \cH$,
\begin{align}
L(h) \le \widehat{L}_{n}(h) + \sqrt{\frac{2 \log \tau_{\cH}(n)}{n}} +\sqrt{\frac{\log(1/\delta)}{2n}} \label{eqn: generalization_bound_growth_number}
\end{align}

Growth function bounds can be also derived directly (without using Rademacher complexity bounds first). The resulting bound is then the following:
\begin{align}
\cP\set{\abs{L(h) - \widehat{L}_{n}(h)} > \epsilon }  \le 4\tau_{\cH}(2n)\;\exp\paren{-\frac{n\epsilon^2}{8}} \label{eqn: generalization_bound_growth_number_2}
\end{align}
which only differs from \eqref{eqn: generalization_bound_growth_number} by constants.
\end{corollary}

\item Applying \emph{Sauer's Lemma} to Corollary \ref{cor: generalization_bound_growth}, we have:
\begin{corollary} \label{cor: generalization_bound_vc} (\textbf{VC-Dimension Generalization Bounds}) \citep{mohri2018foundations}\\
Let $\cH$ be a family of functions taking values in $\set{-1, +1}$ with \textbf{VC-dimension} $d$. Then, for any $\delta > 0$, with probability at least $1 - \delta$, the following holds for all $h \in \cH$:
\begin{align}
L(h) \le \widehat{L}_{n}(h) + \sqrt{\frac{2d \log(en /d)}{n}} +\sqrt{\frac{\log(1/\delta)}{2n}} \label{eqn: generalization_bound_vc_dim}
\end{align}
Thus, the form of this generalization bound is
\begin{align}
L(h) \le \widehat{L}_{n}(h) + O\paren{\sqrt{\frac{\log(n /d)}{(n/d)}}},  \label{eqn: generalization_bound_vc_dim2}
\end{align}
which emphasizes \textbf{the importance of the ratio $n/d$} for generalization. 
\end{corollary}

\item \begin{remark}
The theorem provides another instance of \underline{\emph{\textbf{Occam's razor principle}}} where simplicity is \emph{\textbf{measured}} in terms of \emph{\textbf{smaller VC-dimension}}.

\emph{VC-dimension bounds} can be derived directly \emph{without using an intermediate Rademacher complexity bound}: combining \emph{Sauer’s lemma} with \eqref{eqn: generalization_bound_growth_number_2} leads to the following high-probability bound
\begin{align}
L(h) \le \widehat{L}_{n}(h) + \sqrt{\frac{8d\log(2en /d) + 8\log(4/\delta)}{n}},  \label{eqn: generalization_bound_vc_dim3}
\end{align}
which has the general form of \eqref{eqn: generalization_bound_vc_dim2}. \emph{The log factor plays only a minor role in these bounds}. A finer analysis can be used in fact to eliminate that factor.
\end{remark}

\end{itemize}

\subsection{Examples}

\subsection{Lower Bounds}
\begin{itemize}
\item \begin{remark} (\emph{\textbf{No-Free-Lunch via VC-Dimension}})\\
This section provides \emph{\textbf{lower bounds}} on \emph{the generalization error} of \emph{any learning algorithm} in terms of the \emph{VC-dimension} of the hypothesis set used.

These lower bounds are shown by finding \emph{for any algorithm} a `\emph{\textbf{bad}}' \emph{distribution}. Since the learning algorithm is \emph{arbitrary}, it will be difficult to specify that particular distribution. Instead, it suffices to \emph{prove its \textbf{existence} \textbf{non-constructively}}. At a high level, the proof technique used to achieve this is \emph{\textbf{the probabilistic method}} of Paul Erd\"os. 

In the context of the following proofs, 
\begin{enumerate}
\item \emph{first} \emph{\textbf{a lower bound}} is given on the expected error over \emph{the parameters defining the distributions}.

\item  From that, \emph{the lower bound} is shown to \emph{\textbf{hold for at least one set of parameters}}, that is one distribution.
\end{enumerate}
\end{remark}

\item \begin{proposition} (\textbf{Lower Bound, Realizable Case}) \citep{mohri2018foundations}\\
Let $\cH$ be a hypothesis set with \textbf{VC-dimension} $d > 1$. Then, for \textbf{any} learning algorithm $\cA$, there \textbf{exist} a \textbf{distribution} $\cP$ over $\cX$ and a \textbf{target function} $c \in \cH$ such that
\begin{align*}
\cP^n\set{L_{\cP, c}(\cA_{\cH}(\cD_n)) > \frac{d-1}{32 n}} \ge \frac{1}{100}
\end{align*}
\end{proposition}

\item \begin{remark} (\emph{\textbf{Infinite VC-dimension $\Rightarrow$ Non PAC-learnable}})\\
The theorem shows that for any algorithm $\cA$, there exists a `\emph{bad}' distribution over $\cX$ and a target function $f$ for which the error of the hypothesis returned by $\cA$ is
\begin{align*}
\Omega\paren{\frac{d}{n}}
\end{align*} with some constant probability. This further demonstrates the key role played by the VC-dimension in learning. The result implies in particular that \emph{\textbf{PAC-learning in the non-realizable case} is \textbf{not possible} when the \textbf{VC-dimension} is \textbf{infinite}}.
\end{remark}

\item \begin{proposition} (\textbf{Lower Bound, Non-Realizable Case}) \citep{mohri2018foundations} \\
Let $\cH$ be a hypothesis set with \textbf{VC-dimension} $d > 1$. Then, for any learning algorithm $\cA$, there exists a distribution $\cP$ over $\cX \times \set{0, 1}$ such that:
\begin{align*}
\cP^{n}\set{L_{\cP}(\cA_{\cH}(\cD_n)) - \inf_{h  \in \cH}L_{\cP}(h)  > \sqrt{\frac{d}{320 n}}} \ge \frac{1}{64}
\end{align*} Equivalently, for any learning algorithm, \textbf{the sample complexity} verifies
\begin{align*}
n &\ge \frac{d}{320 \epsilon^2}.
\end{align*}
\end{proposition}

\item \begin{remark}
The theorem shows that \emph{for any algorithm} $\cA$, \emph{\textbf{in the non-realizable case}} (i.e. the Bayes classifier is not in $\cH$), there exists
a `\emph{bad}' distribution over $\cX \times \set{0, 1}$ such that the error of the hypothesis returned by $\cA$ is
\begin{align*}
\Omega\paren{\sqrt{\frac{d}{n}}}
\end{align*} with some constant probability. The \emph{VC-dimension} appears as a critical quantity in learning in \emph{this general setting} as well. In particular, \emph{with an \textbf{infinite VC-dimension}, \textbf{agnostic PAC-learning is not possible}}.
\end{remark}
\end{itemize}


\newpage
\bibliographystyle{plainnat}
\bibliography{reference.bib}
\end{document}